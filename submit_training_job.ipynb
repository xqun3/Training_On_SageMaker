{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f78e3de0-d9f2-4099-88ad-d2932fe952d1",
   "metadata": {},
   "source": [
    "# Multi-Node Training on SageMaker Training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1465fc2e-11a6-49fa-a0ff-84b6e0ea1501",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sagemaker in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (2.241.0)\n",
      "Collecting sagemaker\n",
      "  Downloading sagemaker-2.242.0-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: attrs<24,>=23.1.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (23.2.0)\n",
      "Requirement already satisfied: boto3<2.0,>=1.35.75 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (1.37.7)\n",
      "Requirement already satisfied: cloudpickle>=2.2.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (3.1.1)\n",
      "Requirement already satisfied: docker in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (7.1.0)\n",
      "Requirement already satisfied: fastapi in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (0.115.8)\n",
      "Requirement already satisfied: google-pasta in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (0.2.0)\n",
      "Requirement already satisfied: importlib-metadata<7.0,>=1.4.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (6.11.0)\n",
      "Requirement already satisfied: jsonschema in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (4.23.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.9.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (1.26.4)\n",
      "Requirement already satisfied: omegaconf<=2.3,>=2.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (2.3.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (21.3)\n",
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (2.2.3)\n",
      "Requirement already satisfied: pathos in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (0.3.3)\n",
      "Requirement already satisfied: platformdirs in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (4.3.6)\n",
      "Requirement already satisfied: protobuf<6.0,>=3.12 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (5.29.3)\n",
      "Requirement already satisfied: psutil in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (6.1.1)\n",
      "Requirement already satisfied: pyyaml~=6.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (6.0.2)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (2.32.3)\n",
      "Requirement already satisfied: sagemaker-core<2.0.0,>=1.0.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (1.0.22)\n",
      "Requirement already satisfied: schema in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (0.7.7)\n",
      "Requirement already satisfied: smdebug-rulesconfig==1.0.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (1.0.1)\n",
      "Requirement already satisfied: tblib<4,>=1.7.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (3.0.0)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (4.67.1)\n",
      "Requirement already satisfied: urllib3<3.0.0,>=1.26.8 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (2.3.0)\n",
      "Requirement already satisfied: uvicorn in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker) (0.34.0)\n",
      "Requirement already satisfied: botocore<1.38.0,>=1.37.7 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from boto3<2.0,>=1.35.75->sagemaker) (1.37.7)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from boto3<2.0,>=1.35.75->sagemaker) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.12.0,>=0.11.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from boto3<2.0,>=1.35.75->sagemaker) (0.11.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from importlib-metadata<7.0,>=1.4.0->sagemaker) (3.21.0)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from omegaconf<=2.3,>=2.2->sagemaker) (4.9.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from packaging>=20.0->sagemaker) (3.2.1)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker-core<2.0.0,>=1.0.17->sagemaker) (2.10.6)\n",
      "Requirement already satisfied: rich<14.0.0,>=13.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker-core<2.0.0,>=1.0.17->sagemaker) (13.9.4)\n",
      "Requirement already satisfied: mock<5.0,>4.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sagemaker-core<2.0.0,>=1.0.17->sagemaker) (4.0.3)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jsonschema->sagemaker) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jsonschema->sagemaker) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jsonschema->sagemaker) (0.22.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->sagemaker) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->sagemaker) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->sagemaker) (2025.1.31)\n",
      "Requirement already satisfied: starlette<0.46.0,>=0.40.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from fastapi->sagemaker) (0.45.3)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from fastapi->sagemaker) (4.12.2)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from google-pasta->sagemaker) (1.17.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pandas->sagemaker) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pandas->sagemaker) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pandas->sagemaker) (2025.1)\n",
      "Requirement already satisfied: ppft>=1.7.6.9 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pathos->sagemaker) (1.7.6.9)\n",
      "Requirement already satisfied: dill>=0.3.9 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pathos->sagemaker) (0.3.9)\n",
      "Requirement already satisfied: pox>=0.3.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pathos->sagemaker) (0.3.5)\n",
      "Requirement already satisfied: multiprocess>=0.70.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pathos->sagemaker) (0.70.17)\n",
      "Requirement already satisfied: click>=7.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from uvicorn->sagemaker) (8.1.8)\n",
      "Requirement already satisfied: h11>=0.8 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from uvicorn->sagemaker) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.0.0->sagemaker-core<2.0.0,>=1.0.17->sagemaker) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pydantic<3.0.0,>=2.0.0->sagemaker-core<2.0.0,>=1.0.17->sagemaker) (2.27.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from rich<14.0.0,>=13.0.0->sagemaker-core<2.0.0,>=1.0.17->sagemaker) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from rich<14.0.0,>=13.0.0->sagemaker-core<2.0.0,>=1.0.17->sagemaker) (2.19.1)\n",
      "Requirement already satisfied: anyio<5,>=3.6.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from starlette<0.46.0,>=0.40.0->fastapi->sagemaker) (4.8.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from anyio<5,>=3.6.2->starlette<0.46.0,>=0.40.0->fastapi->sagemaker) (1.2.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from anyio<5,>=3.6.2->starlette<0.46.0,>=0.40.0->fastapi->sagemaker) (1.3.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=13.0.0->sagemaker-core<2.0.0,>=1.0.17->sagemaker) (0.1.2)\n",
      "Downloading sagemaker-2.242.0-py3-none-any.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m81.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sagemaker\n",
      "  Attempting uninstall: sagemaker\n",
      "    Found existing installation: sagemaker 2.241.0\n",
      "    Uninstalling sagemaker-2.241.0:\n",
      "      Successfully uninstalled sagemaker-2.241.0\n",
      "Successfully installed sagemaker-2.242.0\n"
     ]
    }
   ],
   "source": [
    "# ## Update sagemaker python sdk version\n",
    "!pip install -U sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4fc2fef-6ba1-4df5-9c19-169f2de789d6",
   "metadata": {},
   "source": [
    "## Set model, Code and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25d3e133-95da-4751-bffd-71f4c1aa9a2b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/pydantic/_internal/_fields.py:192: UserWarning: Field name \"json\" in \"MonitoringDatasetFormat\" shadows an attribute in parent \"Base\"\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[03/18/25 08:29:40] </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Found credentials from IAM Role:                                   <a href=\"file:///home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/botocore/credentials.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">credentials.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/botocore/credentials.py#1132\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1132</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         BaseNotebookInstanceEc2InstanceRole                                <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                   </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[03/18/25 08:29:40]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Found credentials from IAM Role:                                   \u001b]8;id=505356;file:///home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/botocore/credentials.py\u001b\\\u001b[2mcredentials.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=779687;file:///home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/botocore/credentials.py#1132\u001b\\\u001b[2m1132\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         BaseNotebookInstanceEc2InstanceRole                                \u001b[2m                   \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[03/18/25 08:29:41] </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Found credentials from IAM Role:                                   <a href=\"file:///home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/botocore/credentials.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">credentials.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/botocore/credentials.py#1132\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1132</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         BaseNotebookInstanceEc2InstanceRole                                <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                   </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[03/18/25 08:29:41]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Found credentials from IAM Role:                                   \u001b]8;id=734704;file:///home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/botocore/credentials.py\u001b\\\u001b[2mcredentials.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=501169;file:///home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/botocore/credentials.py#1132\u001b\\\u001b[2m1132\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         BaseNotebookInstanceEc2InstanceRole                                \u001b[2m                   \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Found credentials from IAM Role:                                   <a href=\"file:///home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/botocore/credentials.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">credentials.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/botocore/credentials.py#1132\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1132</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         BaseNotebookInstanceEc2InstanceRole                                <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                   </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Found credentials from IAM Role:                                   \u001b]8;id=594835;file:///home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/botocore/credentials.py\u001b\\\u001b[2mcredentials.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=921921;file:///home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/botocore/credentials.py#1132\u001b\\\u001b[2m1132\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         BaseNotebookInstanceEc2InstanceRole                                \u001b[2m                   \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker_default_bucket: sagemaker-us-east-1-596899493901\n",
      "sagemaker_region: us-east-1\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "sagemaker_default_bucket = sess.default_bucket()\n",
    "region = sess.boto_session.region_name\n",
    "print(\"sagemaker_default_bucket:\", sagemaker_default_bucket)\n",
    "print(\"sagemaker_region:\", region)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3da8a94",
   "metadata": {},
   "source": [
    "## upload pretrain models to s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb1a1a16-f1d7-4f25-bf25-68640f3396a8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting huggingface_hub\n",
      "  Downloading huggingface_hub-0.29.3-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface_hub) (3.17.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface_hub) (2025.2.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface_hub) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface_hub) (6.0.2)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface_hub) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface_hub) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface_hub) (4.12.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from packaging>=20.9->huggingface_hub) (3.2.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->huggingface_hub) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->huggingface_hub) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->huggingface_hub) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->huggingface_hub) (2025.1.31)\n",
      "Downloading huggingface_hub-0.29.3-py3-none-any.whl (468 kB)\n",
      "Installing collected packages: huggingface_hub\n",
      "Successfully installed huggingface_hub-0.29.3\n"
     ]
    }
   ],
   "source": [
    "!pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d471f6ad-47a5-4a56-a752-541373de9a13",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce522d06f2664d4bb9fa488c6431a95c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 13 files:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2be7e6eadb1149a7819532b39a63126a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/119 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8a9a51649b041d9926e63cac0710728",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fe2493fad1c4d5193a8877a4a226b78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/632 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bef2ef6dd8154be28a4ad10c47990535",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fbd777548ae40db8f01ef2b374cdd70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56957be1888343f6bd799a68461d3843",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "LICENSE:   0%|          | 0.00/13.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ae0ea12e54841849d53f1c853eac943",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/6.86k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ad929701cec4240b8beaa3529d52bb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       ".gitattributes:   0%|          | 0.00/1.52k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c49f710f6684ee4b9bc19dfb472cd3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/793 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f6a0f7ab83a435da2480fa97f515266",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00002-of-00002.bin:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4fdcc545a79422dab9fa814e059cdec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.37M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3fc41bea9c9451eb6fb88a4a3c1cc8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00001-of-00002.bin:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5a64e970edd449b9d70df21aedec93b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Code language: python\n",
    "from huggingface_hub import snapshot_download\n",
    "from pathlib import Path\n",
    "\n",
    "local_cache_path = Path(\"./deepseek_coder\")\n",
    "local_cache_path.mkdir(exist_ok=True)\n",
    "\n",
    "model_name = \"deepseek-ai/deepseek-coder-6.7b-base\"\n",
    "\n",
    "# Only download pytorch checkpoint files\n",
    "allow_patterns = [\"*\"]\n",
    "\n",
    "model_download_path = snapshot_download(\n",
    "    repo_id=model_name,\n",
    "    cache_dir=local_cache_path,\n",
    "    allow_patterns=allow_patterns,\n",
    ")\n",
    "model_snapshot_path = list(local_cache_path.glob(\"**/snapshots/*\"))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "372a7552-e0be-4db5-8fb2-145e0b8d0bfc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: deepseek_coder/models--deepseek-ai--deepseek-coder-6.7b-base/snapshots/ce2207a8bfef3ee92bd7dd4cc31c52cfa0046912/README.md to s3://sagemaker-us-east-1-596899493901/Foundation-Models/deepseek_coder/README.md\n",
      "upload: deepseek_coder/models--deepseek-ai--deepseek-coder-6.7b-base/snapshots/ce2207a8bfef3ee92bd7dd4cc31c52cfa0046912/generation_config.json to s3://sagemaker-us-east-1-596899493901/Foundation-Models/deepseek_coder/generation_config.json\n",
      "upload: deepseek_coder/models--deepseek-ai--deepseek-coder-6.7b-base/snapshots/ce2207a8bfef3ee92bd7dd4cc31c52cfa0046912/.gitattributes to s3://sagemaker-us-east-1-596899493901/Foundation-Models/deepseek_coder/.gitattributes\n",
      "upload: deepseek_coder/models--deepseek-ai--deepseek-coder-6.7b-base/snapshots/ce2207a8bfef3ee92bd7dd4cc31c52cfa0046912/LICENSE to s3://sagemaker-us-east-1-596899493901/Foundation-Models/deepseek_coder/LICENSE\n",
      "upload: deepseek_coder/models--deepseek-ai--deepseek-coder-6.7b-base/snapshots/ce2207a8bfef3ee92bd7dd4cc31c52cfa0046912/config.json to s3://sagemaker-us-east-1-596899493901/Foundation-Models/deepseek_coder/config.json\n",
      "upload: deepseek_coder/models--deepseek-ai--deepseek-coder-6.7b-base/snapshots/ce2207a8bfef3ee92bd7dd4cc31c52cfa0046912/model.safetensors.index.json to s3://sagemaker-us-east-1-596899493901/Foundation-Models/deepseek_coder/model.safetensors.index.json\n",
      "upload: deepseek_coder/models--deepseek-ai--deepseek-coder-6.7b-base/snapshots/ce2207a8bfef3ee92bd7dd4cc31c52cfa0046912/pytorch_model.bin.index.json to s3://sagemaker-us-east-1-596899493901/Foundation-Models/deepseek_coder/pytorch_model.bin.index.json\n",
      "upload: deepseek_coder/models--deepseek-ai--deepseek-coder-6.7b-base/snapshots/ce2207a8bfef3ee92bd7dd4cc31c52cfa0046912/tokenizer.json to s3://sagemaker-us-east-1-596899493901/Foundation-Models/deepseek_coder/tokenizer.json\n",
      "upload: deepseek_coder/models--deepseek-ai--deepseek-coder-6.7b-base/snapshots/ce2207a8bfef3ee92bd7dd4cc31c52cfa0046912/tokenizer_config.json to s3://sagemaker-us-east-1-596899493901/Foundation-Models/deepseek_coder/tokenizer_config.json\n",
      "upload: deepseek_coder/models--deepseek-ai--deepseek-coder-6.7b-base/snapshots/ce2207a8bfef3ee92bd7dd4cc31c52cfa0046912/model-00002-of-00002.safetensors to s3://sagemaker-us-east-1-596899493901/Foundation-Models/deepseek_coder/model-00002-of-00002.safetensors\n",
      "upload: deepseek_coder/models--deepseek-ai--deepseek-coder-6.7b-base/snapshots/ce2207a8bfef3ee92bd7dd4cc31c52cfa0046912/pytorch_model-00002-of-00002.bin to s3://sagemaker-us-east-1-596899493901/Foundation-Models/deepseek_coder/pytorch_model-00002-of-00002.bin\n",
      "upload: deepseek_coder/models--deepseek-ai--deepseek-coder-6.7b-base/snapshots/ce2207a8bfef3ee92bd7dd4cc31c52cfa0046912/model-00001-of-00002.safetensors to s3://sagemaker-us-east-1-596899493901/Foundation-Models/deepseek_coder/model-00001-of-00002.safetensors\n",
      "upload: deepseek_coder/models--deepseek-ai--deepseek-coder-6.7b-base/snapshots/ce2207a8bfef3ee92bd7dd4cc31c52cfa0046912/pytorch_model-00001-of-00002.bin to s3://sagemaker-us-east-1-596899493901/Foundation-Models/deepseek_coder/pytorch_model-00001-of-00002.bin\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp {model_snapshot_path} s3://{sagemaker_default_bucket}/Foundation-Models/deepseek_coder --recursive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11311c58",
   "metadata": {},
   "source": [
    "## Setup for wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a09b3ad4-909e-48ce-8823-bc600d563dcd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wandb\n",
      "  Downloading wandb-0.19.8-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from wandb) (8.1.8)\n",
      "Collecting docker-pycreds>=0.4.0 (from wandb)\n",
      "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting gitpython!=3.1.29,>=1.0.0 (from wandb)\n",
      "  Using cached GitPython-3.1.44-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: platformdirs in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from wandb) (4.3.6)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from wandb) (5.29.3)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from wandb) (6.1.1)\n",
      "Requirement already satisfied: pydantic<3,>=2.6 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from wandb) (2.10.6)\n",
      "Requirement already satisfied: pyyaml in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from wandb) (6.0.2)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from wandb) (2.32.3)\n",
      "Collecting sentry-sdk>=2.0.0 (from wandb)\n",
      "  Downloading sentry_sdk-2.23.1-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Collecting setproctitle (from wandb)\n",
      "  Downloading setproctitle-1.3.5-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from wandb) (75.8.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from wandb) (4.12.2)\n",
      "Requirement already satisfied: six>=1.4.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb)\n",
      "  Using cached gitdb-4.0.12-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pydantic<3,>=2.6->wandb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pydantic<3,>=2.6->wandb) (2.27.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2025.1.31)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb)\n",
      "  Using cached smmap-5.0.2-py3-none-any.whl.metadata (4.3 kB)\n",
      "Downloading wandb-0.19.8-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (20.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.8/20.8 MB\u001b[0m \u001b[31m156.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Using cached GitPython-3.1.44-py3-none-any.whl (207 kB)\n",
      "Downloading sentry_sdk-2.23.1-py2.py3-none-any.whl (336 kB)\n",
      "Downloading setproctitle-1.3.5-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
      "Using cached gitdb-4.0.12-py3-none-any.whl (62 kB)\n",
      "Using cached smmap-5.0.2-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, gitpython, wandb\n",
      "Successfully installed docker-pycreds-0.4.0 gitdb-4.0.12 gitpython-3.1.44 sentry-sdk-2.23.1 setproctitle-1.3.5 smmap-5.0.2 wandb-0.19.8\n"
     ]
    }
   ],
   "source": [
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d3bafe1-097f-410b-9919-9e402887f398",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[03/18/25 08:47:26] </span><span style=\"color: #d70000; text-decoration-color: #d70000; font-weight: bold\">ERROR   </span> Failed to detect the name of this notebook, you can set it manually     <a href=\"file:///home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/wandb/jupyter.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">jupyter.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/wandb/jupyter.py#245\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">245</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         with the WANDB_NOTEBOOK_NAME environment variable to enable code        <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">              </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         saving.                                                                 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">              </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[03/18/25 08:47:26]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;215;0;0mERROR   \u001b[0m Failed to detect the name of this notebook, you can set it manually     \u001b]8;id=655096;file:///home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/wandb/jupyter.py\u001b\\\u001b[2mjupyter.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=817806;file:///home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/wandb/jupyter.py#245\u001b\\\u001b[2m245\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         with the WANDB_NOTEBOOK_NAME environment variable to enable code        \u001b[2m              \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         saving.                                                                 \u001b[2m              \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "  ········\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/ec2-user/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33m407383787\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a54ecc7",
   "metadata": {},
   "source": [
    "## Submit Training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b165769a-0949-493e-99e4-c627f5fecf98",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20250318102900\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[03/18/25 10:29:00] </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> SageMaker Python SDK will collect telemetry to help us better  <a href=\"file:///home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/telemetry/telemetry_logging.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">telemetry_logging.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/telemetry/telemetry_logging.py#91\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">91</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         understand our user's needs, diagnose issues, and deliver      <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                       </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         additional features.                                           <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                       </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         To opt out of telemetry, please disable via TelemetryOptOut    <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                       </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         parameter in SDK defaults config. For more information, refer  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                       </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         to                                                             <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                       </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #0069ff; text-decoration-color: #0069ff; text-decoration: underline\">https://sagemaker.readthedocs.io/en/stable/overview.html#confi</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                       </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #0069ff; text-decoration-color: #0069ff; text-decoration: underline\">guring-and-using-defaults-with-the-sagemaker-python-sdk.</span>       <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                       </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[03/18/25 10:29:00]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m SageMaker Python SDK will collect telemetry to help us better  \u001b]8;id=657008;file:///home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/telemetry/telemetry_logging.py\u001b\\\u001b[2mtelemetry_logging.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=362789;file:///home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/telemetry/telemetry_logging.py#91\u001b\\\u001b[2m91\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         understand our user's needs, diagnose issues, and deliver      \u001b[2m                       \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         additional features.                                           \u001b[2m                       \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         To opt out of telemetry, please disable via TelemetryOptOut    \u001b[2m                       \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         parameter in SDK defaults config. For more information, refer  \u001b[2m                       \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         to                                                             \u001b[2m                       \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[4;38;2;0;105;255mhttps://sagemaker.readthedocs.io/en/stable/overview.html#confi\u001b[0m \u001b[2m                       \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[4;38;2;0;105;255mguring-and-using-defaults-with-the-sagemaker-python-sdk.\u001b[0m       \u001b[2m                       \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[03/18/25 10:29:06] </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> image_uri is not presented, retrieving image_uri based on            <a href=\"file:///home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/image_uris.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">image_uris.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/image_uris.py#681\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">681</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         instance_type, framework etc.                                        <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                 </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[03/18/25 10:29:06]\u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m image_uri is not presented, retrieving image_uri based on            \u001b]8;id=288194;file:///home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/image_uris.py\u001b\\\u001b[2mimage_uris.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=507389;file:///home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/image_uris.py#681\u001b\\\u001b[2m681\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         instance_type, framework etc.                                        \u001b[2m                 \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #0069ff; text-decoration-color: #0069ff; font-weight: bold\">INFO    </span> Creating training-job with name:                                       <a href=\"file:///home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/session.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">session.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/session.py#1042\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1042</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         deepseek6-7B-finetune-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-03-18-10-29-00-527                          <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[1;38;2;0;105;255mINFO    \u001b[0m Creating training-job with name:                                       \u001b]8;id=408324;file:///home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/session.py\u001b\\\u001b[2msession.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=889400;file:///home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sagemaker/session.py#1042\u001b\\\u001b[2m1042\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         deepseek6-7B-finetune-\u001b[1;36m2025\u001b[0m-03-18-10-29-00-527                          \u001b[2m               \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-18 10:29:08 Starting - Starting the training job\n",
      "2025-03-18 10:29:08 Pending - Training job waiting for capacity............\n",
      "2025-03-18 10:30:42 Pending - Preparing the instances for training...........................\n",
      "2025-03-18 10:35:20 Downloading - Downloading input data...\n",
      "2025-03-18 10:35:45 Downloading - Downloading the training image...............\n",
      "2025-03-18 10:38:16 Training - Training image download completed. Training in progress.....\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/paramiko/pkey.py:100: CryptographyDeprecationWarning: TripleDES has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.TripleDES and will be removed from this module in 48.0.0.\n",
      "  \"cipher\": algorithms.TripleDES,\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/paramiko/transport.py:259: CryptographyDeprecationWarning: TripleDES has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.TripleDES and will be removed from this module in 48.0.0.\n",
      "  \"class\": algorithms.TripleDES,\u001b[0m\n",
      "\u001b[34m2025-03-18 10:39:14,862 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2025-03-18 10:39:14,976 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2025-03-18 10:39:14,985 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2025-03-18 10:39:14,986 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2025-03-18 10:39:16,996 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt\u001b[0m\n",
      "\u001b[34mIgnoring transformers: markers 'python_version < \"3.10\"' don't match your environment\u001b[0m\n",
      "\u001b[34mCollecting transformers!=4.46.*,!=4.47.*,!=4.48.0,<=4.49.0,>=4.41.2 (from -r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading transformers-4.49.0-py3-none-any.whl.metadata (44 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 44.0/44.0 kB 2.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting datasets<=3.2.0,>=2.16.0 (from -r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\u001b[0m\n",
      "\u001b[34mCollecting accelerate<=1.2.1,>=0.34.0 (from -r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[34mDownloading accelerate-1.2.1-py3-none-any.whl.metadata (19 kB)\u001b[0m\n",
      "\u001b[34mCollecting peft<=0.12.0,>=0.11.1 (from -r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mDownloading peft-0.12.0-py3-none-any.whl.metadata (13 kB)\u001b[0m\n",
      "\u001b[34mCollecting trl<=0.9.6,>=0.8.6 (from -r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mDownloading trl-0.9.6-py3-none-any.whl.metadata (12 kB)\u001b[0m\n",
      "\u001b[34mCollecting tokenizers<=0.21.0,>=0.19.0 (from -r requirements.txt (line 7))\u001b[0m\n",
      "\u001b[34mDownloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\u001b[0m\n",
      "\u001b[34mCollecting gradio<=5.21.0,>=4.38.0 (from -r requirements.txt (line 8))\u001b[0m\n",
      "\u001b[34mDownloading gradio-5.21.0-py3-none-any.whl.metadata (16 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 9)) (2.2.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 10)) (1.14.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: einops in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 11)) (0.8.0)\u001b[0m\n",
      "\u001b[34mCollecting sentencepiece (from -r requirements.txt (line 12))\u001b[0m\n",
      "\u001b[34mDownloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\u001b[0m\n",
      "\u001b[34mCollecting tiktoken (from -r requirements.txt (line 13))\u001b[0m\n",
      "\u001b[34mDownloading tiktoken-0.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 14)) (3.20.3)\u001b[0m\n",
      "\u001b[34mCollecting uvicorn (from -r requirements.txt (line 15))\u001b[0m\n",
      "\u001b[34mDownloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pydantic in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 16)) (2.8.2)\u001b[0m\n",
      "\u001b[34mCollecting fastapi (from -r requirements.txt (line 17))\u001b[0m\n",
      "\u001b[34mDownloading fastapi-0.115.11-py3-none-any.whl.metadata (27 kB)\u001b[0m\n",
      "\u001b[34mCollecting sse-starlette (from -r requirements.txt (line 18))\u001b[0m\n",
      "\u001b[34mDownloading sse_starlette-2.2.1-py3-none-any.whl.metadata (7.8 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: matplotlib>=3.7.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 19)) (3.9.1)\u001b[0m\n",
      "\u001b[34mCollecting fire (from -r requirements.txt (line 20))\u001b[0m\n",
      "\u001b[34mDownloading fire-0.7.0.tar.gz (87 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 87.2/87.2 kB 14.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 21)) (23.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 22)) (6.0.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy<2.0.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 23)) (1.26.4)\u001b[0m\n",
      "\u001b[34mCollecting av (from -r requirements.txt (line 24))\u001b[0m\n",
      "\u001b[34mDownloading av-14.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.6 kB)\u001b[0m\n",
      "\u001b[34mCollecting librosa (from -r requirements.txt (line 25))\u001b[0m\n",
      "\u001b[34mDownloading librosa-0.11.0-py3-none-any.whl.metadata (8.7 kB)\u001b[0m\n",
      "\u001b[34mCollecting tyro<0.9.0 (from -r requirements.txt (line 26))\u001b[0m\n",
      "\u001b[34mDownloading tyro-0.8.14-py3-none-any.whl.metadata (8.4 kB)\u001b[0m\n",
      "\u001b[34mCollecting deepspeed==0.15.4 (from -r requirements.txt (line 27))\u001b[0m\n",
      "\u001b[34mDownloading deepspeed-0.15.4.tar.gz (1.4 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.4/1.4 MB 99.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCollecting wandb (from -r requirements.txt (line 28))\u001b[0m\n",
      "\u001b[34mDownloading wandb-0.19.8-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\u001b[0m\n",
      "\u001b[34mCollecting bitsandbytes>=0.37.0 (from -r requirements.txt (line 29))\u001b[0m\n",
      "\u001b[34mDownloading bitsandbytes-0.45.3-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\u001b[0m\n",
      "\u001b[34mCollecting hjson (from deepspeed==0.15.4->-r requirements.txt (line 27))\u001b[0m\n",
      "\u001b[34mDownloading hjson-3.1.0-py3-none-any.whl.metadata (2.6 kB)\u001b[0m\n",
      "\u001b[34mCollecting msgpack (from deepspeed==0.15.4->-r requirements.txt (line 27))\u001b[0m\n",
      "\u001b[34mDownloading msgpack-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.4 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: ninja in /opt/conda/lib/python3.10/site-packages (from deepspeed==0.15.4->-r requirements.txt (line 27)) (1.11.1.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from deepspeed==0.15.4->-r requirements.txt (line 27)) (6.0.0)\u001b[0m\n",
      "\u001b[34mCollecting py-cpuinfo (from deepspeed==0.15.4->-r requirements.txt (line 27))\u001b[0m\n",
      "\u001b[34mDownloading py_cpuinfo-9.0.0-py3-none-any.whl.metadata (794 bytes)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from deepspeed==0.15.4->-r requirements.txt (line 27)) (2.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from deepspeed==0.15.4->-r requirements.txt (line 27)) (4.66.5)\u001b[0m\n",
      "\u001b[34mCollecting nvidia-ml-py (from deepspeed==0.15.4->-r requirements.txt (line 27))\u001b[0m\n",
      "\u001b[34mDownloading nvidia_ml_py-12.570.86-py3-none-any.whl.metadata (8.7 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers!=4.46.*,!=4.47.*,!=4.48.0,<=4.49.0,>=4.41.2->-r requirements.txt (line 2)) (3.15.4)\u001b[0m\n",
      "\u001b[34mCollecting huggingface-hub<1.0,>=0.26.0 (from transformers!=4.46.*,!=4.47.*,!=4.48.0,<=4.49.0,>=4.41.2->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.29.3-py3-none-any.whl.metadata (13 kB)\u001b[0m\n",
      "\u001b[34mCollecting regex!=2019.12.17 (from transformers!=4.46.*,!=4.47.*,!=4.48.0,<=4.49.0,>=4.41.2->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 40.5/40.5 kB 5.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers!=4.46.*,!=4.47.*,!=4.48.0,<=4.49.0,>=4.41.2->-r requirements.txt (line 2)) (2.32.3)\u001b[0m\n",
      "\u001b[34mCollecting safetensors>=0.4.1 (from transformers!=4.46.*,!=4.47.*,!=4.48.0,<=4.49.0,>=4.41.2->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets<=3.2.0,>=2.16.0->-r requirements.txt (line 3)) (17.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets<=3.2.0,>=2.16.0->-r requirements.txt (line 3)) (0.3.8)\u001b[0m\n",
      "\u001b[34mCollecting xxhash (from datasets<=3.2.0,>=2.16.0->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess<0.70.17 in /opt/conda/lib/python3.10/site-packages (from datasets<=3.2.0,>=2.16.0->-r requirements.txt (line 3)) (0.70.16)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets<=3.2.0,>=2.16.0->-r requirements.txt (line 3)) (2024.6.1)\u001b[0m\n",
      "\u001b[34mCollecting aiohttp (from datasets<=3.2.0,>=2.16.0->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading aiohttp-3.11.14-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\u001b[0m\n",
      "\u001b[34mCollecting aiofiles<24.0,>=22.0 (from gradio<=5.21.0,>=4.38.0->-r requirements.txt (line 8))\u001b[0m\n",
      "\u001b[34mDownloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\u001b[0m\n",
      "\u001b[34mCollecting anyio<5.0,>=3.0 (from gradio<=5.21.0,>=4.38.0->-r requirements.txt (line 8))\u001b[0m\n",
      "\u001b[34mDownloading anyio-4.9.0-py3-none-any.whl.metadata (4.7 kB)\u001b[0m\n",
      "\u001b[34mCollecting ffmpy (from gradio<=5.21.0,>=4.38.0->-r requirements.txt (line 8))\u001b[0m\n",
      "\u001b[34mDownloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\u001b[0m\n",
      "\u001b[34mCollecting gradio-client==1.7.2 (from gradio<=5.21.0,>=4.38.0->-r requirements.txt (line 8))\u001b[0m\n",
      "\u001b[34mDownloading gradio_client-1.7.2-py3-none-any.whl.metadata (7.1 kB)\u001b[0m\n",
      "\u001b[34mCollecting groovy~=0.1 (from gradio<=5.21.0,>=4.38.0->-r requirements.txt (line 8))\u001b[0m\n",
      "\u001b[34mDownloading groovy-0.1.2-py3-none-any.whl.metadata (6.1 kB)\u001b[0m\n",
      "\u001b[34mCollecting httpx>=0.24.1 (from gradio<=5.21.0,>=4.38.0->-r requirements.txt (line 8))\u001b[0m\n",
      "\u001b[34mDownloading httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jinja2<4.0 in /opt/conda/lib/python3.10/site-packages (from gradio<=5.21.0,>=4.38.0->-r requirements.txt (line 8)) (3.1.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: markupsafe~=2.0 in /opt/conda/lib/python3.10/site-packages (from gradio<=5.21.0,>=4.38.0->-r requirements.txt (line 8)) (2.1.5)\u001b[0m\n",
      "\u001b[34mCollecting orjson~=3.0 (from gradio<=5.21.0,>=4.38.0->-r requirements.txt (line 8))\u001b[0m\n",
      "\u001b[34mDownloading orjson-3.10.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (41 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 41.8/41.8 kB 6.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pillow<12.0,>=8.0 in /opt/conda/lib/python3.10/site-packages (from gradio<=5.21.0,>=4.38.0->-r requirements.txt (line 8)) (10.3.0)\u001b[0m\n",
      "\u001b[34mCollecting pydub (from gradio<=5.21.0,>=4.38.0->-r requirements.txt (line 8))\u001b[0m\n",
      "\u001b[34mDownloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\u001b[0m\n",
      "\u001b[34mCollecting python-multipart>=0.0.18 (from gradio<=5.21.0,>=4.38.0->-r requirements.txt (line 8))\u001b[0m\n",
      "\u001b[34mDownloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\u001b[0m\n",
      "\u001b[34mCollecting ruff>=0.9.3 (from gradio<=5.21.0,>=4.38.0->-r requirements.txt (line 8))\u001b[0m\n",
      "\u001b[34mDownloading ruff-0.11.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\u001b[0m\n",
      "\u001b[34mCollecting safehttpx<0.2.0,>=0.1.6 (from gradio<=5.21.0,>=4.38.0->-r requirements.txt (line 8))\u001b[0m\n",
      "\u001b[34mDownloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\u001b[0m\n",
      "\u001b[34mCollecting semantic-version~=2.0 (from gradio<=5.21.0,>=4.38.0->-r requirements.txt (line 8))\u001b[0m\n",
      "\u001b[34mDownloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\u001b[0m\n",
      "\u001b[34mCollecting starlette<1.0,>=0.40.0 (from gradio<=5.21.0,>=4.38.0->-r requirements.txt (line 8))\u001b[0m\n",
      "\u001b[34mDownloading starlette-0.46.1-py3-none-any.whl.metadata (6.2 kB)\u001b[0m\n",
      "\u001b[34mCollecting tomlkit<0.14.0,>=0.12.0 (from gradio<=5.21.0,>=4.38.0->-r requirements.txt (line 8))\u001b[0m\n",
      "\u001b[34mDownloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typer<1.0,>=0.12 in /opt/conda/lib/python3.10/site-packages (from gradio<=5.21.0,>=4.38.0->-r requirements.txt (line 8)) (0.12.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions~=4.0 in /opt/conda/lib/python3.10/site-packages (from gradio<=5.21.0,>=4.38.0->-r requirements.txt (line 8)) (4.12.2)\u001b[0m\n",
      "\u001b[34mCollecting websockets<16.0,>=10.0 (from gradio-client==1.7.2->gradio<=5.21.0,>=4.38.0->-r requirements.txt (line 8))\u001b[0m\n",
      "\u001b[34mDownloading websockets-15.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas>=2.0.0->-r requirements.txt (line 9)) (2.9.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=2.0.0->-r requirements.txt (line 9)) (2024.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas>=2.0.0->-r requirements.txt (line 9)) (2024.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: click>=7.0 in /opt/conda/lib/python3.10/site-packages (from uvicorn->-r requirements.txt (line 15)) (8.1.7)\u001b[0m\n",
      "\u001b[34mCollecting h11>=0.8 (from uvicorn->-r requirements.txt (line 15))\u001b[0m\n",
      "\u001b[34mDownloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic->-r requirements.txt (line 16)) (0.7.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pydantic-core==2.20.1 in /opt/conda/lib/python3.10/site-packages (from pydantic->-r requirements.txt (line 16)) (2.20.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 19)) (1.2.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 19)) (0.12.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 19)) (4.53.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 19)) (1.4.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 19)) (3.1.2)\u001b[0m\n",
      "\u001b[34mCollecting termcolor (from fire->-r requirements.txt (line 20))\u001b[0m\n",
      "\u001b[34mDownloading termcolor-2.5.0-py3-none-any.whl.metadata (6.1 kB)\u001b[0m\n",
      "\u001b[34mCollecting audioread>=2.1.9 (from librosa->-r requirements.txt (line 25))\u001b[0m\n",
      "\u001b[34mDownloading audioread-3.0.1-py3-none-any.whl.metadata (8.4 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numba>=0.51.0 in /opt/conda/lib/python3.10/site-packages (from librosa->-r requirements.txt (line 25)) (0.60.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scikit-learn>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from librosa->-r requirements.txt (line 25)) (1.5.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: joblib>=1.0 in /opt/conda/lib/python3.10/site-packages (from librosa->-r requirements.txt (line 25)) (1.4.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: decorator>=4.3.0 in /opt/conda/lib/python3.10/site-packages (from librosa->-r requirements.txt (line 25)) (5.1.1)\u001b[0m\n",
      "\u001b[34mCollecting soundfile>=0.12.1 (from librosa->-r requirements.txt (line 25))\u001b[0m\n",
      "\u001b[34mDownloading soundfile-0.13.1-py2.py3-none-manylinux_2_28_x86_64.whl.metadata (16 kB)\u001b[0m\n",
      "\u001b[34mCollecting pooch>=1.1 (from librosa->-r requirements.txt (line 25))\u001b[0m\n",
      "\u001b[34mDownloading pooch-1.8.2-py3-none-any.whl.metadata (10 kB)\u001b[0m\n",
      "\u001b[34mCollecting soxr>=0.3.2 (from librosa->-r requirements.txt (line 25))\u001b[0m\n",
      "\u001b[34mDownloading soxr-0.5.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\u001b[0m\n",
      "\u001b[34mCollecting lazy_loader>=0.1 (from librosa->-r requirements.txt (line 25))\u001b[0m\n",
      "\u001b[34mDownloading lazy_loader-0.4-py3-none-any.whl.metadata (7.6 kB)\u001b[0m\n",
      "\u001b[34mCollecting docstring-parser>=0.16 (from tyro<0.9.0->-r requirements.txt (line 26))\u001b[0m\n",
      "\u001b[34mDownloading docstring_parser-0.16-py3-none-any.whl.metadata (3.0 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: rich>=11.1.0 in /opt/conda/lib/python3.10/site-packages (from tyro<0.9.0->-r requirements.txt (line 26)) (13.7.1)\u001b[0m\n",
      "\u001b[34mCollecting shtab>=1.5.6 (from tyro<0.9.0->-r requirements.txt (line 26))\u001b[0m\n",
      "\u001b[34mDownloading shtab-1.7.1-py3-none-any.whl.metadata (7.3 kB)\u001b[0m\n",
      "\u001b[34mCollecting docker-pycreds>=0.4.0 (from wandb->-r requirements.txt (line 28))\u001b[0m\n",
      "\u001b[34mDownloading docker_pycreds-0.4.0-py2.py3-none-any.whl.metadata (1.8 kB)\u001b[0m\n",
      "\u001b[34mCollecting gitpython!=3.1.29,>=1.0.0 (from wandb->-r requirements.txt (line 28))\u001b[0m\n",
      "\u001b[34mDownloading GitPython-3.1.44-py3-none-any.whl.metadata (13 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: platformdirs in /opt/conda/lib/python3.10/site-packages (from wandb->-r requirements.txt (line 28)) (4.2.2)\u001b[0m\n",
      "\u001b[34mCollecting sentry-sdk>=2.0.0 (from wandb->-r requirements.txt (line 28))\u001b[0m\n",
      "\u001b[34mDownloading sentry_sdk-2.23.1-py2.py3-none-any.whl.metadata (10 kB)\u001b[0m\n",
      "\u001b[34mCollecting setproctitle (from wandb->-r requirements.txt (line 28))\u001b[0m\n",
      "\u001b[34mDownloading setproctitle-1.3.5-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from wandb->-r requirements.txt (line 28)) (72.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio<5.0,>=3.0->gradio<=5.21.0,>=4.38.0->-r requirements.txt (line 8)) (1.2.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.10/site-packages (from anyio<5.0,>=3.0->gradio<=5.21.0,>=4.38.0->-r requirements.txt (line 8)) (3.7)\u001b[0m\n",
      "\u001b[34mCollecting sniffio>=1.1 (from anyio<5.0,>=3.0->gradio<=5.21.0,>=4.38.0->-r requirements.txt (line 8))\u001b[0m\n",
      "\u001b[34mDownloading sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb->-r requirements.txt (line 28)) (1.16.0)\u001b[0m\n",
      "\u001b[34mCollecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets<=3.2.0,>=2.16.0->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\u001b[0m\n",
      "\u001b[34mCollecting aiosignal>=1.1.2 (from aiohttp->datasets<=3.2.0,>=2.16.0->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\u001b[0m\n",
      "\u001b[34mCollecting async-timeout<6.0,>=4.0 (from aiohttp->datasets<=3.2.0,>=2.16.0->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading async_timeout-5.0.1-py3-none-any.whl.metadata (5.1 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets<=3.2.0,>=2.16.0->-r requirements.txt (line 3)) (23.2.0)\u001b[0m\n",
      "\u001b[34mCollecting frozenlist>=1.1.1 (from aiohttp->datasets<=3.2.0,>=2.16.0->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading frozenlist-1.5.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\u001b[0m\n",
      "\u001b[34mCollecting multidict<7.0,>=4.5 (from aiohttp->datasets<=3.2.0,>=2.16.0->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading multidict-6.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\u001b[0m\n",
      "\u001b[34mCollecting propcache>=0.2.0 (from aiohttp->datasets<=3.2.0,>=2.16.0->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading propcache-0.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\u001b[0m\n",
      "\u001b[34mCollecting yarl<2.0,>=1.17.0 (from aiohttp->datasets<=3.2.0,>=2.16.0->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading yarl-1.18.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (69 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 69.2/69.2 kB 10.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb->-r requirements.txt (line 28))\u001b[0m\n",
      "\u001b[34mDownloading gitdb-4.0.12-py3-none-any.whl.metadata (1.2 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx>=0.24.1->gradio<=5.21.0,>=4.38.0->-r requirements.txt (line 8)) (2024.7.4)\u001b[0m\n",
      "\u001b[34mCollecting httpcore==1.* (from httpx>=0.24.1->gradio<=5.21.0,>=4.38.0->-r requirements.txt (line 8))\u001b[0m\n",
      "\u001b[34mDownloading httpcore-1.0.7-py3-none-any.whl.metadata (21 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /opt/conda/lib/python3.10/site-packages (from numba>=0.51.0->librosa->-r requirements.txt (line 25)) (0.43.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers!=4.46.*,!=4.47.*,!=4.48.0,<=4.49.0,>=4.41.2->-r requirements.txt (line 2)) (3.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers!=4.46.*,!=4.47.*,!=4.48.0,<=4.49.0,>=4.41.2->-r requirements.txt (line 2)) (1.26.19)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro<0.9.0->-r requirements.txt (line 26)) (3.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro<0.9.0->-r requirements.txt (line 26)) (2.18.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: threadpoolctl>=3.1.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=1.1.0->librosa->-r requirements.txt (line 25)) (3.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: cffi>=1.0 in /opt/conda/lib/python3.10/site-packages (from soundfile>=0.12.1->librosa->-r requirements.txt (line 25)) (1.15.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed==0.15.4->-r requirements.txt (line 27)) (1.13.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->deepspeed==0.15.4->-r requirements.txt (line 27)) (3.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: shellingham>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from typer<1.0,>=0.12->gradio<=5.21.0,>=4.38.0->-r requirements.txt (line 8)) (1.5.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pycparser in /opt/conda/lib/python3.10/site-packages (from cffi>=1.0->soundfile>=0.12.1->librosa->-r requirements.txt (line 25)) (2.21)\u001b[0m\n",
      "\u001b[34mCollecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb->-r requirements.txt (line 28))\u001b[0m\n",
      "\u001b[34mDownloading smmap-5.0.2-py3-none-any.whl.metadata (4.3 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro<0.9.0->-r requirements.txt (line 26)) (0.1.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->deepspeed==0.15.4->-r requirements.txt (line 27)) (1.3.0)\u001b[0m\n",
      "\u001b[34mDownloading transformers-4.49.0-py3-none-any.whl (10.0 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 10.0/10.0 MB 158.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading datasets-3.2.0-py3-none-any.whl (480 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 480.6/480.6 kB 82.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading accelerate-1.2.1-py3-none-any.whl (336 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 336.4/336.4 kB 70.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading peft-0.12.0-py3-none-any.whl (296 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 296.4/296.4 kB 62.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading trl-0.9.6-py3-none-any.whl (245 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 245.8/245.8 kB 58.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.0/3.0 MB 165.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading gradio-5.21.0-py3-none-any.whl (46.2 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 46.2/46.2 MB 67.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading gradio_client-1.7.2-py3-none-any.whl (322 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 322.1/322.1 kB 55.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 133.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading tiktoken-0.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/1.2 MB 128.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 62.3/62.3 kB 18.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading fastapi-0.115.11-py3-none-any.whl (94 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 94.9/94.9 kB 24.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading sse_starlette-2.2.1-py3-none-any.whl (10 kB)\u001b[0m\n",
      "\u001b[34mDownloading av-14.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.8 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 38.8/38.8 MB 81.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading librosa-0.11.0-py3-none-any.whl (260 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 260.7/260.7 kB 57.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading tyro-0.8.14-py3-none-any.whl (109 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 109.8/109.8 kB 29.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading wandb-0.19.8-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (20.8 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 20.8/20.8 MB 124.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading bitsandbytes-0.45.3-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.1/76.1 MB 42.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\u001b[0m\n",
      "\u001b[34mDownloading anyio-4.9.0-py3-none-any.whl (100 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100.9/100.9 kB 22.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading audioread-3.0.1-py3-none-any.whl (23 kB)\u001b[0m\n",
      "\u001b[34mDownloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\u001b[0m\n",
      "\u001b[34mDownloading docstring_parser-0.16-py3-none-any.whl (36 kB)\u001b[0m\n",
      "\u001b[34mDownloading aiohttp-3.11.14-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.6/1.6 MB 140.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading GitPython-3.1.44-py3-none-any.whl (207 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 207.6/207.6 kB 39.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading groovy-0.1.2-py3-none-any.whl (14 kB)\u001b[0m\n",
      "\u001b[34mDownloading h11-0.14.0-py3-none-any.whl (58 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 58.3/58.3 kB 16.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading httpx-0.28.1-py3-none-any.whl (73 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 73.5/73.5 kB 20.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading httpcore-1.0.7-py3-none-any.whl (78 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 78.6/78.6 kB 20.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.29.3-py3-none-any.whl (468 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 469.0/469.0 kB 78.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading lazy_loader-0.4-py3-none-any.whl (12 kB)\u001b[0m\n",
      "\u001b[34mDownloading msgpack-1.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (378 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 378.0/378.0 kB 61.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading orjson-3.10.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 130.3/130.3 kB 30.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading pooch-1.8.2-py3-none-any.whl (64 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 64.6/64.6 kB 17.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\u001b[0m\n",
      "\u001b[34mDownloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (781 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 781.7/781.7 kB 106.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading ruff-0.11.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 11.3/11.3 MB 166.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\u001b[0m\n",
      "\u001b[34mDownloading safetensors-0.5.3-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (471 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 471.6/471.6 kB 81.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\u001b[0m\n",
      "\u001b[34mDownloading sentry_sdk-2.23.1-py2.py3-none-any.whl (336 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 336.3/336.3 kB 66.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading shtab-1.7.1-py3-none-any.whl (14 kB)\u001b[0m\n",
      "\u001b[34mDownloading soundfile-0.13.1-py2.py3-none-manylinux_2_28_x86_64.whl (1.3 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 114.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading soxr-0.5.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (252 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 252.8/252.8 kB 58.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading starlette-0.46.1-py3-none-any.whl (71 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 72.0/72.0 kB 20.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\u001b[0m\n",
      "\u001b[34mDownloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\u001b[0m\n",
      "\u001b[34mDownloading hjson-3.1.0-py3-none-any.whl (54 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 54.0/54.0 kB 14.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading nvidia_ml_py-12.570.86-py3-none-any.whl (44 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 44.4/44.4 kB 12.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\u001b[0m\n",
      "\u001b[34mDownloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\u001b[0m\n",
      "\u001b[34mDownloading setproctitle-1.3.5-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\u001b[0m\n",
      "\u001b[34mDownloading termcolor-2.5.0-py3-none-any.whl (7.8 kB)\u001b[0m\n",
      "\u001b[34mDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 194.1/194.1 kB 46.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\u001b[0m\n",
      "\u001b[34mDownloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\u001b[0m\n",
      "\u001b[34mDownloading async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\u001b[0m\n",
      "\u001b[34mDownloading frozenlist-1.5.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (241 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 241.9/241.9 kB 57.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading gitdb-4.0.12-py3-none-any.whl (62 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 62.8/62.8 kB 18.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading multidict-6.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 129.8/129.8 kB 35.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading propcache-0.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (205 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 205.4/205.4 kB 44.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading sniffio-1.3.1-py3-none-any.whl (10 kB)\u001b[0m\n",
      "\u001b[34mDownloading websockets-15.0.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (181 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 181.6/181.6 kB 40.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading yarl-1.18.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (319 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 319.7/319.7 kB 67.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading smmap-5.0.2-py3-none-any.whl (24 kB)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: deepspeed, fire\u001b[0m\n",
      "\u001b[34mBuilding wheel for deepspeed (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for deepspeed (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for deepspeed: filename=deepspeed-0.15.4-py3-none-any.whl size=1527840 sha256=cdd3d72d332506dedff440729054780816a1ffc3c8f051baa99072534d75c6cb\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/74/bc/b6/836d7c3e3093e25502fa9248e0be9e943db245f2806ba1cd19\u001b[0m\n",
      "\u001b[34mBuilding wheel for fire (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for fire (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for fire: filename=fire-0.7.0-py3-none-any.whl size=114250 sha256=4764fe99ef9ddbe94142a6fa0e85d98a5cefe0c14cd8565fe3608c5b46094cf5\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/19/39/2f/2d3cadc408a8804103f1c34ddd4b9f6a93497b11fa96fe738e\u001b[0m\n",
      "\u001b[34mSuccessfully built deepspeed fire\u001b[0m\n",
      "\u001b[34mInstalling collected packages: sentencepiece, pydub, py-cpuinfo, nvidia-ml-py, hjson, xxhash, websockets, tomlkit, termcolor, soxr, sniffio, smmap, shtab, setproctitle, sentry-sdk, semantic-version, safetensors, ruff, regex, python-multipart, propcache, orjson, multidict, msgpack, lazy_loader, h11, groovy, frozenlist, ffmpy, docstring-parser, docker-pycreds, av, audioread, async-timeout, aiohappyeyeballs, aiofiles, yarl, uvicorn, tiktoken, soundfile, pooch, huggingface-hub, httpcore, gitdb, fire, anyio, aiosignal, tyro, tokenizers, starlette, librosa, httpx, gitpython, deepspeed, bitsandbytes, aiohttp, accelerate, wandb, transformers, sse-starlette, safehttpx, gradio-client, fastapi, peft, gradio, datasets, trl\u001b[0m\n",
      "\u001b[34mAttempting uninstall: accelerate\u001b[0m\n",
      "\u001b[34mFound existing installation: accelerate 0.22.0\u001b[0m\n",
      "\u001b[34mUninstalling accelerate-0.22.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled accelerate-0.22.0\u001b[0m\n",
      "\u001b[34mSuccessfully installed accelerate-1.2.1 aiofiles-23.2.1 aiohappyeyeballs-2.6.1 aiohttp-3.11.14 aiosignal-1.3.2 anyio-4.9.0 async-timeout-5.0.1 audioread-3.0.1 av-14.2.0 bitsandbytes-0.45.3 datasets-3.2.0 deepspeed-0.15.4 docker-pycreds-0.4.0 docstring-parser-0.16 fastapi-0.115.11 ffmpy-0.5.0 fire-0.7.0 frozenlist-1.5.0 gitdb-4.0.12 gitpython-3.1.44 gradio-5.21.0 gradio-client-1.7.2 groovy-0.1.2 h11-0.14.0 hjson-3.1.0 httpcore-1.0.7 httpx-0.28.1 huggingface-hub-0.29.3 lazy_loader-0.4 librosa-0.11.0 msgpack-1.1.0 multidict-6.2.0 nvidia-ml-py-12.570.86 orjson-3.10.15 peft-0.12.0 pooch-1.8.2 propcache-0.3.0 py-cpuinfo-9.0.0 pydub-0.25.1 python-multipart-0.0.20 regex-2024.11.6 ruff-0.11.0 safehttpx-0.1.6 safetensors-0.5.3 semantic-version-2.10.0 sentencepiece-0.2.0 sentry-sdk-2.23.1 setproctitle-1.3.5 shtab-1.7.1 smmap-5.0.2 sniffio-1.3.1 soundfile-0.13.1 soxr-0.5.0.post1 sse-starlette-2.2.1 starlette-0.46.1 termcolor-2.5.0 tiktoken-0.9.0 tokenizers-0.21.0 tomlkit-0.13.2 transformers-4.49.0 trl-0.9.6 tyro-0.8.14 uvicorn-0.34.0 wandb-0.19.8 websockets-15.0.1 xxhash-3.5.0 yarl-1.18.3\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip is available: 24.1.2 -> 25.0.1\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m2025-03-18 10:39:57,641 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2025-03-18 10:39:57,641 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2025-03-18 10:39:57,785 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2025-03-18 10:39:57,914 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2025-03-18 10:39:58,038 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2025-03-18 10:39:58,048 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {},\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.p4d.24xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {},\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {},\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.p4d.24xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": false,\n",
      "    \"is_smddprun_installed\": true,\n",
      "    \"job_name\": \"deepseek6-7B-finetune-2025-03-18-10-29-00-527\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-596899493901/deepseek6-7B-finetune-2025-03-18-10-29-00-527/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"entry\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 96,\n",
      "    \"num_gpus\": 8,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.p4d.24xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p4d.24xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"entry.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=entry.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.p4d.24xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=entry\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=96\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-596899493901/deepseek6-7B-finetune-2025-03-18-10-29-00-527/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.p4d.24xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":false,\"is_smddprun_installed\":true,\"job_name\":\"deepseek6-7B-finetune-2025-03-18-10-29-00-527\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-596899493901/deepseek6-7B-finetune-2025-03-18-10-29-00-527/source/sourcedir.tar.gz\",\"module_name\":\"entry\",\"network_interface_name\":\"eth0\",\"num_cpus\":96,\"num_gpus\":8,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"entry.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python310.zip:/opt/conda/lib/python3.10:/opt/conda/lib/python3.10/lib-dynload:/opt/conda/lib/python3.10/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 entry.py\u001b[0m\n",
      "\u001b[34m2025-03-18 10:39:58,049 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker Debugger as it is not installed.\u001b[0m\n",
      "\u001b[34m2025-03-18 10:39:58,049 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34m*****************start cp pretrain model*****************************\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-596899493901/Foundation-Models/deepseek_coder/.locks/models--deepseek-ai--deepseek-coder-6.7b-base/d7129f354c81de8ae5727684a55e168b684144b6.lock /tmp/pretrain_model/.locks/models--deepseek-ai--deepseek-coder-6.7b-base/d7129f354c81de8ae5727684a55e168b684144b6.lock\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-596899493901/Foundation-Models/deepseek_coder/README.md /tmp/pretrain_model/README.md\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-596899493901/Foundation-Models/deepseek_coder/.gitattributes /tmp/pretrain_model/.gitattributes\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-596899493901/Foundation-Models/deepseek_coder/.locks/models--deepseek-ai--deepseek-coder-6.7b-base/d4338e87a01134a05058c25623116fb6678a90c7.lock /tmp/pretrain_model/.locks/models--deepseek-ai--deepseek-coder-6.7b-base/d4338e87a01134a05058c25623116fb6678a90c7.lock\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-596899493901/Foundation-Models/deepseek_coder/.locks/models--deepseek-ai--deepseek-coder-6.7b-base/b2e101c170f6f5147d34b1d55dbd755afaa80680272a937514d7b0bc1319f282.lock /tmp/pretrain_model/.locks/models--deepseek-ai--deepseek-coder-6.7b-base/b2e101c170f6f5147d34b1d55dbd755afaa80680272a937514d7b0bc1319f282.lock\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-596899493901/Foundation-Models/deepseek_coder/.locks/models--deepseek-ai--deepseek-coder-6.7b-base/340b041b4df26ed5685d5a8ad8654dccdac838c2.lock /tmp/pretrain_model/.locks/models--deepseek-ai--deepseek-coder-6.7b-base/340b041b4df26ed5685d5a8ad8654dccdac838c2.lock\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-596899493901/Foundation-Models/deepseek_coder/.locks/models--deepseek-ai--deepseek-coder-6.7b-base/03ed4bdaa9d6707eefe2678470686f040dea73c9.lock /tmp/pretrain_model/.locks/models--deepseek-ai--deepseek-coder-6.7b-base/03ed4bdaa9d6707eefe2678470686f040dea73c9.lock\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-596899493901/Foundation-Models/deepseek_coder/.locks/models--deepseek-ai--deepseek-coder-6.7b-base/0dfa4268afc2cf45515dec553772d7ca79f79762.lock /tmp/pretrain_model/.locks/models--deepseek-ai--deepseek-coder-6.7b-base/0dfa4268afc2cf45515dec553772d7ca79f79762.lock\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-596899493901/Foundation-Models/deepseek_coder/.locks/models--deepseek-ai--deepseek-coder-6.7b-base/726f9234cc8761581e25ac3555e69683b7ef02a1a913358398ec296c8a6bac58.lock /tmp/pretrain_model/.locks/models--deepseek-ai--deepseek-coder-6.7b-base/726f9234cc8761581e25ac3555e69683b7ef02a1a913358398ec296c8a6bac58.lock\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-596899493901/Foundation-Models/deepseek_coder/.locks/models--deepseek-ai--deepseek-coder-6.7b-base/78a5bffe3094f3c3b91b559872166dae80198032.lock /tmp/pretrain_model/.locks/models--deepseek-ai--deepseek-coder-6.7b-base/78a5bffe3094f3c3b91b559872166dae80198032.lock\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-596899493901/Foundation-Models/deepseek_coder/.locks/models--deepseek-ai--deepseek-coder-6.7b-base/ac09290cb2947183ab7f19d7477511dad3600a7a.lock /tmp/pretrain_model/.locks/models--deepseek-ai--deepseek-coder-6.7b-base/ac09290cb2947183ab7f19d7477511dad3600a7a.lock\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-596899493901/Foundation-Models/deepseek_coder/.locks/models--deepseek-ai--deepseek-coder-6.7b-base/aca644afc72b8f4e86910c5bd130b32d5e21c92120a820b085e69b0a3a44ac53.lock /tmp/pretrain_model/.locks/models--deepseek-ai--deepseek-coder-6.7b-base/aca644afc72b8f4e86910c5bd130b32d5e21c92120a820b085e69b0a3a44ac53.lock\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-596899493901/Foundation-Models/deepseek_coder/.locks/models--deepseek-ai--deepseek-coder-6.7b-base/a6344aac8c09253b3b630fb776ae94478aa0275b.lock /tmp/pretrain_model/.locks/models--deepseek-ai--deepseek-coder-6.7b-base/a6344aac8c09253b3b630fb776ae94478aa0275b.lock\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-596899493901/Foundation-Models/deepseek_coder/.locks/models--deepseek-ai--deepseek-coder-6.7b-base/ab70e63cbf14ce6861f1414e091a1fb608cd78c7.lock /tmp/pretrain_model/.locks/models--deepseek-ai--deepseek-coder-6.7b-base/ab70e63cbf14ce6861f1414e091a1fb608cd78c7.lock\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-596899493901/Foundation-Models/deepseek_coder/models--deepseek-ai--deepseek-coder-6.7b-base/snapshots/ce2207a8bfef3ee92bd7dd4cc31c52cfa0046912/generation_config.json /tmp/pretrain_model/models--deepseek-ai--deepseek-coder-6.7b-base/snapshots/ce2207a8bfef3ee92bd7dd4cc31c52cfa0046912/generation_config.json\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-596899493901/Foundation-Models/deepseek_coder/generation_config.json /tmp/pretrain_model/generation_config.json\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-596899493901/Foundation-Models/deepseek_coder/models--deepseek-ai--deepseek-coder-6.7b-base/snapshots/ce2207a8bfef3ee92bd7dd4cc31c52cfa0046912/pytorch_model.bin.index.json /tmp/pretrain_model/models--deepseek-ai--deepseek-coder-6.7b-base/snapshots/ce2207a8bfef3ee92bd7dd4cc31c52cfa0046912/pytorch_model.bin.index.json\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-596899493901/Foundation-Models/deepseek_coder/models--deepseek-ai--deepseek-coder-6.7b-base/blobs/78a5bffe3094f3c3b91b559872166dae80198032 /tmp/pretrain_model/models--deepseek-ai--deepseek-coder-6.7b-base/blobs/78a5bffe3094f3c3b91b559872166dae80198032\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-596899493901/Foundation-Models/deepseek_coder/.locks/models--deepseek-ai--deepseek-coder-6.7b-base/f8717011d6b4f4f0309d8ccd413ec0ea39e9c9b948dc4bd3c19e707d4b36e0f2.lock /tmp/pretrain_model/.locks/models--deepseek-ai--deepseek-coder-6.7b-base/f8717011d6b4f4f0309d8ccd413ec0ea39e9c9b948dc4bd3c19e707d4b36e0f2.lock\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-596899493901/Foundation-Models/deepseek_coder/models--deepseek-ai--deepseek-coder-6.7b-base/snapshots/ce2207a8bfef3ee92bd7dd4cc31c52cfa0046912/.gitattributes /tmp/pretrain_model/models--deepseek-ai--deepseek-coder-6.7b-base/snapshots/ce2207a8bfef3ee92bd7dd4cc31c52cfa0046912/.gitattributes\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-596899493901/Foundation-Models/deepseek_coder/models--deepseek-ai--deepseek-coder-6.7b-base/refs/main /tmp/pretrain_model/models--deepseek-ai--deepseek-coder-6.7b-base/refs/main\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-596899493901/Foundation-Models/deepseek_coder/tokenizer_config.json /tmp/pretrain_model/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-596899493901/Foundation-Models/deepseek_coder/models--deepseek-ai--deepseek-coder-6.7b-base/blobs/ac09290cb2947183ab7f19d7477511dad3600a7a /tmp/pretrain_model/models--deepseek-ai--deepseek-coder-6.7b-base/blobs/ac09290cb2947183ab7f19d7477511dad3600a7a\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-596899493901/Foundation-Models/deepseek_coder/models--deepseek-ai--deepseek-coder-6.7b-base/blobs/ab70e63cbf14ce6861f1414e091a1fb608cd78c7 /tmp/pretrain_model/models--deepseek-ai--deepseek-coder-6.7b-base/blobs/ab70e63cbf14ce6861f1414e091a1fb608cd78c7\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-596899493901/Foundation-Models/deepseek_coder/pytorch_model.bin.index.json /tmp/pretrain_model/pytorch_model.bin.index.json\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-596899493901/Foundation-Models/deepseek_coder/models--deepseek-ai--deepseek-coder-6.7b-base/snapshots/ce2207a8bfef3ee92bd7dd4cc31c52cfa0046912/config.json /tmp/pretrain_model/models--deepseek-ai--deepseek-coder-6.7b-base/snapshots/ce2207a8bfef3ee92bd7dd4cc31c52cfa0046912/config.json\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-596899493901/Foundation-Models/deepseek_coder/models--deepseek-ai--deepseek-coder-6.7b-base/snapshots/ce2207a8bfef3ee92bd7dd4cc31c52cfa0046912/model.safetensors.index.json /tmp/pretrain_model/models--deepseek-ai--deepseek-coder-6.7b-base/snapshots/ce2207a8bfef3ee92bd7dd4cc31c52cfa0046912/model.safetensors.index.json\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-596899493901/Foundation-Models/deepseek_coder/models--deepseek-ai--deepseek-coder-6.7b-base/blobs/a6344aac8c09253b3b630fb776ae94478aa0275b /tmp/pretrain_model/models--deepseek-ai--deepseek-coder-6.7b-base/blobs/a6344aac8c09253b3b630fb776ae94478aa0275b\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-596899493901/Foundation-Models/deepseek_coder/model.safetensors.index.json /tmp/pretrain_model/model.safetensors.index.json\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-596899493901/Foundation-Models/deepseek_coder/models--deepseek-ai--deepseek-coder-6.7b-base/snapshots/ce2207a8bfef3ee92bd7dd4cc31c52cfa0046912/README.md /tmp/pretrain_model/models--deepseek-ai--deepseek-coder-6.7b-base/snapshots/ce2207a8bfef3ee92bd7dd4cc31c52cfa0046912/README.md\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-596899493901/Foundation-Models/deepseek_coder/models--deepseek-ai--deepseek-coder-6.7b-base/blobs/340b041b4df26ed5685d5a8ad8654dccdac838c2 /tmp/pretrain_model/models--deepseek-ai--deepseek-coder-6.7b-base/blobs/340b041b4df26ed5685d5a8ad8654dccdac838c2\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-596899493901/Foundation-Models/deepseek_coder/LICENSE /tmp/pretrain_model/LICENSE\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-596899493901/Foundation-Models/deepseek_coder/models--deepseek-ai--deepseek-coder-6.7b-base/blobs/03ed4bdaa9d6707eefe2678470686f040dea73c9 /tmp/pretrain_model/models--deepseek-ai--deepseek-coder-6.7b-base/blobs/03ed4bdaa9d6707eefe2678470686f040dea73c9\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-596899493901/Foundation-Models/deepseek_coder/models--deepseek-ai--deepseek-coder-6.7b-base/snapshots/ce2207a8bfef3ee92bd7dd4cc31c52cfa0046912/tokenizer_config.json /tmp/pretrain_model/models--deepseek-ai--deepseek-coder-6.7b-base/snapshots/ce2207a8bfef3ee92bd7dd4cc31c52cfa0046912/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-596899493901/Foundation-Models/deepseek_coder/models--deepseek-ai--deepseek-coder-6.7b-base/snapshots/ce2207a8bfef3ee92bd7dd4cc31c52cfa0046912/LICENSE /tmp/pretrain_model/models--deepseek-ai--deepseek-coder-6.7b-base/snapshots/ce2207a8bfef3ee92bd7dd4cc31c52cfa0046912/LICENSE\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-596899493901/Foundation-Models/deepseek_coder/config.json /tmp/pretrain_model/config.json\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-596899493901/Foundation-Models/deepseek_coder/models--deepseek-ai--deepseek-coder-6.7b-base/blobs/d7129f354c81de8ae5727684a55e168b684144b6 /tmp/pretrain_model/models--deepseek-ai--deepseek-coder-6.7b-base/blobs/d7129f354c81de8ae5727684a55e168b684144b6\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-596899493901/Foundation-Models/deepseek_coder/tokenizer.json /tmp/pretrain_model/tokenizer.json\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-596899493901/Foundation-Models/deepseek_coder/models--deepseek-ai--deepseek-coder-6.7b-base/blobs/0dfa4268afc2cf45515dec553772d7ca79f79762 /tmp/pretrain_model/models--deepseek-ai--deepseek-coder-6.7b-base/blobs/0dfa4268afc2cf45515dec553772d7ca79f79762\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-596899493901/Foundation-Models/deepseek_coder/models--deepseek-ai--deepseek-coder-6.7b-base/blobs/d4338e87a01134a05058c25623116fb6678a90c7 /tmp/pretrain_model/models--deepseek-ai--deepseek-coder-6.7b-base/blobs/d4338e87a01134a05058c25623116fb6678a90c7\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-596899493901/Foundation-Models/deepseek_coder/models--deepseek-ai--deepseek-coder-6.7b-base/snapshots/ce2207a8bfef3ee92bd7dd4cc31c52cfa0046912/tokenizer.json /tmp/pretrain_model/models--deepseek-ai--deepseek-coder-6.7b-base/snapshots/ce2207a8bfef3ee92bd7dd4cc31c52cfa0046912/tokenizer.json\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-596899493901/Foundation-Models/deepseek_coder/models--deepseek-ai--deepseek-coder-6.7b-base/snapshots/ce2207a8bfef3ee92bd7dd4cc31c52cfa0046912/pytorch_model-00002-of-00002.bin /tmp/pretrain_model/models--deepseek-ai--deepseek-coder-6.7b-base/snapshots/ce2207a8bfef3ee92bd7dd4cc31c52cfa0046912/pytorch_model-00002-of-00002.bin\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-596899493901/Foundation-Models/deepseek_coder/models--deepseek-ai--deepseek-coder-6.7b-base/snapshots/ce2207a8bfef3ee92bd7dd4cc31c52cfa0046912/model-00002-of-00002.safetensors /tmp/pretrain_model/models--deepseek-ai--deepseek-coder-6.7b-base/snapshots/ce2207a8bfef3ee92bd7dd4cc31c52cfa0046912/model-00002-of-00002.safetensors\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-596899493901/Foundation-Models/deepseek_coder/models--deepseek-ai--deepseek-coder-6.7b-base/blobs/aca644afc72b8f4e86910c5bd130b32d5e21c92120a820b085e69b0a3a44ac53 /tmp/pretrain_model/models--deepseek-ai--deepseek-coder-6.7b-base/blobs/aca644afc72b8f4e86910c5bd130b32d5e21c92120a820b085e69b0a3a44ac53\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-596899493901/Foundation-Models/deepseek_coder/models--deepseek-ai--deepseek-coder-6.7b-base/blobs/b2e101c170f6f5147d34b1d55dbd755afaa80680272a937514d7b0bc1319f282 /tmp/pretrain_model/models--deepseek-ai--deepseek-coder-6.7b-base/blobs/b2e101c170f6f5147d34b1d55dbd755afaa80680272a937514d7b0bc1319f282\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-596899493901/Foundation-Models/deepseek_coder/model-00002-of-00002.safetensors /tmp/pretrain_model/model-00002-of-00002.safetensors\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-596899493901/Foundation-Models/deepseek_coder/pytorch_model-00002-of-00002.bin /tmp/pretrain_model/pytorch_model-00002-of-00002.bin\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-596899493901/Foundation-Models/deepseek_coder/models--deepseek-ai--deepseek-coder-6.7b-base/blobs/f8717011d6b4f4f0309d8ccd413ec0ea39e9c9b948dc4bd3c19e707d4b36e0f2 /tmp/pretrain_model/models--deepseek-ai--deepseek-coder-6.7b-base/blobs/f8717011d6b4f4f0309d8ccd413ec0ea39e9c9b948dc4bd3c19e707d4b36e0f2\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-596899493901/Foundation-Models/deepseek_coder/models--deepseek-ai--deepseek-coder-6.7b-base/blobs/726f9234cc8761581e25ac3555e69683b7ef02a1a913358398ec296c8a6bac58 /tmp/pretrain_model/models--deepseek-ai--deepseek-coder-6.7b-base/blobs/726f9234cc8761581e25ac3555e69683b7ef02a1a913358398ec296c8a6bac58\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-596899493901/Foundation-Models/deepseek_coder/models--deepseek-ai--deepseek-coder-6.7b-base/snapshots/ce2207a8bfef3ee92bd7dd4cc31c52cfa0046912/model-00001-of-00002.safetensors /tmp/pretrain_model/models--deepseek-ai--deepseek-coder-6.7b-base/snapshots/ce2207a8bfef3ee92bd7dd4cc31c52cfa0046912/model-00001-of-00002.safetensors\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-596899493901/Foundation-Models/deepseek_coder/models--deepseek-ai--deepseek-coder-6.7b-base/snapshots/ce2207a8bfef3ee92bd7dd4cc31c52cfa0046912/pytorch_model-00001-of-00002.bin /tmp/pretrain_model/models--deepseek-ai--deepseek-coder-6.7b-base/snapshots/ce2207a8bfef3ee92bd7dd4cc31c52cfa0046912/pytorch_model-00001-of-00002.bin\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-596899493901/Foundation-Models/deepseek_coder/model-00001-of-00002.safetensors /tmp/pretrain_model/model-00001-of-00002.safetensors\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-596899493901/Foundation-Models/deepseek_coder/pytorch_model-00001-of-00002.bin /tmp/pretrain_model/pytorch_model-00001-of-00002.bin\u001b[0m\n",
      "\u001b[34m-----finished cp-------\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:03,585] torch.distributed.run: [WARNING] \u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:03,585] torch.distributed.run: [WARNING] *****************************************\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:03,585] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:03,585] torch.distributed.run: [WARNING] *****************************************\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:18,741] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34mdf: /root/.triton/autotune: No such file or directory\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:19,072] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:19,090] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:19,122] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:19,122] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:19,214] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:19,219] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:19,219] [INFO] [real_accelerator.py:219:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:20,520] [INFO] [comm.py:652:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34m[INFO|2025-03-18 10:41:20] llamafactory.hparams.parser:384 >> Process rank: 3, world size: 8, device: cuda:3, distributed training: True, compute dtype: torch.bfloat16\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:20,925] [INFO] [comm.py:652:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:20,993] [INFO] [comm.py:652:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:20,998] [INFO] [comm.py:652:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:21,032] [INFO] [comm.py:652:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:21,033] [INFO] [comm.py:683:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:21,180] [INFO] [comm.py:652:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:21,180] [INFO] [comm.py:652:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:21,180] [INFO] [comm.py:652:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34m[INFO|2025-03-18 10:41:21] llamafactory.hparams.parser:384 >> Process rank: 0, world size: 8, device: cuda:0, distributed training: True, compute dtype: torch.bfloat16\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:697] 2025-03-18 10:41:21,673 >> loading configuration file /tmp/pretrain_model/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:697] 2025-03-18 10:41:21,673 >> loading configuration file /tmp/pretrain_model/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:771] 2025-03-18 10:41:21,674 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"/tmp/pretrain_model\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 32013,\n",
      "  \"eos_token_id\": 32014,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 16384,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 4.0,\n",
      "    \"rope_type\": \"linear\",\n",
      "    \"type\": \"linear\"\n",
      "  },\n",
      "  \"rope_theta\": 100000,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32256\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:771] 2025-03-18 10:41:21,674 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"/tmp/pretrain_model\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 32013,\n",
      "  \"eos_token_id\": 32014,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 16384,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 4.0,\n",
      "    \"rope_type\": \"linear\",\n",
      "    \"type\": \"linear\"\n",
      "  },\n",
      "  \"rope_theta\": 100000,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32256\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2048] 2025-03-18 10:41:21,678 >> loading file tokenizer.model\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2048] 2025-03-18 10:41:21,678 >> loading file tokenizer.model\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2048] 2025-03-18 10:41:21,679 >> loading file tokenizer.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2048] 2025-03-18 10:41:21,679 >> loading file added_tokens.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2048] 2025-03-18 10:41:21,679 >> loading file tokenizer.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2048] 2025-03-18 10:41:21,679 >> loading file added_tokens.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2048] 2025-03-18 10:41:21,679 >> loading file special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2048] 2025-03-18 10:41:21,679 >> loading file tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2048] 2025-03-18 10:41:21,679 >> loading file chat_template.jinja\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2048] 2025-03-18 10:41:21,679 >> loading file special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2048] 2025-03-18 10:41:21,679 >> loading file tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2048] 2025-03-18 10:41:21,679 >> loading file chat_template.jinja\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2313] 2025-03-18 10:41:21,745 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2313] 2025-03-18 10:41:21,745 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:697] 2025-03-18 10:41:21,747 >> loading configuration file /tmp/pretrain_model/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:697] 2025-03-18 10:41:21,747 >> loading configuration file /tmp/pretrain_model/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:771] 2025-03-18 10:41:21,748 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"/tmp/pretrain_model\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 32013,\n",
      "  \"eos_token_id\": 32014,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 16384,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 4.0,\n",
      "    \"rope_type\": \"linear\",\n",
      "    \"type\": \"linear\"\n",
      "  },\n",
      "  \"rope_theta\": 100000,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32256\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:771] 2025-03-18 10:41:21,748 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"/tmp/pretrain_model\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 32013,\n",
      "  \"eos_token_id\": 32014,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 16384,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 4.0,\n",
      "    \"rope_type\": \"linear\",\n",
      "    \"type\": \"linear\"\n",
      "  },\n",
      "  \"rope_theta\": 100000,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32256\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2048] 2025-03-18 10:41:21,748 >> loading file tokenizer.model\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2048] 2025-03-18 10:41:21,748 >> loading file tokenizer.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2048] 2025-03-18 10:41:21,748 >> loading file added_tokens.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2048] 2025-03-18 10:41:21,748 >> loading file special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2048] 2025-03-18 10:41:21,748 >> loading file tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2048] 2025-03-18 10:41:21,748 >> loading file chat_template.jinja\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2048] 2025-03-18 10:41:21,748 >> loading file tokenizer.model\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2048] 2025-03-18 10:41:21,748 >> loading file tokenizer.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2048] 2025-03-18 10:41:21,748 >> loading file added_tokens.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2048] 2025-03-18 10:41:21,748 >> loading file special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2048] 2025-03-18 10:41:21,748 >> loading file tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2048] 2025-03-18 10:41:21,748 >> loading file chat_template.jinja\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2313] 2025-03-18 10:41:21,814 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2313] 2025-03-18 10:41:21,814 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34m[INFO|2025-03-18 10:41:21] llamafactory.data.loader:157 >> Loading dataset Evol_Instruct_Code_12k.json...\u001b[0m\n",
      "\u001b[34m[INFO|2025-03-18 10:41:21] llamafactory.hparams.parser:384 >> Process rank: 6, world size: 8, device: cuda:6, distributed training: True, compute dtype: torch.bfloat16\u001b[0m\n",
      "\u001b[34mSetting num_proc from 16 back to 1 for the train split to disable multiprocessing as it only contains one shard.\u001b[0m\n",
      "\u001b[34mGenerating train split: 0 examples [00:00, ? examples/s]\u001b[0m\n",
      "\u001b[34m[INFO|2025-03-18 10:41:21] llamafactory.hparams.parser:384 >> Process rank: 1, world size: 8, device: cuda:1, distributed training: True, compute dtype: torch.bfloat16\u001b[0m\n",
      "\u001b[34m[INFO|2025-03-18 10:41:21] llamafactory.hparams.parser:384 >> Process rank: 2, world size: 8, device: cuda:2, distributed training: True, compute dtype: torch.bfloat16\u001b[0m\n",
      "\u001b[34mGenerating train split: 12000 examples [00:00, 119778.79 examples/s]\u001b[0m\n",
      "\u001b[34m[INFO|2025-03-18 10:41:22] llamafactory.hparams.parser:384 >> Process rank: 4, world size: 8, device: cuda:4, distributed training: True, compute dtype: torch.bfloat16\u001b[0m\n",
      "\u001b[34m[INFO|2025-03-18 10:41:22] llamafactory.hparams.parser:384 >> Process rank: 7, world size: 8, device: cuda:7, distributed training: True, compute dtype: torch.bfloat16\u001b[0m\n",
      "\u001b[34m[INFO|2025-03-18 10:41:22] llamafactory.hparams.parser:384 >> Process rank: 5, world size: 8, device: cuda:5, distributed training: True, compute dtype: torch.bfloat16\u001b[0m\n",
      "\u001b[34mConverting format of dataset (num_proc=16):   0%|          | 0/12000 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34mConverting format of dataset (num_proc=16):   6%|▋         | 750/12000 [00:00<00:01, 6352.71 examples/s]\u001b[0m\n",
      "\u001b[34mConverting format of dataset (num_proc=16): 100%|██████████| 12000/12000 [00:00<00:00, 43590.66 examples/s]\u001b[0m\n",
      "\u001b[34malgo-1:238:238 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0\u001b[0m\n",
      "\u001b[34malgo-1:238:238 [0] NCCL INFO Bootstrap : Using eth0:10.0.208.76<0>\u001b[0m\n",
      "\u001b[34malgo-1:238:238 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.\u001b[0m\n",
      "\u001b[34malgo-1:238:238 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).\u001b[0m\n",
      "\u001b[34malgo-1:238:238 [0] NCCL INFO cudaDriverVersion 12040\u001b[0m\n",
      "\u001b[34mNCCL version 2.18.5+cuda12.1\u001b[0m\n",
      "\u001b[34malgo-1:241:241 [3] NCCL INFO cudaDriverVersion 12040\u001b[0m\n",
      "\u001b[34malgo-1:244:244 [6] NCCL INFO cudaDriverVersion 12040\u001b[0m\n",
      "\u001b[34malgo-1:240:240 [2] NCCL INFO cudaDriverVersion 12040\u001b[0m\n",
      "\u001b[34malgo-1:239:239 [1] NCCL INFO cudaDriverVersion 12040\u001b[0m\n",
      "\u001b[34malgo-1:242:242 [4] NCCL INFO cudaDriverVersion 12040\u001b[0m\n",
      "\u001b[34malgo-1:243:243 [5] NCCL INFO cudaDriverVersion 12040\u001b[0m\n",
      "\u001b[34malgo-1:245:245 [7] NCCL INFO cudaDriverVersion 12040\u001b[0m\n",
      "\u001b[34malgo-1:241:241 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0\u001b[0m\n",
      "\u001b[34malgo-1:244:244 [6] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0\u001b[0m\n",
      "\u001b[34malgo-1:240:240 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0\u001b[0m\n",
      "\u001b[34malgo-1:239:239 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0\u001b[0m\n",
      "\u001b[34malgo-1:242:242 [4] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0\u001b[0m\n",
      "\u001b[34malgo-1:243:243 [5] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0\u001b[0m\n",
      "\u001b[34malgo-1:245:245 [7] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0\u001b[0m\n",
      "\u001b[34malgo-1:241:241 [3] NCCL INFO Bootstrap : Using eth0:10.0.208.76<0>\u001b[0m\n",
      "\u001b[34malgo-1:244:244 [6] NCCL INFO Bootstrap : Using eth0:10.0.208.76<0>\u001b[0m\n",
      "\u001b[34malgo-1:240:240 [2] NCCL INFO Bootstrap : Using eth0:10.0.208.76<0>\u001b[0m\n",
      "\u001b[34malgo-1:239:239 [1] NCCL INFO Bootstrap : Using eth0:10.0.208.76<0>\u001b[0m\n",
      "\u001b[34malgo-1:245:245 [7] NCCL INFO Bootstrap : Using eth0:10.0.208.76<0>\u001b[0m\n",
      "\u001b[34malgo-1:242:242 [4] NCCL INFO Bootstrap : Using eth0:10.0.208.76<0>\u001b[0m\n",
      "\u001b[34malgo-1:243:243 [5] NCCL INFO Bootstrap : Using eth0:10.0.208.76<0>\u001b[0m\n",
      "\u001b[34malgo-1:241:241 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.\u001b[0m\n",
      "\u001b[34malgo-1:243:243 [5] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.\u001b[0m\n",
      "\u001b[34malgo-1:241:241 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).\u001b[0m\n",
      "\u001b[34malgo-1:243:243 [5] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).\u001b[0m\n",
      "\u001b[34malgo-1:240:240 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.\u001b[0m\n",
      "\u001b[34malgo-1:239:239 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.\u001b[0m\n",
      "\u001b[34malgo-1:240:240 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).\u001b[0m\n",
      "\u001b[34malgo-1:239:239 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).\u001b[0m\n",
      "\u001b[34malgo-1:244:244 [6] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.\u001b[0m\n",
      "\u001b[34malgo-1:245:245 [7] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.\u001b[0m\n",
      "\u001b[34malgo-1:242:242 [4] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.\u001b[0m\n",
      "\u001b[34malgo-1:244:244 [6] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).\u001b[0m\n",
      "\u001b[34malgo-1:245:245 [7] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).\u001b[0m\n",
      "\u001b[34malgo-1:242:242 [4] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).\u001b[0m\n",
      "\u001b[34malgo-1:243:651 [5] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.9.1-aws\u001b[0m\n",
      "\u001b[34malgo-1:243:651 [5] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.9.1-aws\u001b[0m\n",
      "\u001b[34malgo-1:243:651 [5] NCCL INFO NET/OFI Using Libfabric version 1.19\u001b[0m\n",
      "\u001b[34malgo-1:243:651 [5] NCCL INFO NET/OFI Using CUDA driver version 12040\u001b[0m\n",
      "\u001b[34malgo-1:243:651 [5] NCCL INFO NET/OFI Configuring AWS-specific options\u001b[0m\n",
      "\u001b[34malgo-1:243:651 [5] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34malgo-1:243:651 [5] NCCL INFO NET/OFI Setting NCCL_NVLSTREE_MAX_CHUNKSIZE to 512KiB\u001b[0m\n",
      "\u001b[34malgo-1:243:651 [5] NCCL INFO NET/OFI Setting NCCL_NVLS_CHUNKSIZE to 512KiB\u001b[0m\n",
      "\u001b[34malgo-1:243:651 [5] NCCL INFO NET/OFI Running on p4d.24xlarge platform, Setting NCCL_TOPO_FILE environment variable to /opt/conda/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[34malgo-1:243:651 [5] NCCL INFO NET/OFI Internode latency set at 75.0 us\u001b[0m\n",
      "\u001b[34malgo-1:243:651 [5] NCCL INFO NET/OFI Using transport protocol SENDRECV\u001b[0m\n",
      "\u001b[34malgo-1:238:646 [0] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.9.1-aws\u001b[0m\n",
      "\u001b[34malgo-1:238:646 [0] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.9.1-aws\u001b[0m\n",
      "\u001b[34malgo-1:238:646 [0] NCCL INFO NET/OFI Using Libfabric version 1.19\u001b[0m\n",
      "\u001b[34malgo-1:238:646 [0] NCCL INFO NET/OFI Using CUDA driver version 12040\u001b[0m\n",
      "\u001b[34malgo-1:238:646 [0] NCCL INFO NET/OFI Configuring AWS-specific options\u001b[0m\n",
      "\u001b[34malgo-1:238:646 [0] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34malgo-1:238:646 [0] NCCL INFO NET/OFI Setting NCCL_NVLSTREE_MAX_CHUNKSIZE to 512KiB\u001b[0m\n",
      "\u001b[34malgo-1:238:646 [0] NCCL INFO NET/OFI Setting NCCL_NVLS_CHUNKSIZE to 512KiB\u001b[0m\n",
      "\u001b[34malgo-1:238:646 [0] NCCL INFO NET/OFI Running on p4d.24xlarge platform, Setting NCCL_TOPO_FILE environment variable to /opt/conda/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[34malgo-1:238:646 [0] NCCL INFO NET/OFI Internode latency set at 75.0 us\u001b[0m\n",
      "\u001b[34malgo-1:238:646 [0] NCCL INFO NET/OFI Using transport protocol SENDRECV\u001b[0m\n",
      "\u001b[34malgo-1:245:653 [7] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.9.1-aws\u001b[0m\n",
      "\u001b[34malgo-1:245:653 [7] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.9.1-aws\u001b[0m\n",
      "\u001b[34malgo-1:245:653 [7] NCCL INFO NET/OFI Using Libfabric version 1.19\u001b[0m\n",
      "\u001b[34malgo-1:245:653 [7] NCCL INFO NET/OFI Using CUDA driver version 12040\u001b[0m\n",
      "\u001b[34malgo-1:245:653 [7] NCCL INFO NET/OFI Configuring AWS-specific options\u001b[0m\n",
      "\u001b[34malgo-1:245:653 [7] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34malgo-1:245:653 [7] NCCL INFO NET/OFI Setting NCCL_NVLSTREE_MAX_CHUNKSIZE to 512KiB\u001b[0m\n",
      "\u001b[34malgo-1:245:653 [7] NCCL INFO NET/OFI Setting NCCL_NVLS_CHUNKSIZE to 512KiB\u001b[0m\n",
      "\u001b[34malgo-1:245:653 [7] NCCL INFO NET/OFI Running on p4d.24xlarge platform, Setting NCCL_TOPO_FILE environment variable to /opt/conda/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[34malgo-1:245:653 [7] NCCL INFO NET/OFI Internode latency set at 75.0 us\u001b[0m\n",
      "\u001b[34malgo-1:245:653 [7] NCCL INFO NET/OFI Using transport protocol SENDRECV\u001b[0m\n",
      "\u001b[34malgo-1:242:652 [4] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.9.1-aws\u001b[0m\n",
      "\u001b[34malgo-1:242:652 [4] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.9.1-aws\u001b[0m\n",
      "\u001b[34malgo-1:242:652 [4] NCCL INFO NET/OFI Using Libfabric version 1.19\u001b[0m\n",
      "\u001b[34malgo-1:242:652 [4] NCCL INFO NET/OFI Using CUDA driver version 12040\u001b[0m\n",
      "\u001b[34malgo-1:242:652 [4] NCCL INFO NET/OFI Configuring AWS-specific options\u001b[0m\n",
      "\u001b[34malgo-1:242:652 [4] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34malgo-1:242:652 [4] NCCL INFO NET/OFI Setting NCCL_NVLSTREE_MAX_CHUNKSIZE to 512KiB\u001b[0m\n",
      "\u001b[34malgo-1:242:652 [4] NCCL INFO NET/OFI Setting NCCL_NVLS_CHUNKSIZE to 512KiB\u001b[0m\n",
      "\u001b[34malgo-1:242:652 [4] NCCL INFO NET/OFI Running on p4d.24xlarge platform, Setting NCCL_TOPO_FILE environment variable to /opt/conda/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[34malgo-1:242:652 [4] NCCL INFO NET/OFI Internode latency set at 75.0 us\u001b[0m\n",
      "\u001b[34malgo-1:242:652 [4] NCCL INFO NET/OFI Using transport protocol SENDRECV\u001b[0m\n",
      "\u001b[34malgo-1:244:649 [6] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.9.1-aws\u001b[0m\n",
      "\u001b[34malgo-1:244:649 [6] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.9.1-aws\u001b[0m\n",
      "\u001b[34malgo-1:244:649 [6] NCCL INFO NET/OFI Using Libfabric version 1.19\u001b[0m\n",
      "\u001b[34malgo-1:244:649 [6] NCCL INFO NET/OFI Using CUDA driver version 12040\u001b[0m\n",
      "\u001b[34malgo-1:244:649 [6] NCCL INFO NET/OFI Configuring AWS-specific options\u001b[0m\n",
      "\u001b[34malgo-1:244:649 [6] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34malgo-1:244:649 [6] NCCL INFO NET/OFI Setting NCCL_NVLSTREE_MAX_CHUNKSIZE to 512KiB\u001b[0m\n",
      "\u001b[34malgo-1:244:649 [6] NCCL INFO NET/OFI Setting NCCL_NVLS_CHUNKSIZE to 512KiB\u001b[0m\n",
      "\u001b[34malgo-1:244:649 [6] NCCL INFO NET/OFI Running on p4d.24xlarge platform, Setting NCCL_TOPO_FILE environment variable to /opt/conda/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[34malgo-1:244:649 [6] NCCL INFO NET/OFI Internode latency set at 75.0 us\u001b[0m\n",
      "\u001b[34malgo-1:244:649 [6] NCCL INFO NET/OFI Using transport protocol SENDRECV\u001b[0m\n",
      "\u001b[34malgo-1:241:647 [3] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.9.1-aws\u001b[0m\n",
      "\u001b[34malgo-1:241:647 [3] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.9.1-aws\u001b[0m\n",
      "\u001b[34malgo-1:241:647 [3] NCCL INFO NET/OFI Using Libfabric version 1.19\u001b[0m\n",
      "\u001b[34malgo-1:241:647 [3] NCCL INFO NET/OFI Using CUDA driver version 12040\u001b[0m\n",
      "\u001b[34malgo-1:241:647 [3] NCCL INFO NET/OFI Configuring AWS-specific options\u001b[0m\n",
      "\u001b[34malgo-1:241:647 [3] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34malgo-1:241:647 [3] NCCL INFO NET/OFI Setting NCCL_NVLSTREE_MAX_CHUNKSIZE to 512KiB\u001b[0m\n",
      "\u001b[34malgo-1:241:647 [3] NCCL INFO NET/OFI Setting NCCL_NVLS_CHUNKSIZE to 512KiB\u001b[0m\n",
      "\u001b[34malgo-1:241:647 [3] NCCL INFO NET/OFI Running on p4d.24xlarge platform, Setting NCCL_TOPO_FILE environment variable to /opt/conda/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[34malgo-1:241:647 [3] NCCL INFO NET/OFI Internode latency set at 75.0 us\u001b[0m\n",
      "\u001b[34malgo-1:241:647 [3] NCCL INFO NET/OFI Using transport protocol SENDRECV\u001b[0m\n",
      "\u001b[34malgo-1:240:648 [2] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.9.1-aws\u001b[0m\n",
      "\u001b[34malgo-1:240:648 [2] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.9.1-aws\u001b[0m\n",
      "\u001b[34malgo-1:240:648 [2] NCCL INFO NET/OFI Using Libfabric version 1.19\u001b[0m\n",
      "\u001b[34malgo-1:240:648 [2] NCCL INFO NET/OFI Using CUDA driver version 12040\u001b[0m\n",
      "\u001b[34malgo-1:240:648 [2] NCCL INFO NET/OFI Configuring AWS-specific options\u001b[0m\n",
      "\u001b[34malgo-1:240:648 [2] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34malgo-1:240:648 [2] NCCL INFO NET/OFI Setting NCCL_NVLSTREE_MAX_CHUNKSIZE to 512KiB\u001b[0m\n",
      "\u001b[34malgo-1:240:648 [2] NCCL INFO NET/OFI Setting NCCL_NVLS_CHUNKSIZE to 512KiB\u001b[0m\n",
      "\u001b[34malgo-1:240:648 [2] NCCL INFO NET/OFI Running on p4d.24xlarge platform, Setting NCCL_TOPO_FILE environment variable to /opt/conda/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[34malgo-1:240:648 [2] NCCL INFO NET/OFI Internode latency set at 75.0 us\u001b[0m\n",
      "\u001b[34malgo-1:240:648 [2] NCCL INFO NET/OFI Using transport protocol SENDRECV\u001b[0m\n",
      "\u001b[34malgo-1:245:653 [7] NCCL INFO NET/OFI Selected Provider is efa (found 4 nics)\u001b[0m\n",
      "\u001b[34malgo-1:243:651 [5] NCCL INFO NET/OFI Selected Provider is efa (found 4 nics)\u001b[0m\n",
      "\u001b[34malgo-1:238:646 [0] NCCL INFO NET/OFI Selected Provider is efa (found 4 nics)\u001b[0m\n",
      "\u001b[34malgo-1:242:652 [4] NCCL INFO NET/OFI Selected Provider is efa (found 4 nics)\u001b[0m\n",
      "\u001b[34malgo-1:244:649 [6] NCCL INFO NET/OFI Selected Provider is efa (found 4 nics)\u001b[0m\n",
      "\u001b[34malgo-1:239:650 [1] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.9.1-aws\u001b[0m\n",
      "\u001b[34malgo-1:239:650 [1] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.9.1-aws\u001b[0m\n",
      "\u001b[34malgo-1:239:650 [1] NCCL INFO NET/OFI Using Libfabric version 1.19\u001b[0m\n",
      "\u001b[34malgo-1:239:650 [1] NCCL INFO NET/OFI Using CUDA driver version 12040\u001b[0m\n",
      "\u001b[34malgo-1:239:650 [1] NCCL INFO NET/OFI Configuring AWS-specific options\u001b[0m\n",
      "\u001b[34malgo-1:241:647 [3] NCCL INFO NET/OFI Selected Provider is efa (found 4 nics)\u001b[0m\n",
      "\u001b[34malgo-1:239:650 [1] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34malgo-1:239:650 [1] NCCL INFO NET/OFI Setting NCCL_NVLSTREE_MAX_CHUNKSIZE to 512KiB\u001b[0m\n",
      "\u001b[34malgo-1:239:650 [1] NCCL INFO NET/OFI Setting NCCL_NVLS_CHUNKSIZE to 512KiB\u001b[0m\n",
      "\u001b[34malgo-1:239:650 [1] NCCL INFO NET/OFI Running on p4d.24xlarge platform, Setting NCCL_TOPO_FILE environment variable to /opt/conda/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[34malgo-1:239:650 [1] NCCL INFO NET/OFI Internode latency set at 75.0 us\u001b[0m\n",
      "\u001b[34malgo-1:239:650 [1] NCCL INFO NET/OFI Using transport protocol SENDRECV\u001b[0m\n",
      "\u001b[34malgo-1:240:648 [2] NCCL INFO NET/OFI Selected Provider is efa (found 4 nics)\u001b[0m\n",
      "\u001b[34malgo-1:239:650 [1] NCCL INFO NET/OFI Selected Provider is efa (found 4 nics)\u001b[0m\n",
      "\u001b[34malgo-1:245:653 [7] NCCL INFO NET/OFI Setting FI_OPT_EFA_SENDRECV_IN_ORDER_ALIGNED_128_BYTES not supported.\u001b[0m\n",
      "\u001b[34malgo-1:243:651 [5] NCCL INFO NET/OFI Setting FI_OPT_EFA_SENDRECV_IN_ORDER_ALIGNED_128_BYTES not supported.\u001b[0m\n",
      "\u001b[34malgo-1:238:646 [0] NCCL INFO NET/OFI Setting FI_OPT_EFA_SENDRECV_IN_ORDER_ALIGNED_128_BYTES not supported.\u001b[0m\n",
      "\u001b[34malgo-1:242:652 [4] NCCL INFO NET/OFI Setting FI_OPT_EFA_SENDRECV_IN_ORDER_ALIGNED_128_BYTES not supported.\u001b[0m\n",
      "\u001b[34malgo-1:244:649 [6] NCCL INFO NET/OFI Setting FI_OPT_EFA_SENDRECV_IN_ORDER_ALIGNED_128_BYTES not supported.\u001b[0m\n",
      "\u001b[34malgo-1:241:647 [3] NCCL INFO NET/OFI Setting FI_OPT_EFA_SENDRECV_IN_ORDER_ALIGNED_128_BYTES not supported.\u001b[0m\n",
      "\u001b[34malgo-1:240:648 [2] NCCL INFO NET/OFI Setting FI_OPT_EFA_SENDRECV_IN_ORDER_ALIGNED_128_BYTES not supported.\u001b[0m\n",
      "\u001b[34malgo-1:239:650 [1] NCCL INFO NET/OFI Setting FI_OPT_EFA_SENDRECV_IN_ORDER_ALIGNED_128_BYTES not supported.\u001b[0m\n",
      "\u001b[34malgo-1:245:653 [7] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34malgo-1:243:651 [5] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34malgo-1:238:646 [0] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34malgo-1:242:652 [4] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34malgo-1:244:649 [6] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34malgo-1:241:647 [3] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34malgo-1:240:648 [2] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34malgo-1:239:650 [1] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34malgo-1:245:653 [7] NCCL INFO DMA-BUF is available on GPU device 7\u001b[0m\n",
      "\u001b[34malgo-1:243:651 [5] NCCL INFO DMA-BUF is available on GPU device 5\u001b[0m\n",
      "\u001b[34malgo-1:238:646 [0] NCCL INFO DMA-BUF is available on GPU device 0\u001b[0m\n",
      "\u001b[34malgo-1:242:652 [4] NCCL INFO DMA-BUF is available on GPU device 4\u001b[0m\n",
      "\u001b[34malgo-1:244:649 [6] NCCL INFO DMA-BUF is available on GPU device 6\u001b[0m\n",
      "\u001b[34malgo-1:240:648 [2] NCCL INFO DMA-BUF is available on GPU device 2\u001b[0m\n",
      "\u001b[34malgo-1:241:647 [3] NCCL INFO DMA-BUF is available on GPU device 3\u001b[0m\n",
      "\u001b[34malgo-1:239:650 [1] NCCL INFO DMA-BUF is available on GPU device 1\u001b[0m\n",
      "\u001b[34malgo-1:239:650 [1] NCCL INFO comm 0x55c924273b40 rank 1 nranks 8 cudaDev 1 nvmlDev 1 busId 101d0 commId 0xabc13d18bf88ffe5 - Init START\u001b[0m\n",
      "\u001b[34malgo-1:238:646 [0] NCCL INFO comm 0x558a507dcb20 rank 0 nranks 8 cudaDev 0 nvmlDev 0 busId 101c0 commId 0xabc13d18bf88ffe5 - Init START\u001b[0m\n",
      "\u001b[34malgo-1:240:648 [2] NCCL INFO comm 0x559e972e94e0 rank 2 nranks 8 cudaDev 2 nvmlDev 2 busId 201c0 commId 0xabc13d18bf88ffe5 - Init START\u001b[0m\n",
      "\u001b[34malgo-1:245:653 [7] NCCL INFO comm 0x557258f6f080 rank 7 nranks 8 cudaDev 7 nvmlDev 7 busId a01d0 commId 0xabc13d18bf88ffe5 - Init START\u001b[0m\n",
      "\u001b[34malgo-1:244:649 [6] NCCL INFO comm 0x5654f9cbfaf0 rank 6 nranks 8 cudaDev 6 nvmlDev 6 busId a01c0 commId 0xabc13d18bf88ffe5 - Init START\u001b[0m\n",
      "\u001b[34malgo-1:243:651 [5] NCCL INFO comm 0x5619918da890 rank 5 nranks 8 cudaDev 5 nvmlDev 5 busId 901d0 commId 0xabc13d18bf88ffe5 - Init START\u001b[0m\n",
      "\u001b[34malgo-1:242:652 [4] NCCL INFO comm 0x5620e3a98880 rank 4 nranks 8 cudaDev 4 nvmlDev 4 busId 901c0 commId 0xabc13d18bf88ffe5 - Init START\u001b[0m\n",
      "\u001b[34malgo-1:241:647 [3] NCCL INFO comm 0x564f62db4b40 rank 3 nranks 8 cudaDev 3 nvmlDev 3 busId 201d0 commId 0xabc13d18bf88ffe5 - Init START\u001b[0m\n",
      "\u001b[34malgo-1:243:651 [5] NCCL INFO NCCL_TOPO_FILE set by environment to /opt/conda/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[34malgo-1:244:649 [6] NCCL INFO NCCL_TOPO_FILE set by environment to /opt/conda/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[34malgo-1:242:652 [4] NCCL INFO NCCL_TOPO_FILE set by environment to /opt/conda/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[34malgo-1:245:653 [7] NCCL INFO NCCL_TOPO_FILE set by environment to /opt/conda/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[34malgo-1:239:650 [1] NCCL INFO NCCL_TOPO_FILE set by environment to /opt/conda/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[34malgo-1:240:648 [2] NCCL INFO NCCL_TOPO_FILE set by environment to /opt/conda/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[34malgo-1:241:647 [3] NCCL INFO NCCL_TOPO_FILE set by environment to /opt/conda/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[34malgo-1:238:646 [0] NCCL INFO NCCL_TOPO_FILE set by environment to /opt/conda/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[34malgo-1:242:652 [4] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:242:652 [4] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:242:652 [4] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:242:652 [4] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:243:651 [5] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:243:651 [5] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:243:651 [5] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:243:651 [5] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:244:649 [6] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:244:649 [6] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:244:649 [6] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:244:649 [6] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:245:653 [7] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:245:653 [7] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:245:653 [7] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:245:653 [7] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:239:650 [1] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:239:650 [1] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:239:650 [1] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:239:650 [1] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:241:647 [3] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:241:647 [3] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:241:647 [3] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:241:647 [3] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:240:648 [2] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:240:648 [2] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:240:648 [2] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:240:648 [2] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:238:646 [0] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:238:646 [0] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:238:646 [0] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:238:646 [0] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:244:649 [6] NCCL INFO Setting affinity for GPU 6 to ffffff00,0000ffff,ff000000\u001b[0m\n",
      "\u001b[34malgo-1:243:651 [5] NCCL INFO Setting affinity for GPU 5 to ffffff00,0000ffff,ff000000\u001b[0m\n",
      "\u001b[34malgo-1:243:651 [5] NCCL INFO NVLS multicast support is not available on dev 5\u001b[0m\n",
      "\u001b[34malgo-1:244:649 [6] NCCL INFO NVLS multicast support is not available on dev 6\u001b[0m\n",
      "\u001b[34malgo-1:242:652 [4] NCCL INFO Setting affinity for GPU 4 to ffffff00,0000ffff,ff000000\u001b[0m\n",
      "\u001b[34malgo-1:242:652 [4] NCCL INFO NVLS multicast support is not available on dev 4\u001b[0m\n",
      "\u001b[34malgo-1:245:653 [7] NCCL INFO Setting affinity for GPU 7 to ffffff00,0000ffff,ff000000\u001b[0m\n",
      "\u001b[34malgo-1:245:653 [7] NCCL INFO NVLS multicast support is not available on dev 7\u001b[0m\n",
      "\u001b[34malgo-1:240:648 [2] NCCL INFO Setting affinity for GPU 2 to ff,ffff0000,00ffffff\u001b[0m\n",
      "\u001b[34malgo-1:240:648 [2] NCCL INFO NVLS multicast support is not available on dev 2\u001b[0m\n",
      "\u001b[34malgo-1:239:650 [1] NCCL INFO Setting affinity for GPU 1 to ff,ffff0000,00ffffff\u001b[0m\n",
      "\u001b[34malgo-1:239:650 [1] NCCL INFO NVLS multicast support is not available on dev 1\u001b[0m\n",
      "\u001b[34malgo-1:241:647 [3] NCCL INFO Setting affinity for GPU 3 to ff,ffff0000,00ffffff\u001b[0m\n",
      "\u001b[34malgo-1:241:647 [3] NCCL INFO NVLS multicast support is not available on dev 3\u001b[0m\n",
      "\u001b[34malgo-1:238:646 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffff0000,00ffffff\u001b[0m\n",
      "\u001b[34malgo-1:238:646 [0] NCCL INFO NVLS multicast support is not available on dev 0\u001b[0m\n",
      "\u001b[34malgo-1:238:646 [0] NCCL INFO Channel 00/24 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34malgo-1:238:646 [0] NCCL INFO Channel 01/24 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34malgo-1:238:646 [0] NCCL INFO Channel 02/24 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34malgo-1:238:646 [0] NCCL INFO Channel 03/24 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34malgo-1:238:646 [0] NCCL INFO Channel 04/24 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34malgo-1:238:646 [0] NCCL INFO Channel 05/24 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34malgo-1:245:653 [7] NCCL INFO Trees [0] -1/-1/-1->7->6 [1] -1/-1/-1->7->6 [2] -1/-1/-1->7->6 [3] -1/-1/-1->7->6 [4] -1/-1/-1->7->6 [5] -1/-1/-1->7->6 [6] -1/-1/-1->7->6 [7] -1/-1/-1->7->6 [8] -1/-1/-1->7->6 [9] -1/-1/-1->7->6 [10] -1/-1/-1->7->6 [11] -1/-1/-1->7->6 [12] -1/-1/-1->7->6 [13] -1/-1/-1->7->6 [14] -1/-1/-1->7->6 [15] -1/-1/-1->7->6 [16] -1/-1/-1->7->6 [17] -1/-1/-1->7->6 [18] -1/-1/-1->7->6 [19] -1/-1/-1->7->6 [20] -1/-1/-1->7->6 [21] -1/-1/-1->7->6 [22] -1/-1/-1->7->6 [23] -1/-1/-1->7->6\u001b[0m\n",
      "\u001b[34malgo-1:238:646 [0] NCCL INFO Channel 06/24 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34malgo-1:239:650 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0 [2] 2/-1/-1->1->0 [3] 2/-1/-1->1->0 [4] 2/-1/-1->1->0 [5] 2/-1/-1->1->0 [6] 2/-1/-1->1->0 [7] 2/-1/-1->1->0 [8] 2/-1/-1->1->0 [9] 2/-1/-1->1->0 [10] 2/-1/-1->1->0 [11] 2/-1/-1->1->0 [12] 2/-1/-1->1->0 [13] 2/-1/-1->1->0 [14] 2/-1/-1->1->0 [15] 2/-1/-1->1->0 [16] 2/-1/-1->1->0 [17] 2/-1/-1->1->0 [18] 2/-1/-1->1->0 [19] 2/-1/-1->1->0 [20] 2/-1/-1->1->0 [21] 2/-1/-1->1->0 [22] 2/-1/-1->1->0 [23] 2/-1/-1->1->0\u001b[0m\n",
      "\u001b[34malgo-1:238:646 [0] NCCL INFO Channel 07/24 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34malgo-1:245:653 [7] NCCL INFO P2P Chunksize set to 524288\u001b[0m\n",
      "\u001b[34malgo-1:239:650 [1] NCCL INFO P2P Chunksize set to 524288\u001b[0m\n",
      "\u001b[34malgo-1:238:646 [0] NCCL INFO Channel 08/24 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34malgo-1:238:646 [0] NCCL INFO Channel 09/24 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34malgo-1:244:649 [6] NCCL INFO Trees [0] 7/-1/-1->6->5 [1] 7/-1/-1->6->5 [2] 7/-1/-1->6->5 [3] 7/-1/-1->6->5 [4] 7/-1/-1->6->5 [5] 7/-1/-1->6->5 [6] 7/-1/-1->6->5 [7] 7/-1/-1->6->5 [8] 7/-1/-1->6->5 [9] 7/-1/-1->6->5 [10] 7/-1/-1->6->5 [11] 7/-1/-1->6->5 [12] 7/-1/-1->6->5 [13] 7/-1/-1->6->5 [14] 7/-1/-1->6->5 [15] 7/-1/-1->6->5 [16] 7/-1/-1->6->5 [17] 7/-1/-1->6->5 [18] 7/-1/-1->6->5 [19] 7/-1/-1->6->5 [20] 7/-1/-1->6->5 [21] 7/-1/-1->6->5 [22] 7/-1/-1->6->5 [23] 7/-1/-1->6->5\u001b[0m\n",
      "\u001b[34malgo-1:238:646 [0] NCCL INFO Channel 10/24 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34malgo-1:238:646 [0] NCCL INFO Channel 11/24 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34malgo-1:244:649 [6] NCCL INFO P2P Chunksize set to 524288\u001b[0m\n",
      "\u001b[34malgo-1:238:646 [0] NCCL INFO Channel 12/24 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34malgo-1:243:651 [5] NCCL INFO Trees [0] 6/-1/-1->5->4 [1] 6/-1/-1->5->4 [2] 6/-1/-1->5->4 [3] 6/-1/-1->5->4 [4] 6/-1/-1->5->4 [5] 6/-1/-1->5->4 [6] 6/-1/-1->5->4 [7] 6/-1/-1->5->4 [8] 6/-1/-1->5->4 [9] 6/-1/-1->5->4 [10] 6/-1/-1->5->4 [11] 6/-1/-1->5->4 [12] 6/-1/-1->5->4 [13] 6/-1/-1->5->4 [14] 6/-1/-1->5->4 [15] 6/-1/-1->5->4 [16] 6/-1/-1->5->4 [17] 6/-1/-1->5->4 [18] 6/-1/-1->5->4 [19] 6/-1/-1->5->4 [20] 6/-1/-1->5->4 [21] 6/-1/-1->5->4 [22] 6/-1/-1->5->4 [23] 6/-1/-1->5->4\u001b[0m\n",
      "\u001b[34malgo-1:238:646 [0] NCCL INFO Channel 13/24 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34malgo-1:238:646 [0] NCCL INFO Channel 14/24 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34malgo-1:243:651 [5] NCCL INFO P2P Chunksize set to 524288\u001b[0m\n",
      "\u001b[34malgo-1:238:646 [0] NCCL INFO Channel 15/24 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34malgo-1:238:646 [0] NCCL INFO Channel 16/24 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34malgo-1:242:652 [4] NCCL INFO Trees [0] 5/-1/-1->4->3 [1] 5/-1/-1->4->3 [2] 5/-1/-1->4->3 [3] 5/-1/-1->4->3 [4] 5/-1/-1->4->3 [5] 5/-1/-1->4->3 [6] 5/-1/-1->4->3 [7] 5/-1/-1->4->3 [8] 5/-1/-1->4->3 [9] 5/-1/-1->4->3 [10] 5/-1/-1->4->3 [11] 5/-1/-1->4->3 [12] 5/-1/-1->4->3 [13] 5/-1/-1->4->3 [14] 5/-1/-1->4->3 [15] 5/-1/-1->4->3 [16] 5/-1/-1->4->3 [17] 5/-1/-1->4->3 [18] 5/-1/-1->4->3 [19] 5/-1/-1->4->3 [20] 5/-1/-1->4->3 [21] 5/-1/-1->4->3 [22] 5/-1/-1->4->3 [23] 5/-1/-1->4->3\u001b[0m\n",
      "\u001b[34malgo-1:238:646 [0] NCCL INFO Channel 17/24 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34malgo-1:242:652 [4] NCCL INFO P2P Chunksize set to 524288\u001b[0m\n",
      "\u001b[34malgo-1:238:646 [0] NCCL INFO Channel 18/24 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34malgo-1:238:646 [0] NCCL INFO Channel 19/24 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34malgo-1:238:646 [0] NCCL INFO Channel 20/24 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34malgo-1:238:646 [0] NCCL INFO Channel 21/24 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34malgo-1:238:646 [0] NCCL INFO Channel 22/24 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34malgo-1:238:646 [0] NCCL INFO Channel 23/24 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34malgo-1:241:647 [3] NCCL INFO Trees [0] 4/-1/-1->3->2 [1] 4/-1/-1->3->2 [2] 4/-1/-1->3->2 [3] 4/-1/-1->3->2 [4] 4/-1/-1->3->2 [5] 4/-1/-1->3->2 [6] 4/-1/-1->3->2 [7] 4/-1/-1->3->2 [8] 4/-1/-1->3->2 [9] 4/-1/-1->3->2 [10] 4/-1/-1->3->2 [11] 4/-1/-1->3->2 [12] 4/-1/-1->3->2 [13] 4/-1/-1->3->2 [14] 4/-1/-1->3->2 [15] 4/-1/-1->3->2 [16] 4/-1/-1->3->2 [17] 4/-1/-1->3->2 [18] 4/-1/-1->3->2 [19] 4/-1/-1->3->2 [20] 4/-1/-1->3->2 [21] 4/-1/-1->3->2 [22] 4/-1/-1->3->2 [23] 4/-1/-1->3->2\u001b[0m\n",
      "\u001b[34malgo-1:240:648 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1 [2] 3/-1/-1->2->1 [3] 3/-1/-1->2->1 [4] 3/-1/-1->2->1 [5] 3/-1/-1->2->1 [6] 3/-1/-1->2->1 [7] 3/-1/-1->2->1 [8] 3/-1/-1->2->1 [9] 3/-1/-1->2->1 [10] 3/-1/-1->2->1 [11] 3/-1/-1->2->1 [12] 3/-1/-1->2->1 [13] 3/-1/-1->2->1 [14] 3/-1/-1->2->1 [15] 3/-1/-1->2->1 [16] 3/-1/-1->2->1 [17] 3/-1/-1->2->1 [18] 3/-1/-1->2->1 [19] 3/-1/-1->2->1 [20] 3/-1/-1->2->1 [21] 3/-1/-1->2->1 [22] 3/-1/-1->2->1 [23] 3/-1/-1->2->1\u001b[0m\n",
      "\u001b[34malgo-1:241:647 [3] NCCL INFO P2P Chunksize set to 524288\u001b[0m\n",
      "\u001b[34malgo-1:240:648 [2] NCCL INFO P2P Chunksize set to 524288\u001b[0m\n",
      "\u001b[34malgo-1:238:646 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1 [2] 1/-1/-1->0->-1 [3] 1/-1/-1->0->-1 [4] 1/-1/-1->0->-1 [5] 1/-1/-1->0->-1 [6] 1/-1/-1->0->-1 [7] 1/-1/-1->0->-1 [8] 1/-1/-1->0->-1 [9] 1/-1/-1->0->-1 [10] 1/-1/-1->0->-1 [11] 1/-1/-1->0->-1 [12] 1/-1/-1->0->-1 [13] 1/-1/-1->0->-1 [14] 1/-1/-1->0->-1 [15] 1/-1/-1->0->-1 [16] 1/-1/-1->0->-1 [17] 1/-1/-1->0->-1 [18] 1/-1/-1->0->-1 [19] 1/-1/-1->0->-1 [20] 1/-1/-1->0->-1 [21] 1/-1/-1->0->-1 [22] 1/-1/-1->0->-1 [23] 1/-1/-1->0->-1\u001b[0m\n",
      "\u001b[34malgo-1:238:646 [0] NCCL INFO P2P Chunksize set to 524288\u001b[0m\n",
      "\u001b[34malgo-1:243:651 [5] NCCL INFO Channel 00/0 : 5[5] -> 6[6] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:245:653 [7] NCCL INFO Channel 00/0 : 7[7] -> 0[0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:244:649 [6] NCCL INFO Channel 00/0 : 6[6] -> 7[7] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:242:652 [4] NCCL INFO Channel 00/0 : 4[4] -> 5[5] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:238:646 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[1] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:243:651 [5] NCCL INFO Channel 01/0 : 5[5] -> 6[6] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:245:653 [7] NCCL INFO Channel 01/0 : 7[7] -> 0[0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:244:649 [6] NCCL INFO Channel 01/0 : 6[6] -> 7[7] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:242:652 [4] NCCL INFO Channel 01/0 : 4[4] -> 5[5] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:238:646 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[1] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:243:651 [5] NCCL INFO Channel 02/0 : 5[5] -> 6[6] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:245:653 [7] NCCL INFO Channel 02/0 : 7[7] -> 0[0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:244:649 [6] NCCL INFO Channel 02/0 : 6[6] -> 7[7] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:240:648 [2] NCCL INFO Channel 00/0 : 2[2] -> 3[3] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:242:652 [4] NCCL INFO Channel 02/0 : 4[4] -> 5[5] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:238:646 [0] NCCL INFO Channel 02/0 : 0[0] -> 1[1] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:243:651 [5] NCCL INFO Channel 03/0 : 5[5] -> 6[6] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:241:647 [3] NCCL INFO Channel 00/0 : 3[3] -> 4[4] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:245:653 [7] NCCL INFO Channel 03/0 : 7[7] -> 0[0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:244:649 [6] NCCL INFO Channel 03/0 : 6[6] -> 7[7] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:240:648 [2] NCCL INFO Channel 01/0 : 2[2] -> 3[3] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:242:652 [4] NCCL INFO Channel 03/0 : 4[4] -> 5[5] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:238:646 [0] NCCL INFO Channel 03/0 : 0[0] -> 1[1] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:243:651 [5] NCCL INFO Channel 04/0 : 5[5] -> 6[6] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:241:647 [3] NCCL INFO Channel 01/0 : 3[3] -> 4[4] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:245:653 [7] NCCL INFO Channel 04/0 : 7[7] -> 0[0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:244:649 [6] NCCL INFO Channel 04/0 : 6[6] -> 7[7] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:240:648 [2] NCCL INFO Channel 02/0 : 2[2] -> 3[3] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:238:646 [0] NCCL INFO Channel 04/0 : 0[0] -> 1[1] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:242:652 [4] NCCL INFO Channel 04/0 : 4[4] -> 5[5] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:243:651 [5] NCCL INFO Channel 05/0 : 5[5] -> 6[6] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:241:647 [3] NCCL INFO Channel 02/0 : 3[3] -> 4[4] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:245:653 [7] NCCL INFO Channel 05/0 : 7[7] -> 0[0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:244:649 [6] NCCL INFO Channel 05/0 : 6[6] -> 7[7] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:238:646 [0] NCCL INFO Channel 05/0 : 0[0] -> 1[1] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:240:648 [2] NCCL INFO Channel 03/0 : 2[2] -> 3[3] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:242:652 [4] NCCL INFO Channel 05/0 : 4[4] -> 5[5] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:243:651 [5] NCCL INFO Channel 06/0 : 5[5] -> 6[6] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:241:647 [3] NCCL INFO Channel 03/0 : 3[3] -> 4[4] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:245:653 [7] NCCL INFO Channel 06/0 : 7[7] -> 0[0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:238:646 [0] NCCL INFO Channel 06/0 : 0[0] -> 1[1] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:240:648 [2] NCCL INFO Channel 04/0 : 2[2] -> 3[3] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:244:649 [6] NCCL INFO Channel 06/0 : 6[6] -> 7[7] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:241:647 [3] NCCL INFO Channel 04/0 : 3[3] -> 4[4] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:242:652 [4] NCCL INFO Channel 06/0 : 4[4] -> 5[5] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:243:651 [5] NCCL INFO Channel 07/0 : 5[5] -> 6[6] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:244:649 [6] NCCL INFO Channel 07/0 : 6[6] -> 7[7] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:245:653 [7] NCCL INFO Channel 07/0 : 7[7] -> 0[0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:238:646 [0] NCCL INFO Channel 07/0 : 0[0] -> 1[1] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:240:648 [2] NCCL INFO Channel 05/0 : 2[2] -> 3[3] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:242:652 [4] NCCL INFO Channel 07/0 : 4[4] -> 5[5] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:241:647 [3] NCCL INFO Channel 05/0 : 3[3] -> 4[4] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:243:651 [5] NCCL INFO Channel 08/0 : 5[5] -> 6[6] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:244:649 [6] NCCL INFO Channel 08/0 : 6[6] -> 7[7] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:240:648 [2] NCCL INFO Channel 06/0 : 2[2] -> 3[3] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:245:653 [7] NCCL INFO Channel 08/0 : 7[7] -> 0[0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:238:646 [0] NCCL INFO Channel 08/0 : 0[0] -> 1[1] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:242:652 [4] NCCL INFO Channel 08/0 : 4[4] -> 5[5] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:243:651 [5] NCCL INFO Channel 09/0 : 5[5] -> 6[6] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:241:647 [3] NCCL INFO Channel 06/0 : 3[3] -> 4[4] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:244:649 [6] NCCL INFO Channel 09/0 : 6[6] -> 7[7] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:240:648 [2] NCCL INFO Channel 07/0 : 2[2] -> 3[3] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:245:653 [7] NCCL INFO Channel 09/0 : 7[7] -> 0[0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:238:646 [0] NCCL INFO Channel 09/0 : 0[0] -> 1[1] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:242:652 [4] NCCL INFO Channel 09/0 : 4[4] -> 5[5] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:243:651 [5] NCCL INFO Channel 10/0 : 5[5] -> 6[6] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:241:647 [3] NCCL INFO Channel 07/0 : 3[3] -> 4[4] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:244:649 [6] NCCL INFO Channel 10/0 : 6[6] -> 7[7] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:245:653 [7] NCCL INFO Channel 10/0 : 7[7] -> 0[0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:240:648 [2] NCCL INFO Channel 08/0 : 2[2] -> 3[3] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:238:646 [0] NCCL INFO Channel 10/0 : 0[0] -> 1[1] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:242:652 [4] NCCL INFO Channel 10/0 : 4[4] -> 5[5] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:243:651 [5] NCCL INFO Channel 11/0 : 5[5] -> 6[6] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:241:647 [3] NCCL INFO Channel 08/0 : 3[3] -> 4[4] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:244:649 [6] NCCL INFO Channel 11/0 : 6[6] -> 7[7] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:245:653 [7] NCCL INFO Channel 11/0 : 7[7] -> 0[0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:238:646 [0] NCCL INFO Channel 11/0 : 0[0] -> 1[1] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:240:648 [2] NCCL INFO Channel 09/0 : 2[2] -> 3[3] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:242:652 [4] NCCL INFO Channel 11/0 : 4[4] -> 5[5] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:243:651 [5] NCCL INFO Channel 12/0 : 5[5] -> 6[6] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:241:647 [3] NCCL INFO Channel 09/0 : 3[3] -> 4[4] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:244:649 [6] NCCL INFO Channel 12/0 : 6[6] -> 7[7] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:245:653 [7] NCCL INFO Channel 12/0 : 7[7] -> 0[0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:238:646 [0] NCCL INFO Channel 12/0 : 0[0] -> 1[1] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:240:648 [2] NCCL INFO Channel 10/0 : 2[2] -> 3[3] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:242:652 [4] NCCL INFO Channel 12/0 : 4[4] -> 5[5] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:243:651 [5] NCCL INFO Channel 13/0 : 5[5] -> 6[6] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:239:650 [1] NCCL INFO Channel 00/0 : 1[1] -> 2[2] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:241:647 [3] NCCL INFO Channel 10/0 : 3[3] -> 4[4] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:244:649 [6] NCCL INFO Channel 13/0 : 6[6] -> 7[7] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:245:653 [7] NCCL INFO Channel 13/0 : 7[7] -> 0[0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:238:646 [0] NCCL INFO Channel 13/0 : 0[0] -> 1[1] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:240:648 [2] NCCL INFO Channel 11/0 : 2[2] -> 3[3] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:242:652 [4] NCCL INFO Channel 13/0 : 4[4] -> 5[5] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:243:651 [5] NCCL INFO Channel 14/0 : 5[5] -> 6[6] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:239:650 [1] NCCL INFO Channel 01/0 : 1[1] -> 2[2] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:241:647 [3] NCCL INFO Channel 11/0 : 3[3] -> 4[4] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:244:649 [6] NCCL INFO Channel 14/0 : 6[6] -> 7[7] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:245:653 [7] NCCL INFO Channel 14/0 : 7[7] -> 0[0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:238:646 [0] NCCL INFO Channel 14/0 : 0[0] -> 1[1] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:240:648 [2] NCCL INFO Channel 12/0 : 2[2] -> 3[3] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:242:652 [4] NCCL INFO Channel 14/0 : 4[4] -> 5[5] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:243:651 [5] NCCL INFO Channel 15/0 : 5[5] -> 6[6] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:239:650 [1] NCCL INFO Channel 02/0 : 1[1] -> 2[2] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:244:649 [6] NCCL INFO Channel 15/0 : 6[6] -> 7[7] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:241:647 [3] NCCL INFO Channel 12/0 : 3[3] -> 4[4] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:245:653 [7] NCCL INFO Channel 15/0 : 7[7] -> 0[0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:238:646 [0] NCCL INFO Channel 15/0 : 0[0] -> 1[1] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:240:648 [2] NCCL INFO Channel 13/0 : 2[2] -> 3[3] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:242:652 [4] NCCL INFO Channel 15/0 : 4[4] -> 5[5] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:243:651 [5] NCCL INFO Channel 16/0 : 5[5] -> 6[6] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:239:650 [1] NCCL INFO Channel 03/0 : 1[1] -> 2[2] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:244:649 [6] NCCL INFO Channel 16/0 : 6[6] -> 7[7] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:241:647 [3] NCCL INFO Channel 13/0 : 3[3] -> 4[4] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:245:653 [7] NCCL INFO Channel 16/0 : 7[7] -> 0[0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:238:646 [0] NCCL INFO Channel 16/0 : 0[0] -> 1[1] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:242:652 [4] NCCL INFO Channel 16/0 : 4[4] -> 5[5] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:240:648 [2] NCCL INFO Channel 14/0 : 2[2] -> 3[3] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:243:651 [5] NCCL INFO Channel 17/0 : 5[5] -> 6[6] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:239:650 [1] NCCL INFO Channel 04/0 : 1[1] -> 2[2] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:244:649 [6] NCCL INFO Channel 17/0 : 6[6] -> 7[7] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:241:647 [3] NCCL INFO Channel 14/0 : 3[3] -> 4[4] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:245:653 [7] NCCL INFO Channel 17/0 : 7[7] -> 0[0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:238:646 [0] NCCL INFO Channel 17/0 : 0[0] -> 1[1] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:242:652 [4] NCCL INFO Channel 17/0 : 4[4] -> 5[5] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:240:648 [2] NCCL INFO Channel 15/0 : 2[2] -> 3[3] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:243:651 [5] NCCL INFO Channel 18/0 : 5[5] -> 6[6] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:239:650 [1] NCCL INFO Channel 05/0 : 1[1] -> 2[2] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:244:649 [6] NCCL INFO Channel 18/0 : 6[6] -> 7[7] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:241:647 [3] NCCL INFO Channel 15/0 : 3[3] -> 4[4] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:245:653 [7] NCCL INFO Channel 18/0 : 7[7] -> 0[0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:238:646 [0] NCCL INFO Channel 18/0 : 0[0] -> 1[1] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:242:652 [4] NCCL INFO Channel 18/0 : 4[4] -> 5[5] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:240:648 [2] NCCL INFO Channel 16/0 : 2[2] -> 3[3] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:243:651 [5] NCCL INFO Channel 19/0 : 5[5] -> 6[6] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:239:650 [1] NCCL INFO Channel 06/0 : 1[1] -> 2[2] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:244:649 [6] NCCL INFO Channel 19/0 : 6[6] -> 7[7] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:241:647 [3] NCCL INFO Channel 16/0 : 3[3] -> 4[4] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:245:653 [7] NCCL INFO Channel 19/0 : 7[7] -> 0[0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:238:646 [0] NCCL INFO Channel 19/0 : 0[0] -> 1[1] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:242:652 [4] NCCL INFO Channel 19/0 : 4[4] -> 5[5] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:240:648 [2] NCCL INFO Channel 17/0 : 2[2] -> 3[3] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:243:651 [5] NCCL INFO Channel 20/0 : 5[5] -> 6[6] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:239:650 [1] NCCL INFO Channel 07/0 : 1[1] -> 2[2] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:244:649 [6] NCCL INFO Channel 20/0 : 6[6] -> 7[7] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:241:647 [3] NCCL INFO Channel 17/0 : 3[3] -> 4[4] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:245:653 [7] NCCL INFO Channel 20/0 : 7[7] -> 0[0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:238:646 [0] NCCL INFO Channel 20/0 : 0[0] -> 1[1] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:242:652 [4] NCCL INFO Channel 20/0 : 4[4] -> 5[5] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:240:648 [2] NCCL INFO Channel 18/0 : 2[2] -> 3[3] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:243:651 [5] NCCL INFO Channel 21/0 : 5[5] -> 6[6] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:239:650 [1] NCCL INFO Channel 08/0 : 1[1] -> 2[2] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:244:649 [6] NCCL INFO Channel 21/0 : 6[6] -> 7[7] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:245:653 [7] NCCL INFO Channel 21/0 : 7[7] -> 0[0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:241:647 [3] NCCL INFO Channel 18/0 : 3[3] -> 4[4] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:238:646 [0] NCCL INFO Channel 21/0 : 0[0] -> 1[1] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:242:652 [4] NCCL INFO Channel 21/0 : 4[4] -> 5[5] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:240:648 [2] NCCL INFO Channel 19/0 : 2[2] -> 3[3] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:239:650 [1] NCCL INFO Channel 09/0 : 1[1] -> 2[2] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:243:651 [5] NCCL INFO Channel 22/0 : 5[5] -> 6[6] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:244:649 [6] NCCL INFO Channel 22/0 : 6[6] -> 7[7] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:245:653 [7] NCCL INFO Channel 22/0 : 7[7] -> 0[0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:241:647 [3] NCCL INFO Channel 19/0 : 3[3] -> 4[4] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:238:646 [0] NCCL INFO Channel 22/0 : 0[0] -> 1[1] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:242:652 [4] NCCL INFO Channel 22/0 : 4[4] -> 5[5] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:240:648 [2] NCCL INFO Channel 20/0 : 2[2] -> 3[3] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:239:650 [1] NCCL INFO Channel 10/0 : 1[1] -> 2[2] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:243:651 [5] NCCL INFO Channel 23/0 : 5[5] -> 6[6] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:244:649 [6] NCCL INFO Channel 23/0 : 6[6] -> 7[7] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:245:653 [7] NCCL INFO Channel 23/0 : 7[7] -> 0[0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:241:647 [3] NCCL INFO Channel 20/0 : 3[3] -> 4[4] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:238:646 [0] NCCL INFO Channel 23/0 : 0[0] -> 1[1] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:242:652 [4] NCCL INFO Channel 23/0 : 4[4] -> 5[5] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:240:648 [2] NCCL INFO Channel 21/0 : 2[2] -> 3[3] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:239:650 [1] NCCL INFO Channel 11/0 : 1[1] -> 2[2] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:241:647 [3] NCCL INFO Channel 21/0 : 3[3] -> 4[4] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:239:650 [1] NCCL INFO Channel 12/0 : 1[1] -> 2[2] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:240:648 [2] NCCL INFO Channel 22/0 : 2[2] -> 3[3] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:241:647 [3] NCCL INFO Channel 22/0 : 3[3] -> 4[4] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:239:650 [1] NCCL INFO Channel 13/0 : 1[1] -> 2[2] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:240:648 [2] NCCL INFO Channel 23/0 : 2[2] -> 3[3] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:241:647 [3] NCCL INFO Channel 23/0 : 3[3] -> 4[4] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:239:650 [1] NCCL INFO Channel 14/0 : 1[1] -> 2[2] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:239:650 [1] NCCL INFO Channel 15/0 : 1[1] -> 2[2] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:239:650 [1] NCCL INFO Channel 16/0 : 1[1] -> 2[2] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:239:650 [1] NCCL INFO Channel 17/0 : 1[1] -> 2[2] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:239:650 [1] NCCL INFO Channel 18/0 : 1[1] -> 2[2] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:239:650 [1] NCCL INFO Channel 19/0 : 1[1] -> 2[2] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:239:650 [1] NCCL INFO Channel 20/0 : 1[1] -> 2[2] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:239:650 [1] NCCL INFO Channel 21/0 : 1[1] -> 2[2] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:239:650 [1] NCCL INFO Channel 22/0 : 1[1] -> 2[2] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:239:650 [1] NCCL INFO Channel 23/0 : 1[1] -> 2[2] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:242:652 [4] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34malgo-1:245:653 [7] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34malgo-1:245:653 [7] NCCL INFO Channel 00/0 : 7[7] -> 6[6] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:241:647 [3] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34malgo-1:243:651 [5] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34malgo-1:245:653 [7] NCCL INFO Channel 01/0 : 7[7] -> 6[6] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:245:653 [7] NCCL INFO Channel 02/0 : 7[7] -> 6[6] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:245:653 [7] NCCL INFO Channel 03/0 : 7[7] -> 6[6] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:245:653 [7] NCCL INFO Channel 04/0 : 7[7] -> 6[6] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:245:653 [7] NCCL INFO Channel 05/0 : 7[7] -> 6[6] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:245:653 [7] NCCL INFO Channel 06/0 : 7[7] -> 6[6] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:244:649 [6] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34malgo-1:245:653 [7] NCCL INFO Channel 07/0 : 7[7] -> 6[6] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:245:653 [7] NCCL INFO Channel 08/0 : 7[7] -> 6[6] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:239:650 [1] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34malgo-1:245:653 [7] NCCL INFO Channel 09/0 : 7[7] -> 6[6] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:245:653 [7] NCCL INFO Channel 10/0 : 7[7] -> 6[6] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:245:653 [7] NCCL INFO Channel 11/0 : 7[7] -> 6[6] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:245:653 [7] NCCL INFO Channel 12/0 : 7[7] -> 6[6] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:240:648 [2] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34malgo-1:245:653 [7] NCCL INFO Channel 13/0 : 7[7] -> 6[6] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:245:653 [7] NCCL INFO Channel 14/0 : 7[7] -> 6[6] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:245:653 [7] NCCL INFO Channel 15/0 : 7[7] -> 6[6] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:245:653 [7] NCCL INFO Channel 16/0 : 7[7] -> 6[6] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:245:653 [7] NCCL INFO Channel 17/0 : 7[7] -> 6[6] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:245:653 [7] NCCL INFO Channel 18/0 : 7[7] -> 6[6] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:245:653 [7] NCCL INFO Channel 19/0 : 7[7] -> 6[6] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:245:653 [7] NCCL INFO Channel 20/0 : 7[7] -> 6[6] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:238:646 [0] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34malgo-1:245:653 [7] NCCL INFO Channel 21/0 : 7[7] -> 6[6] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:245:653 [7] NCCL INFO Channel 22/0 : 7[7] -> 6[6] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:245:653 [7] NCCL INFO Channel 23/0 : 7[7] -> 6[6] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:243:651 [5] NCCL INFO Channel 00/0 : 5[5] -> 4[4] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:242:652 [4] NCCL INFO Channel 00/0 : 4[4] -> 3[3] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:243:651 [5] NCCL INFO Channel 01/0 : 5[5] -> 4[4] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:242:652 [4] NCCL INFO Channel 01/0 : 4[4] -> 3[3] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:243:651 [5] NCCL INFO Channel 02/0 : 5[5] -> 4[4] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:241:647 [3] NCCL INFO Channel 00/0 : 3[3] -> 2[2] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:242:652 [4] NCCL INFO Channel 02/0 : 4[4] -> 3[3] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:243:651 [5] NCCL INFO Channel 03/0 : 5[5] -> 4[4] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:241:647 [3] NCCL INFO Channel 01/0 : 3[3] -> 2[2] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:242:652 [4] NCCL INFO Channel 03/0 : 4[4] -> 3[3] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:243:651 [5] NCCL INFO Channel 04/0 : 5[5] -> 4[4] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:241:647 [3] NCCL INFO Channel 02/0 : 3[3] -> 2[2] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:242:652 [4] NCCL INFO Channel 04/0 : 4[4] -> 3[3] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:243:651 [5] NCCL INFO Channel 05/0 : 5[5] -> 4[4] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:241:647 [3] NCCL INFO Channel 03/0 : 3[3] -> 2[2] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:242:652 [4] NCCL INFO Channel 05/0 : 4[4] -> 3[3] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:243:651 [5] NCCL INFO Channel 06/0 : 5[5] -> 4[4] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:241:647 [3] NCCL INFO Channel 04/0 : 3[3] -> 2[2] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:242:652 [4] NCCL INFO Channel 06/0 : 4[4] -> 3[3] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:243:651 [5] NCCL INFO Channel 07/0 : 5[5] -> 4[4] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:244:649 [6] NCCL INFO Channel 00/0 : 6[6] -> 5[5] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:241:647 [3] NCCL INFO Channel 05/0 : 3[3] -> 2[2] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:242:652 [4] NCCL INFO Channel 07/0 : 4[4] -> 3[3] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:243:651 [5] NCCL INFO Channel 08/0 : 5[5] -> 4[4] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:244:649 [6] NCCL INFO Channel 01/0 : 6[6] -> 5[5] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:241:647 [3] NCCL INFO Channel 06/0 : 3[3] -> 2[2] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:242:652 [4] NCCL INFO Channel 08/0 : 4[4] -> 3[3] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:243:651 [5] NCCL INFO Channel 09/0 : 5[5] -> 4[4] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:239:650 [1] NCCL INFO Channel 00/0 : 1[1] -> 0[0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:244:649 [6] NCCL INFO Channel 02/0 : 6[6] -> 5[5] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:242:652 [4] NCCL INFO Channel 09/0 : 4[4] -> 3[3] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:241:647 [3] NCCL INFO Channel 07/0 : 3[3] -> 2[2] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:239:650 [1] NCCL INFO Channel 01/0 : 1[1] -> 0[0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:243:651 [5] NCCL INFO Channel 10/0 : 5[5] -> 4[4] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:244:649 [6] NCCL INFO Channel 03/0 : 6[6] -> 5[5] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:242:652 [4] NCCL INFO Channel 10/0 : 4[4] -> 3[3] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:241:647 [3] NCCL INFO Channel 08/0 : 3[3] -> 2[2] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:243:651 [5] NCCL INFO Channel 11/0 : 5[5] -> 4[4] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:239:650 [1] NCCL INFO Channel 02/0 : 1[1] -> 0[0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:244:649 [6] NCCL INFO Channel 04/0 : 6[6] -> 5[5] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:242:652 [4] NCCL INFO Channel 11/0 : 4[4] -> 3[3] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:241:647 [3] NCCL INFO Channel 09/0 : 3[3] -> 2[2] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:243:651 [5] NCCL INFO Channel 12/0 : 5[5] -> 4[4] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:239:650 [1] NCCL INFO Channel 03/0 : 1[1] -> 0[0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:244:649 [6] NCCL INFO Channel 05/0 : 6[6] -> 5[5] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:242:652 [4] NCCL INFO Channel 12/0 : 4[4] -> 3[3] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:241:647 [3] NCCL INFO Channel 10/0 : 3[3] -> 2[2] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:243:651 [5] NCCL INFO Channel 13/0 : 5[5] -> 4[4] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:239:650 [1] NCCL INFO Channel 04/0 : 1[1] -> 0[0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:244:649 [6] NCCL INFO Channel 06/0 : 6[6] -> 5[5] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:240:648 [2] NCCL INFO Channel 00/0 : 2[2] -> 1[1] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:242:652 [4] NCCL INFO Channel 13/0 : 4[4] -> 3[3] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:241:647 [3] NCCL INFO Channel 11/0 : 3[3] -> 2[2] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:243:651 [5] NCCL INFO Channel 14/0 : 5[5] -> 4[4] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:239:650 [1] NCCL INFO Channel 05/0 : 1[1] -> 0[0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:244:649 [6] NCCL INFO Channel 07/0 : 6[6] -> 5[5] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:242:652 [4] NCCL INFO Channel 14/0 : 4[4] -> 3[3] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:240:648 [2] NCCL INFO Channel 01/0 : 2[2] -> 1[1] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:241:647 [3] NCCL INFO Channel 12/0 : 3[3] -> 2[2] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:239:650 [1] NCCL INFO Channel 06/0 : 1[1] -> 0[0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:243:651 [5] NCCL INFO Channel 15/0 : 5[5] -> 4[4] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:242:652 [4] NCCL INFO Channel 15/0 : 4[4] -> 3[3] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:244:649 [6] NCCL INFO Channel 08/0 : 6[6] -> 5[5] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:240:648 [2] NCCL INFO Channel 02/0 : 2[2] -> 1[1] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:241:647 [3] NCCL INFO Channel 13/0 : 3[3] -> 2[2] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:239:650 [1] NCCL INFO Channel 07/0 : 1[1] -> 0[0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:243:651 [5] NCCL INFO Channel 16/0 : 5[5] -> 4[4] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:242:652 [4] NCCL INFO Channel 16/0 : 4[4] -> 3[3] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:244:649 [6] NCCL INFO Channel 09/0 : 6[6] -> 5[5] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:240:648 [2] NCCL INFO Channel 03/0 : 2[2] -> 1[1] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:241:647 [3] NCCL INFO Channel 14/0 : 3[3] -> 2[2] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:239:650 [1] NCCL INFO Channel 08/0 : 1[1] -> 0[0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:243:651 [5] NCCL INFO Channel 17/0 : 5[5] -> 4[4] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:242:652 [4] NCCL INFO Channel 17/0 : 4[4] -> 3[3] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:244:649 [6] NCCL INFO Channel 10/0 : 6[6] -> 5[5] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:240:648 [2] NCCL INFO Channel 04/0 : 2[2] -> 1[1] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:241:647 [3] NCCL INFO Channel 15/0 : 3[3] -> 2[2] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:239:650 [1] NCCL INFO Channel 09/0 : 1[1] -> 0[0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:243:651 [5] NCCL INFO Channel 18/0 : 5[5] -> 4[4] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:242:652 [4] NCCL INFO Channel 18/0 : 4[4] -> 3[3] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:244:649 [6] NCCL INFO Channel 11/0 : 6[6] -> 5[5] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:240:648 [2] NCCL INFO Channel 05/0 : 2[2] -> 1[1] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:241:647 [3] NCCL INFO Channel 16/0 : 3[3] -> 2[2] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:239:650 [1] NCCL INFO Channel 10/0 : 1[1] -> 0[0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:243:651 [5] NCCL INFO Channel 19/0 : 5[5] -> 4[4] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:242:652 [4] NCCL INFO Channel 19/0 : 4[4] -> 3[3] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:244:649 [6] NCCL INFO Channel 12/0 : 6[6] -> 5[5] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:240:648 [2] NCCL INFO Channel 06/0 : 2[2] -> 1[1] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:241:647 [3] NCCL INFO Channel 17/0 : 3[3] -> 2[2] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:239:650 [1] NCCL INFO Channel 11/0 : 1[1] -> 0[0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:243:651 [5] NCCL INFO Channel 20/0 : 5[5] -> 4[4] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:242:652 [4] NCCL INFO Channel 20/0 : 4[4] -> 3[3] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:244:649 [6] NCCL INFO Channel 13/0 : 6[6] -> 5[5] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:240:648 [2] NCCL INFO Channel 07/0 : 2[2] -> 1[1] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:241:647 [3] NCCL INFO Channel 18/0 : 3[3] -> 2[2] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:239:650 [1] NCCL INFO Channel 12/0 : 1[1] -> 0[0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:242:652 [4] NCCL INFO Channel 21/0 : 4[4] -> 3[3] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:243:651 [5] NCCL INFO Channel 21/0 : 5[5] -> 4[4] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:244:649 [6] NCCL INFO Channel 14/0 : 6[6] -> 5[5] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:240:648 [2] NCCL INFO Channel 08/0 : 2[2] -> 1[1] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:241:647 [3] NCCL INFO Channel 19/0 : 3[3] -> 2[2] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:239:650 [1] NCCL INFO Channel 13/0 : 1[1] -> 0[0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:242:652 [4] NCCL INFO Channel 22/0 : 4[4] -> 3[3] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:243:651 [5] NCCL INFO Channel 22/0 : 5[5] -> 4[4] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:244:649 [6] NCCL INFO Channel 15/0 : 6[6] -> 5[5] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:240:648 [2] NCCL INFO Channel 09/0 : 2[2] -> 1[1] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:241:647 [3] NCCL INFO Channel 20/0 : 3[3] -> 2[2] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:239:650 [1] NCCL INFO Channel 14/0 : 1[1] -> 0[0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:242:652 [4] NCCL INFO Channel 23/0 : 4[4] -> 3[3] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:243:651 [5] NCCL INFO Channel 23/0 : 5[5] -> 4[4] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:244:649 [6] NCCL INFO Channel 16/0 : 6[6] -> 5[5] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:240:648 [2] NCCL INFO Channel 10/0 : 2[2] -> 1[1] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:239:650 [1] NCCL INFO Channel 15/0 : 1[1] -> 0[0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:241:647 [3] NCCL INFO Channel 21/0 : 3[3] -> 2[2] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:244:649 [6] NCCL INFO Channel 17/0 : 6[6] -> 5[5] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:240:648 [2] NCCL INFO Channel 11/0 : 2[2] -> 1[1] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:239:650 [1] NCCL INFO Channel 16/0 : 1[1] -> 0[0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:241:647 [3] NCCL INFO Channel 22/0 : 3[3] -> 2[2] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:244:649 [6] NCCL INFO Channel 18/0 : 6[6] -> 5[5] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:239:650 [1] NCCL INFO Channel 17/0 : 1[1] -> 0[0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:240:648 [2] NCCL INFO Channel 12/0 : 2[2] -> 1[1] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:241:647 [3] NCCL INFO Channel 23/0 : 3[3] -> 2[2] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:244:649 [6] NCCL INFO Channel 19/0 : 6[6] -> 5[5] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:239:650 [1] NCCL INFO Channel 18/0 : 1[1] -> 0[0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:240:648 [2] NCCL INFO Channel 13/0 : 2[2] -> 1[1] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:244:649 [6] NCCL INFO Channel 20/0 : 6[6] -> 5[5] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:239:650 [1] NCCL INFO Channel 19/0 : 1[1] -> 0[0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:240:648 [2] NCCL INFO Channel 14/0 : 2[2] -> 1[1] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:244:649 [6] NCCL INFO Channel 21/0 : 6[6] -> 5[5] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:239:650 [1] NCCL INFO Channel 20/0 : 1[1] -> 0[0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:240:648 [2] NCCL INFO Channel 15/0 : 2[2] -> 1[1] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:244:649 [6] NCCL INFO Channel 22/0 : 6[6] -> 5[5] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:239:650 [1] NCCL INFO Channel 21/0 : 1[1] -> 0[0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:240:648 [2] NCCL INFO Channel 16/0 : 2[2] -> 1[1] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:244:649 [6] NCCL INFO Channel 23/0 : 6[6] -> 5[5] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:239:650 [1] NCCL INFO Channel 22/0 : 1[1] -> 0[0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:240:648 [2] NCCL INFO Channel 17/0 : 2[2] -> 1[1] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:239:650 [1] NCCL INFO Channel 23/0 : 1[1] -> 0[0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:240:648 [2] NCCL INFO Channel 18/0 : 2[2] -> 1[1] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:240:648 [2] NCCL INFO Channel 19/0 : 2[2] -> 1[1] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:240:648 [2] NCCL INFO Channel 20/0 : 2[2] -> 1[1] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:240:648 [2] NCCL INFO Channel 21/0 : 2[2] -> 1[1] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:240:648 [2] NCCL INFO Channel 22/0 : 2[2] -> 1[1] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:240:648 [2] NCCL INFO Channel 23/0 : 2[2] -> 1[1] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:245:653 [7] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34malgo-1:245:653 [7] NCCL INFO NCCL_PROTO set by environment to simple\u001b[0m\n",
      "\u001b[34malgo-1:245:653 [7] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34malgo-1:245:653 [7] NCCL INFO 24 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer\u001b[0m\n",
      "\u001b[34malgo-1:238:646 [0] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34malgo-1:238:646 [0] NCCL INFO NCCL_PROTO set by environment to simple\u001b[0m\n",
      "\u001b[34malgo-1:238:646 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34malgo-1:238:646 [0] NCCL INFO 24 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer\u001b[0m\n",
      "\u001b[34malgo-1:242:652 [4] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34malgo-1:242:652 [4] NCCL INFO NCCL_PROTO set by environment to simple\u001b[0m\n",
      "\u001b[34malgo-1:242:652 [4] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34malgo-1:242:652 [4] NCCL INFO 24 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer\u001b[0m\n",
      "\u001b[34malgo-1:243:651 [5] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34malgo-1:243:651 [5] NCCL INFO NCCL_PROTO set by environment to simple\u001b[0m\n",
      "\u001b[34malgo-1:243:651 [5] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34malgo-1:243:651 [5] NCCL INFO 24 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer\u001b[0m\n",
      "\u001b[34malgo-1:239:650 [1] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34malgo-1:239:650 [1] NCCL INFO NCCL_PROTO set by environment to simple\u001b[0m\n",
      "\u001b[34malgo-1:239:650 [1] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34malgo-1:239:650 [1] NCCL INFO 24 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer\u001b[0m\n",
      "\u001b[34malgo-1:244:649 [6] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34malgo-1:244:649 [6] NCCL INFO NCCL_PROTO set by environment to simple\u001b[0m\n",
      "\u001b[34malgo-1:244:649 [6] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34malgo-1:244:649 [6] NCCL INFO 24 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer\u001b[0m\n",
      "\u001b[34malgo-1:240:648 [2] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34malgo-1:240:648 [2] NCCL INFO NCCL_PROTO set by environment to simple\u001b[0m\n",
      "\u001b[34malgo-1:241:647 [3] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34malgo-1:240:648 [2] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34malgo-1:240:648 [2] NCCL INFO 24 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer\u001b[0m\n",
      "\u001b[34malgo-1:241:647 [3] NCCL INFO NCCL_PROTO set by environment to simple\u001b[0m\n",
      "\u001b[34malgo-1:241:647 [3] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34malgo-1:241:647 [3] NCCL INFO 24 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer\u001b[0m\n",
      "\u001b[34malgo-1:245:653 [7] NCCL INFO comm 0x557258f6f080 rank 7 nranks 8 cudaDev 7 nvmlDev 7 busId a01d0 commId 0xabc13d18bf88ffe5 - Init COMPLETE\u001b[0m\n",
      "\u001b[34malgo-1:241:647 [3] NCCL INFO comm 0x564f62db4b40 rank 3 nranks 8 cudaDev 3 nvmlDev 3 busId 201d0 commId 0xabc13d18bf88ffe5 - Init COMPLETE\u001b[0m\n",
      "\u001b[34malgo-1:243:651 [5] NCCL INFO comm 0x5619918da890 rank 5 nranks 8 cudaDev 5 nvmlDev 5 busId 901d0 commId 0xabc13d18bf88ffe5 - Init COMPLETE\u001b[0m\n",
      "\u001b[34malgo-1:239:650 [1] NCCL INFO comm 0x55c924273b40 rank 1 nranks 8 cudaDev 1 nvmlDev 1 busId 101d0 commId 0xabc13d18bf88ffe5 - Init COMPLETE\u001b[0m\n",
      "\u001b[34malgo-1:244:649 [6] NCCL INFO comm 0x5654f9cbfaf0 rank 6 nranks 8 cudaDev 6 nvmlDev 6 busId a01c0 commId 0xabc13d18bf88ffe5 - Init COMPLETE\u001b[0m\n",
      "\u001b[34malgo-1:240:648 [2] NCCL INFO comm 0x559e972e94e0 rank 2 nranks 8 cudaDev 2 nvmlDev 2 busId 201c0 commId 0xabc13d18bf88ffe5 - Init COMPLETE\u001b[0m\n",
      "\u001b[34malgo-1:242:652 [4] NCCL INFO comm 0x5620e3a98880 rank 4 nranks 8 cudaDev 4 nvmlDev 4 busId 901c0 commId 0xabc13d18bf88ffe5 - Init COMPLETE\u001b[0m\n",
      "\u001b[34malgo-1:238:646 [0] NCCL INFO comm 0x558a507dcb20 rank 0 nranks 8 cudaDev 0 nvmlDev 0 busId 101c0 commId 0xabc13d18bf88ffe5 - Init COMPLETE\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=16):   0%|          | 0/12000 [00:00<?, ? examples/s]\u001b[0m\n",
      "\u001b[34m[WARNING|tokenization_utils_base.py:3945] 2025-03-18 10:41:25,385 >> Token indices sequence length is longer than the specified maximum sequence length for this model (252 > 128). Running this sequence through the model will result in indexing errors\u001b[0m\n",
      "\u001b[34m[WARNING|tokenization_utils_base.py:3945] 2025-03-18 10:41:25,385 >> Token indices sequence length is longer than the specified maximum sequence length for this model (252 > 128). Running this sequence through the model will result in indexing errors\u001b[0m\n",
      "\u001b[34m[WARNING|tokenization_utils_base.py:3945] 2025-03-18 10:41:25,419 >> Token indices sequence length is longer than the specified maximum sequence length for this model (539 > 128). Running this sequence through the model will result in indexing errors\u001b[0m\n",
      "\u001b[34m[WARNING|tokenization_utils_base.py:3945] 2025-03-18 10:41:25,419 >> Token indices sequence length is longer than the specified maximum sequence length for this model (539 > 128). Running this sequence through the model will result in indexing errors\u001b[0m\n",
      "\u001b[34m[WARNING|tokenization_utils_base.py:3945] 2025-03-18 10:41:25,443 >> Token indices sequence length is longer than the specified maximum sequence length for this model (177 > 128). Running this sequence through the model will result in indexing errors\u001b[0m\n",
      "\u001b[34m[WARNING|tokenization_utils_base.py:3945] 2025-03-18 10:41:25,443 >> Token indices sequence length is longer than the specified maximum sequence length for this model (177 > 128). Running this sequence through the model will result in indexing errors\u001b[0m\n",
      "\u001b[34m[WARNING|tokenization_utils_base.py:3945] 2025-03-18 10:41:25,464 >> Token indices sequence length is longer than the specified maximum sequence length for this model (364 > 128). Running this sequence through the model will result in indexing errors\u001b[0m\n",
      "\u001b[34m[WARNING|tokenization_utils_base.py:3945] 2025-03-18 10:41:25,464 >> Token indices sequence length is longer than the specified maximum sequence length for this model (364 > 128). Running this sequence through the model will result in indexing errors\u001b[0m\n",
      "\u001b[34m[WARNING|tokenization_utils_base.py:3945] 2025-03-18 10:41:25,476 >> Token indices sequence length is longer than the specified maximum sequence length for this model (626 > 128). Running this sequence through the model will result in indexing errors\u001b[0m\n",
      "\u001b[34m[WARNING|tokenization_utils_base.py:3945] 2025-03-18 10:41:25,476 >> Token indices sequence length is longer than the specified maximum sequence length for this model (626 > 128). Running this sequence through the model will result in indexing errors\u001b[0m\n",
      "\u001b[34m[WARNING|tokenization_utils_base.py:3945] 2025-03-18 10:41:25,490 >> Token indices sequence length is longer than the specified maximum sequence length for this model (166 > 128). Running this sequence through the model will result in indexing errors\u001b[0m\n",
      "\u001b[34m[WARNING|tokenization_utils_base.py:3945] 2025-03-18 10:41:25,490 >> Token indices sequence length is longer than the specified maximum sequence length for this model (166 > 128). Running this sequence through the model will result in indexing errors\u001b[0m\n",
      "\u001b[34m[WARNING|tokenization_utils_base.py:3945] 2025-03-18 10:41:25,498 >> Token indices sequence length is longer than the specified maximum sequence length for this model (484 > 128). Running this sequence through the model will result in indexing errors\u001b[0m\n",
      "\u001b[34m[WARNING|tokenization_utils_base.py:3945] 2025-03-18 10:41:25,498 >> Token indices sequence length is longer than the specified maximum sequence length for this model (484 > 128). Running this sequence through the model will result in indexing errors\u001b[0m\n",
      "\u001b[34m[WARNING|tokenization_utils_base.py:3945] 2025-03-18 10:41:25,515 >> Token indices sequence length is longer than the specified maximum sequence length for this model (138 > 128). Running this sequence through the model will result in indexing errors\u001b[0m\n",
      "\u001b[34m[WARNING|tokenization_utils_base.py:3945] 2025-03-18 10:41:25,515 >> Token indices sequence length is longer than the specified maximum sequence length for this model (138 > 128). Running this sequence through the model will result in indexing errors\u001b[0m\n",
      "\u001b[34m[WARNING|tokenization_utils_base.py:3945] 2025-03-18 10:41:25,536 >> Token indices sequence length is longer than the specified maximum sequence length for this model (160 > 128). Running this sequence through the model will result in indexing errors\u001b[0m\n",
      "\u001b[34m[WARNING|tokenization_utils_base.py:3945] 2025-03-18 10:41:25,536 >> Token indices sequence length is longer than the specified maximum sequence length for this model (160 > 128). Running this sequence through the model will result in indexing errors\u001b[0m\n",
      "\u001b[34m[WARNING|tokenization_utils_base.py:3945] 2025-03-18 10:41:25,556 >> Token indices sequence length is longer than the specified maximum sequence length for this model (130 > 128). Running this sequence through the model will result in indexing errors\u001b[0m\n",
      "\u001b[34m[WARNING|tokenization_utils_base.py:3945] 2025-03-18 10:41:25,556 >> Token indices sequence length is longer than the specified maximum sequence length for this model (130 > 128). Running this sequence through the model will result in indexing errors\u001b[0m\n",
      "\u001b[34m[WARNING|tokenization_utils_base.py:3945] 2025-03-18 10:41:25,577 >> Token indices sequence length is longer than the specified maximum sequence length for this model (457 > 128). Running this sequence through the model will result in indexing errors\u001b[0m\n",
      "\u001b[34m[WARNING|tokenization_utils_base.py:3945] 2025-03-18 10:41:25,577 >> Token indices sequence length is longer than the specified maximum sequence length for this model (457 > 128). Running this sequence through the model will result in indexing errors\u001b[0m\n",
      "\u001b[34m[WARNING|tokenization_utils_base.py:3945] 2025-03-18 10:41:25,593 >> Token indices sequence length is longer than the specified maximum sequence length for this model (172 > 128). Running this sequence through the model will result in indexing errors\u001b[0m\n",
      "\u001b[34m[WARNING|tokenization_utils_base.py:3945] 2025-03-18 10:41:25,593 >> Token indices sequence length is longer than the specified maximum sequence length for this model (172 > 128). Running this sequence through the model will result in indexing errors\u001b[0m\n",
      "\u001b[34m[WARNING|tokenization_utils_base.py:3945] 2025-03-18 10:41:25,615 >> Token indices sequence length is longer than the specified maximum sequence length for this model (231 > 128). Running this sequence through the model will result in indexing errors\u001b[0m\n",
      "\u001b[34m[WARNING|tokenization_utils_base.py:3945] 2025-03-18 10:41:25,615 >> Token indices sequence length is longer than the specified maximum sequence length for this model (231 > 128). Running this sequence through the model will result in indexing errors\u001b[0m\n",
      "\u001b[34m[WARNING|tokenization_utils_base.py:3945] 2025-03-18 10:41:25,642 >> Token indices sequence length is longer than the specified maximum sequence length for this model (440 > 128). Running this sequence through the model will result in indexing errors\u001b[0m\n",
      "\u001b[34m[WARNING|tokenization_utils_base.py:3945] 2025-03-18 10:41:25,642 >> Token indices sequence length is longer than the specified maximum sequence length for this model (440 > 128). Running this sequence through the model will result in indexing errors\u001b[0m\n",
      "\u001b[34m[WARNING|tokenization_utils_base.py:3945] 2025-03-18 10:41:25,656 >> Token indices sequence length is longer than the specified maximum sequence length for this model (1173 > 128). Running this sequence through the model will result in indexing errors\u001b[0m\n",
      "\u001b[34m[WARNING|tokenization_utils_base.py:3945] 2025-03-18 10:41:25,656 >> Token indices sequence length is longer than the specified maximum sequence length for this model (1173 > 128). Running this sequence through the model will result in indexing errors\u001b[0m\n",
      "\u001b[34m[WARNING|tokenization_utils_base.py:3945] 2025-03-18 10:41:25,674 >> Token indices sequence length is longer than the specified maximum sequence length for this model (250 > 128). Running this sequence through the model will result in indexing errors\u001b[0m\n",
      "\u001b[34m[WARNING|tokenization_utils_base.py:3945] 2025-03-18 10:41:25,674 >> Token indices sequence length is longer than the specified maximum sequence length for this model (250 > 128). Running this sequence through the model will result in indexing errors\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=16):   6%|▋         | 750/12000 [00:01<00:19, 591.63 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=16):  44%|████▍     | 5250/12000 [00:01<00:01, 5036.75 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=16): 100%|██████████| 12000/12000 [00:01<00:00, 11797.68 examples/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset (num_proc=16): 100%|██████████| 12000/12000 [00:01<00:00, 7412.11 examples/s]\u001b[0m\n",
      "\u001b[34mtraining example:\u001b[0m\n",
      "\u001b[34minput_ids:\u001b[0m\n",
      "\u001b[34m[32013, 5719, 25, 8939, 254, 2773, 11, 2594, 245, 756, 1517, 280, 14445, 1064, 519, 14445, 279, 254, 756, 1517, 463, 274, 6910, 5875, 1019, 5359, 13, 185, 2589, 62, 2493, 405, 17625, 31702, 950, 440, 11145, 950, 440, 5466, 950, 440, 554, 950, 440, 4209, 1956, 60, 185, 185, 5618, 15481, 25, 1829, 62, 2493, 405, 9635, 185, 1459, 2649, 279, 597, 62, 2493, 25, 185, 315, 756, 62, 2493, 13, 6880, 7, 2600, 13, 5285, 2097, 938, 4683, 185, 185, 4128, 7, 1829, 62, 2493, 8, 1494, 25061, 12036, 17535, 1183, 651, 20139, 1183, 651, 2808, 1183, 651, 7427, 1183, 651, 2042, 30, 3676, 32014]\u001b[0m\n",
      "\u001b[34minputs:\u001b[0m\n",
      "\u001b[34m<｜begin▁of▁sentence｜>User: Using the input, create a new list of strings where all strings in the new list have an uppercase first letter.\u001b[0m\n",
      "\u001b[34mmy_list = [\"hello\", \"world\", \"how\", \"are\", \"you?\"]\u001b[0m\n",
      "\u001b[34mAssistant:new_list = []\u001b[0m\n",
      "\u001b[34mfor string in my_list:\n",
      "    new_list.append(string.capitalize())\u001b[0m\n",
      "\u001b[34mprint(new_list) # prints ['Hello', 'World', 'How', 'Are', 'You?']<｜end▁of▁sentence｜>\u001b[0m\n",
      "\u001b[34mlabel_ids:\u001b[0m\n",
      "\u001b[34m[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 1829, 62, 2493, 405, 9635, 185, 1459, 2649, 279, 597, 62, 2493, 25, 185, 315, 756, 62, 2493, 13, 6880, 7, 2600, 13, 5285, 2097, 938, 4683, 185, 185, 4128, 7, 1829, 62, 2493, 8, 1494, 25061, 12036, 17535, 1183, 651, 20139, 1183, 651, 2808, 1183, 651, 7427, 1183, 651, 2042, 30, 3676, 32014]\u001b[0m\n",
      "\u001b[34mlabels:\u001b[0m\n",
      "\u001b[34mnew_list = []\u001b[0m\n",
      "\u001b[34mfor string in my_list:\n",
      "    new_list.append(string.capitalize())\u001b[0m\n",
      "\u001b[34mprint(new_list) # prints ['Hello', 'World', 'How', 'Are', 'You?']<｜end▁of▁sentence｜>\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:697] 2025-03-18 10:41:26,879 >> loading configuration file /tmp/pretrain_model/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:697] 2025-03-18 10:41:26,879 >> loading configuration file /tmp/pretrain_model/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:771] 2025-03-18 10:41:26,880 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"/tmp/pretrain_model\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 32013,\n",
      "  \"eos_token_id\": 32014,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 16384,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 4.0,\n",
      "    \"rope_type\": \"linear\",\n",
      "    \"type\": \"linear\"\n",
      "  },\n",
      "  \"rope_theta\": 100000,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32256\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:771] 2025-03-18 10:41:26,880 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"/tmp/pretrain_model\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 32013,\n",
      "  \"eos_token_id\": 32014,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 11008,\n",
      "  \"max_position_embeddings\": 16384,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 4.0,\n",
      "    \"rope_type\": \"linear\",\n",
      "    \"type\": \"linear\"\n",
      "  },\n",
      "  \"rope_theta\": 100000,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.49.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32256\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:3979] 2025-03-18 10:41:26,920 >> loading weights file /tmp/pretrain_model/model.safetensors.index.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:3979] 2025-03-18 10:41:26,920 >> loading weights file /tmp/pretrain_model/model.safetensors.index.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:4162] 2025-03-18 10:41:26,920 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:4162] 2025-03-18 10:41:26,920 >> Detected DeepSpeed ZeRO-3: activating zero.init() for this model\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:26,921] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 8\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:1140] 2025-03-18 10:41:26,932 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 32013,\n",
      "  \"eos_token_id\": 32014\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:1140] 2025-03-18 10:41:26,932 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 32013,\n",
      "  \"eos_token_id\": 32014\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:26,989] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 8\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:26,989] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 8\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:26,989] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 8\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:26,993] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 8\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:26,994] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 8\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:26,994] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 8\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:26,995] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 8\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:27,217] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 291, num_elems = 6.74B\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.80s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.82s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.82s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.87s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.87s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.87s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.87s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.30s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.37s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.32s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.40s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.32s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.40s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.34s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.42s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.34s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.42s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.34s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.42s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.34s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.42s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.15s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.13s/it]\u001b[0m\n",
      "\u001b[34mLoading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.29s/it]\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:4970] 2025-03-18 10:41:31,807 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:4970] 2025-03-18 10:41:31,807 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:4978] 2025-03-18 10:41:31,807 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at /tmp/pretrain_model.\u001b[0m\n",
      "\u001b[34mIf your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:4978] 2025-03-18 10:41:31,807 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at /tmp/pretrain_model.\u001b[0m\n",
      "\u001b[34mIf your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:1093] 2025-03-18 10:41:31,876 >> loading configuration file /tmp/pretrain_model/generation_config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:1093] 2025-03-18 10:41:31,876 >> loading configuration file /tmp/pretrain_model/generation_config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:1140] 2025-03-18 10:41:31,877 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 32013,\n",
      "  \"eos_token_id\": 32014\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:1140] 2025-03-18 10:41:31,877 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 32013,\n",
      "  \"eos_token_id\": 32014\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[INFO|2025-03-18 10:41:31] llamafactory.model.model_utils.checkpointing:157 >> Gradient checkpointing enabled.\u001b[0m\n",
      "\u001b[34m[INFO|2025-03-18 10:41:31] llamafactory.model.model_utils.attention:157 >> Using vanilla attention implementation.\u001b[0m\n",
      "\u001b[34m[INFO|2025-03-18 10:41:31] llamafactory.model.adapter:157 >> ZeRO3 / FSDP detected, remaining trainable params in float32.\u001b[0m\n",
      "\u001b[34m[INFO|2025-03-18 10:41:31] llamafactory.model.adapter:157 >> Fine-tuning method: Full\u001b[0m\n",
      "\u001b[34m[INFO|2025-03-18 10:41:31] llamafactory.model.loader:157 >> trainable params: 6,740,512,768 || all params: 6,740,512,768 || trainable%: 100.0000\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:746] 2025-03-18 10:41:31,896 >> Using auto half precision backend\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:746] 2025-03-18 10:41:31,896 >> Using auto half precision backend\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:32,133] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed info: version=0.15.4, git-hash=unknown, git-branch=unknown\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:32,133] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 8\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:32,141] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:32,142] [INFO] [logging.py:128:log_dist] [Rank 0] Using client Optimizer as basic optimizer\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:32,142] [INFO] [logging.py:128:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:32,153] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:32,153] [INFO] [utils.py:59:is_zero_supported_optimizer] Checking ZeRO support for optimizer=AdamW type=<class 'torch.optim.adamw.AdamW'>\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:32,153] [INFO] [logging.py:128:log_dist] [Rank 0] Creating fp16 ZeRO stage 3 optimizer, MiCS is enabled False, Hierarchical params gather False\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:32,153] [INFO] [logging.py:128:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 3 optimizer\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:32,358] [INFO] [utils.py:781:see_memory_usage] Stage 3 initialize beginning\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:32,359] [INFO] [utils.py:782:see_memory_usage] MA 1.57 GB         Max_MA 2.28 GB         CA 2.08 GB         Max_CA 3 GB\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:32,359] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 38.68 GB, percent = 3.4%\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:32,361] [INFO] [stage3.py:166:__init__] Reduce bucket size 16777216\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:32,361] [INFO] [stage3.py:167:__init__] Prefetch bucket size 15099494\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:32,564] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [begin]\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:32,564] [INFO] [utils.py:782:see_memory_usage] MA 1.57 GB         Max_MA 1.57 GB         CA 2.08 GB         Max_CA 2 GB\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:32,564] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 38.68 GB, percent = 3.4%\u001b[0m\n",
      "\u001b[34mParameter Offload: Total persistent parameters: 266240 in 65 params\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:32,787] [INFO] [utils.py:781:see_memory_usage] DeepSpeedZeRoOffload initialize [end]\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:32,788] [INFO] [utils.py:782:see_memory_usage] MA 1.57 GB         Max_MA 1.57 GB         CA 2.08 GB         Max_CA 2 GB\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:32,788] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 38.68 GB, percent = 3.4%\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:32,995] [INFO] [utils.py:781:see_memory_usage] Before creating fp16 partitions\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:32,995] [INFO] [utils.py:782:see_memory_usage] MA 1.57 GB         Max_MA 1.57 GB         CA 2.08 GB         Max_CA 2 GB\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:32,995] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 38.68 GB, percent = 3.4%\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:35,229] [INFO] [utils.py:781:see_memory_usage] After creating fp16 partitions: 2\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:35,230] [INFO] [utils.py:782:see_memory_usage] MA 1.57 GB         Max_MA 1.57 GB         CA 1.57 GB         Max_CA 2 GB\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:35,230] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 38.67 GB, percent = 3.4%\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:35,439] [INFO] [utils.py:781:see_memory_usage] Before creating fp32 partitions\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:35,440] [INFO] [utils.py:782:see_memory_usage] MA 1.57 GB         Max_MA 1.57 GB         CA 1.57 GB         Max_CA 2 GB\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:35,440] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 38.67 GB, percent = 3.4%\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:35,650] [INFO] [utils.py:781:see_memory_usage] After creating fp32 partitions\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:35,650] [INFO] [utils.py:782:see_memory_usage] MA 4.71 GB         Max_MA 6.28 GB         CA 6.28 GB         Max_CA 6 GB\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:35,651] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 38.67 GB, percent = 3.4%\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:35,857] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:35,858] [INFO] [utils.py:782:see_memory_usage] MA 4.71 GB         Max_MA 4.71 GB         CA 6.28 GB         Max_CA 6 GB\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:35,858] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 38.67 GB, percent = 3.4%\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:36,065] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:36,066] [INFO] [utils.py:782:see_memory_usage] MA 4.71 GB         Max_MA 7.85 GB         CA 9.42 GB         Max_CA 9 GB\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:36,066] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 38.67 GB, percent = 3.4%\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:36,066] [INFO] [stage3.py:521:_setup_for_real_optimizer] optimizer state initialized\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:36,438] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:36,439] [INFO] [utils.py:782:see_memory_usage] MA 6.31 GB         Max_MA 6.8 GB         CA 10.08 GB         Max_CA 10 GB\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:36,439] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 39.36 GB, percent = 3.5%\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:36,439] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer_Stage3\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:36,439] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = None\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:36,439] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:36,439] [INFO] [logging.py:128:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0, 0.0], mom=[(0.9, 0.999), (0.9, 0.999)]\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:36,441] [INFO] [config.py:999:print] DeepSpeedEngine configuration:\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:36,441] [INFO] [config.py:1003:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:36,441] [INFO] [config.py:1003:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:36,441] [INFO] [config.py:1003:print]   amp_enabled .................. False\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:36,441] [INFO] [config.py:1003:print]   amp_params ................... False\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:36,442] [INFO] [config.py:1003:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:36,442] [INFO] [config.py:1003:print]   bfloat16_enabled ............. True\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:36,442] [INFO] [config.py:1003:print]   bfloat16_immediate_grad_update  False\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:36,442] [INFO] [config.py:1003:print]   checkpoint_parallel_write_pipeline  False\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:36,442] [INFO] [config.py:1003:print]   checkpoint_tag_validation_enabled  True\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:36,442] [INFO] [config.py:1003:print]   checkpoint_tag_validation_fail  False\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:36,442] [INFO] [config.py:1003:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f10cb252ec0>\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:36,442] [INFO] [config.py:1003:print]   communication_data_type ...... None\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:36,442] [INFO] [config.py:1003:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:36,442] [INFO] [config.py:1003:print]   curriculum_enabled_legacy .... False\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:36,442] [INFO] [config.py:1003:print]   curriculum_params_legacy ..... False\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:36,442] [INFO] [config.py:1003:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:36,442] [INFO] [config.py:1003:print]   data_efficiency_enabled ...... False\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:36,442] [INFO] [config.py:1003:print]   dataloader_drop_last ......... False\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:36,442] [INFO] [config.py:1003:print]   disable_allgather ............ False\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:36,442] [INFO] [config.py:1003:print]   dump_state ................... False\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:36,442] [INFO] [config.py:1003:print]   dynamic_loss_scale_args ...... None\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:36,442] [INFO] [config.py:1003:print]   eigenvalue_enabled ........... False\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:36,442] [INFO] [config.py:1003:print]   eigenvalue_gas_boundary_resolution  1\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:36,442] [INFO] [config.py:1003:print]   eigenvalue_layer_name ........ bert.encoder.layer\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:36,442] [INFO] [config.py:1003:print]   eigenvalue_layer_num ......... 0\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:36,442] [INFO] [config.py:1003:print]   eigenvalue_max_iter .......... 100\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:36,442] [INFO] [config.py:1003:print]   eigenvalue_stability ......... 1e-06\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:36,442] [INFO] [config.py:1003:print]   eigenvalue_tol ............... 0.01\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:36,442] [INFO] [config.py:1003:print]   eigenvalue_verbose ........... False\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:36,442] [INFO] [config.py:1003:print]   elasticity_enabled ........... False\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:36,442] [INFO] [config.py:1003:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:36,442] [INFO] [config.py:1003:print]   fp16_auto_cast ............... None\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:36,442] [INFO] [config.py:1003:print]   fp16_enabled ................. False\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:36,442] [INFO] [config.py:1003:print]   fp16_master_weights_and_gradients  False\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:36,442] [INFO] [config.py:1003:print]   global_rank .................. 0\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:36,442] [INFO] [config.py:1003:print]   grad_accum_dtype ............. None\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:36,442] [INFO] [config.py:1003:print]   gradient_accumulation_steps .. 4\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:36,442] [INFO] [config.py:1003:print]   gradient_clipping ............ 1.0\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:36,442] [INFO] [config.py:1003:print]   gradient_predivide_factor .... 1.0\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:36,442] [INFO] [config.py:1003:print]   graph_harvesting ............. False\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:36,442] [INFO] [config.py:1003:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:36,442] [INFO] [config.py:1003:print]   initial_dynamic_scale ........ 1\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:36,442] [INFO] [config.py:1003:print]   load_universal_checkpoint .... False\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:36,442] [INFO] [config.py:1003:print]   loss_scale ................... 1.0\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:36,442] [INFO] [config.py:1003:print]   memory_breakdown ............. False\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:36,443] [INFO] [config.py:1003:print]   mics_hierarchial_params_gather  False\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:36,443] [INFO] [config.py:1003:print]   mics_shard_size .............. -1\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:36,443] [INFO] [config.py:1003:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:36,443] [INFO] [config.py:1003:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:36,443] [INFO] [config.py:1003:print]   optimizer_legacy_fusion ...... False\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:36,443] [INFO] [config.py:1003:print]   optimizer_name ............... None\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:36,443] [INFO] [config.py:1003:print]   optimizer_params ............. None\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:36,443] [INFO] [config.py:1003:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:36,443] [INFO] [config.py:1003:print]   pld_enabled .................. False\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:36,443] [INFO] [config.py:1003:print]   pld_params ................... False\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:36,443] [INFO] [config.py:1003:print]   prescale_gradients ........... False\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:36,443] [INFO] [config.py:1003:print]   scheduler_name ............... None\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:36,443] [INFO] [config.py:1003:print]   scheduler_params ............. None\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:36,443] [INFO] [config.py:1003:print]   seq_parallel_communication_data_type  torch.float32\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:36,443] [INFO] [config.py:1003:print]   sparse_attention ............. None\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:36,443] [INFO] [config.py:1003:print]   sparse_gradients_enabled ..... False\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:36,443] [INFO] [config.py:1003:print]   steps_per_print .............. inf\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:36,443] [INFO] [config.py:1003:print]   timers_config ................ enabled=True synchronized=True\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:36,443] [INFO] [config.py:1003:print]   train_batch_size ............. 32\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:36,443] [INFO] [config.py:1003:print]   train_micro_batch_size_per_gpu  1\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:36,443] [INFO] [config.py:1003:print]   use_data_before_expert_parallel_  False\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:36,443] [INFO] [config.py:1003:print]   use_node_local_storage ....... True\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:36,443] [INFO] [config.py:1003:print]   wall_clock_breakdown ......... False\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:36,443] [INFO] [config.py:1003:print]   weight_quantization_config ... None\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:36,443] [INFO] [config.py:1003:print]   world_size ................... 8\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:36,443] [INFO] [config.py:1003:print]   zero_allow_untested_optimizer  True\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:36,443] [INFO] [config.py:1003:print]   zero_config .................. stage=3 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=16777216 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=15099494 param_persistence_threshold=40960 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=True use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:36,443] [INFO] [config.py:1003:print]   zero_enabled ................. True\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:36,443] [INFO] [config.py:1003:print]   zero_force_ds_cpu_optimizer .. True\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:36,443] [INFO] [config.py:1003:print]   zero_optimization_stage ...... 3\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:41:36,443] [INFO] [config.py:989:print_user_config]   json = {\n",
      "    \"train_batch_size\": 32, \n",
      "    \"train_micro_batch_size_per_gpu\": 1, \n",
      "    \"gradient_accumulation_steps\": 4, \n",
      "    \"gradient_clipping\": 1.0, \n",
      "    \"zero_allow_untested_optimizer\": true, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false, \n",
      "        \"loss_scale\": 0, \n",
      "        \"loss_scale_window\": 1000, \n",
      "        \"initial_scale_power\": 16, \n",
      "        \"hysteresis\": 2, \n",
      "        \"min_loss_scale\": 1\n",
      "    }, \n",
      "    \"bf16\": {\n",
      "        \"enabled\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 3, \n",
      "        \"overlap_comm\": false, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09, \n",
      "        \"reduce_bucket_size\": 1.677722e+07, \n",
      "        \"stage3_prefetch_bucket_size\": 1.509949e+07, \n",
      "        \"stage3_param_persistence_threshold\": 4.096000e+04, \n",
      "        \"stage3_max_live_parameters\": 1.000000e+09, \n",
      "        \"stage3_max_reuse_distance\": 1.000000e+09, \n",
      "        \"stage3_gather_16bit_weights_on_model_save\": true\n",
      "    }, \n",
      "    \"checkpoint\": {\n",
      "        \"use_node_local_storage\": true\n",
      "    }, \n",
      "    \"steps_per_print\": inf\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2405] 2025-03-18 10:41:36,445 >> ***** Running training *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2405] 2025-03-18 10:41:36,445 >> ***** Running training *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2406] 2025-03-18 10:41:36,445 >>   Num examples = 10,800\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2407] 2025-03-18 10:41:36,445 >>   Num Epochs = 1\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2406] 2025-03-18 10:41:36,445 >>   Num examples = 10,800\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2407] 2025-03-18 10:41:36,445 >>   Num Epochs = 1\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2408] 2025-03-18 10:41:36,445 >>   Instantaneous batch size per device = 1\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2408] 2025-03-18 10:41:36,445 >>   Instantaneous batch size per device = 1\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2411] 2025-03-18 10:41:36,445 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2412] 2025-03-18 10:41:36,445 >>   Gradient Accumulation steps = 4\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2413] 2025-03-18 10:41:36,445 >>   Total optimization steps = 337\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2411] 2025-03-18 10:41:36,445 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2412] 2025-03-18 10:41:36,445 >>   Gradient Accumulation steps = 4\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2413] 2025-03-18 10:41:36,445 >>   Total optimization steps = 337\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2414] 2025-03-18 10:41:36,446 >>   Number of trainable parameters = 6,740,512,768\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2414] 2025-03-18 10:41:36,446 >>   Number of trainable parameters = 6,740,512,768\u001b[0m\n",
      "\u001b[34m[INFO|integration_utils.py:817] 2025-03-18 10:41:36,447 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\u001b[0m\n",
      "\u001b[34m[INFO|integration_utils.py:817] 2025-03-18 10:41:36,447 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\u001b[0m\n",
      "\u001b[34mwandb: WARNING The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\u001b[0m\n",
      "\u001b[34mwandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\u001b[0m\n",
      "\u001b[34mwandb: Currently logged in as: 407383787 to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\u001b[0m\n",
      "\u001b[34mwandb: Tracking run with wandb version 0.19.8\u001b[0m\n",
      "\u001b[34mwandb: Run data is saved locally in /opt/ml/code/wandb/run-20250318_104136-deepseek6-7B-finetune-2025-03-18-10-29-00-527-3pztry-algo-1\u001b[0m\n",
      "\u001b[34mwandb: Run `wandb offline` to turn off syncing.\u001b[0m\n",
      "\u001b[34mwandb: Syncing run /tmp/finetuned_model\u001b[0m\n",
      "\u001b[34mwandb: ⭐️ View project at https://wandb.ai/407383787/llamafactory\u001b[0m\n",
      "\u001b[34mwandb: 🚀 View run at https://wandb.ai/407383787/llamafactory/runs/deepseek6-7B-finetune-2025-03-18-10-29-00-527-3pztry-algo-1\u001b[0m\n",
      "\u001b[34m0%|          | 0/337 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34malgo-1:238:979 [0] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34malgo-1:238:979 [0] NCCL INFO DMA-BUF is available on GPU device 0\u001b[0m\n",
      "\u001b[34malgo-1:241:980 [3] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34malgo-1:240:981 [2] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34malgo-1:241:980 [3] NCCL INFO DMA-BUF is available on GPU device 3\u001b[0m\n",
      "\u001b[34malgo-1:240:981 [2] NCCL INFO DMA-BUF is available on GPU device 2\u001b[0m\n",
      "\u001b[34malgo-1:239:982 [1] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34malgo-1:239:982 [1] NCCL INFO DMA-BUF is available on GPU device 1\u001b[0m\n",
      "\u001b[34malgo-1:242:983 [4] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34malgo-1:242:983 [4] NCCL INFO DMA-BUF is available on GPU device 4\u001b[0m\n",
      "\u001b[34malgo-1:245:984 [7] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34malgo-1:245:984 [7] NCCL INFO DMA-BUF is available on GPU device 7\u001b[0m\n",
      "\u001b[34malgo-1:244:985 [6] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34malgo-1:244:985 [6] NCCL INFO DMA-BUF is available on GPU device 6\u001b[0m\n",
      "\u001b[34malgo-1:243:986 [5] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34malgo-1:243:986 [5] NCCL INFO DMA-BUF is available on GPU device 5\u001b[0m\n",
      "\u001b[34malgo-1:241:980 [3] NCCL INFO comm 0x7f094005cde0 rank 3 nranks 8 cudaDev 3 nvmlDev 3 busId 201d0 commId 0x16dd7c1664f67960 - Init START\u001b[0m\n",
      "\u001b[34malgo-1:245:984 [7] NCCL INFO comm 0x7fe32005ce20 rank 7 nranks 8 cudaDev 7 nvmlDev 7 busId a01d0 commId 0x16dd7c1664f67960 - Init START\u001b[0m\n",
      "\u001b[34malgo-1:240:981 [2] NCCL INFO comm 0x7f816405cdd0 rank 2 nranks 8 cudaDev 2 nvmlDev 2 busId 201c0 commId 0x16dd7c1664f67960 - Init START\u001b[0m\n",
      "\u001b[34malgo-1:244:985 [6] NCCL INFO comm 0x7f54c405cf20 rank 6 nranks 8 cudaDev 6 nvmlDev 6 busId a01c0 commId 0x16dd7c1664f67960 - Init START\u001b[0m\n",
      "\u001b[34malgo-1:243:986 [5] NCCL INFO comm 0x7fc48405d250 rank 5 nranks 8 cudaDev 5 nvmlDev 5 busId 901d0 commId 0x16dd7c1664f67960 - Init START\u001b[0m\n",
      "\u001b[34malgo-1:242:983 [4] NCCL INFO comm 0x7f647c05cfd0 rank 4 nranks 8 cudaDev 4 nvmlDev 4 busId 901c0 commId 0x16dd7c1664f67960 - Init START\u001b[0m\n",
      "\u001b[34malgo-1:238:979 [0] NCCL INFO comm 0x7f0f1005d700 rank 0 nranks 8 cudaDev 0 nvmlDev 0 busId 101c0 commId 0x16dd7c1664f67960 - Init START\u001b[0m\n",
      "\u001b[34malgo-1:239:982 [1] NCCL INFO comm 0x7f615c05ce60 rank 1 nranks 8 cudaDev 1 nvmlDev 1 busId 101d0 commId 0x16dd7c1664f67960 - Init START\u001b[0m\n",
      "\u001b[34malgo-1:241:980 [3] NCCL INFO NCCL_TOPO_FILE set by environment to /opt/conda/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[34malgo-1:240:981 [2] NCCL INFO NCCL_TOPO_FILE set by environment to /opt/conda/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[34malgo-1:243:986 [5] NCCL INFO NCCL_TOPO_FILE set by environment to /opt/conda/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[34malgo-1:244:985 [6] NCCL INFO NCCL_TOPO_FILE set by environment to /opt/conda/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[34malgo-1:245:984 [7] NCCL INFO NCCL_TOPO_FILE set by environment to /opt/conda/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[34malgo-1:242:983 [4] NCCL INFO NCCL_TOPO_FILE set by environment to /opt/conda/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[34malgo-1:238:979 [0] NCCL INFO NCCL_TOPO_FILE set by environment to /opt/conda/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[34malgo-1:239:982 [1] NCCL INFO NCCL_TOPO_FILE set by environment to /opt/conda/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[34malgo-1:239:982 [1] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:239:982 [1] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:239:982 [1] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:239:982 [1] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:241:980 [3] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:241:980 [3] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:241:980 [3] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:241:980 [3] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:242:983 [4] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:242:983 [4] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:242:983 [4] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:242:983 [4] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:240:981 [2] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:240:981 [2] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:240:981 [2] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:240:981 [2] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:243:986 [5] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:243:986 [5] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:243:986 [5] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:243:986 [5] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:245:984 [7] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:245:984 [7] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:245:984 [7] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:245:984 [7] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:238:979 [0] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:238:979 [0] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:238:979 [0] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:238:979 [0] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:239:982 [1] NCCL INFO Setting affinity for GPU 1 to ff,ffff0000,00ffffff\u001b[0m\n",
      "\u001b[34malgo-1:239:982 [1] NCCL INFO NVLS multicast support is not available on dev 1\u001b[0m\n",
      "\u001b[34malgo-1:244:985 [6] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:244:985 [6] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:244:985 [6] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:244:985 [6] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:241:980 [3] NCCL INFO Setting affinity for GPU 3 to ff,ffff0000,00ffffff\u001b[0m\n",
      "\u001b[34malgo-1:241:980 [3] NCCL INFO NVLS multicast support is not available on dev 3\u001b[0m\n",
      "\u001b[34malgo-1:242:983 [4] NCCL INFO Setting affinity for GPU 4 to ffffff00,0000ffff,ff000000\u001b[0m\n",
      "\u001b[34malgo-1:242:983 [4] NCCL INFO NVLS multicast support is not available on dev 4\u001b[0m\n",
      "\u001b[34malgo-1:240:981 [2] NCCL INFO Setting affinity for GPU 2 to ff,ffff0000,00ffffff\u001b[0m\n",
      "\u001b[34malgo-1:240:981 [2] NCCL INFO NVLS multicast support is not available on dev 2\u001b[0m\n",
      "\u001b[34malgo-1:243:986 [5] NCCL INFO Setting affinity for GPU 5 to ffffff00,0000ffff,ff000000\u001b[0m\n",
      "\u001b[34malgo-1:243:986 [5] NCCL INFO NVLS multicast support is not available on dev 5\u001b[0m\n",
      "\u001b[34malgo-1:245:984 [7] NCCL INFO Setting affinity for GPU 7 to ffffff00,0000ffff,ff000000\u001b[0m\n",
      "\u001b[34malgo-1:245:984 [7] NCCL INFO NVLS multicast support is not available on dev 7\u001b[0m\n",
      "\u001b[34malgo-1:238:979 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffff0000,00ffffff\u001b[0m\n",
      "\u001b[34malgo-1:238:979 [0] NCCL INFO NVLS multicast support is not available on dev 0\u001b[0m\n",
      "\u001b[34malgo-1:244:985 [6] NCCL INFO Setting affinity for GPU 6 to ffffff00,0000ffff,ff000000\u001b[0m\n",
      "\u001b[34malgo-1:244:985 [6] NCCL INFO NVLS multicast support is not available on dev 6\u001b[0m\n",
      "\u001b[34malgo-1:244:985 [6] NCCL INFO Trees [0] 7/-1/-1->6->5 [1] 7/-1/-1->6->5 [2] 7/-1/-1->6->5 [3] 7/-1/-1->6->5 [4] 7/-1/-1->6->5 [5] 7/-1/-1->6->5 [6] 7/-1/-1->6->5 [7] 7/-1/-1->6->5 [8] 7/-1/-1->6->5 [9] 7/-1/-1->6->5 [10] 7/-1/-1->6->5 [11] 7/-1/-1->6->5 [12] 7/-1/-1->6->5 [13] 7/-1/-1->6->5 [14] 7/-1/-1->6->5 [15] 7/-1/-1->6->5 [16] 7/-1/-1->6->5 [17] 7/-1/-1->6->5 [18] 7/-1/-1->6->5 [19] 7/-1/-1->6->5 [20] 7/-1/-1->6->5 [21] 7/-1/-1->6->5 [22] 7/-1/-1->6->5 [23] 7/-1/-1->6->5\u001b[0m\n",
      "\u001b[34malgo-1:238:979 [0] NCCL INFO Channel 00/24 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34malgo-1:243:986 [5] NCCL INFO Trees [0] 6/-1/-1->5->4 [1] 6/-1/-1->5->4 [2] 6/-1/-1->5->4 [3] 6/-1/-1->5->4 [4] 6/-1/-1->5->4 [5] 6/-1/-1->5->4 [6] 6/-1/-1->5->4 [7] 6/-1/-1->5->4 [8] 6/-1/-1->5->4 [9] 6/-1/-1->5->4 [10] 6/-1/-1->5->4 [11] 6/-1/-1->5->4 [12] 6/-1/-1->5->4 [13] 6/-1/-1->5->4 [14] 6/-1/-1->5->4 [15] 6/-1/-1->5->4 [16] 6/-1/-1->5->4 [17] 6/-1/-1->5->4 [18] 6/-1/-1->5->4 [19] 6/-1/-1->5->4 [20] 6/-1/-1->5->4 [21] 6/-1/-1->5->4 [22] 6/-1/-1->5->4 [23] 6/-1/-1->5->4\u001b[0m\n",
      "\u001b[34malgo-1:238:979 [0] NCCL INFO Channel 01/24 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34malgo-1:244:985 [6] NCCL INFO P2P Chunksize set to 524288\u001b[0m\n",
      "\u001b[34malgo-1:241:980 [3] NCCL INFO Trees [0] 4/-1/-1->3->2 [1] 4/-1/-1->3->2 [2] 4/-1/-1->3->2 [3] 4/-1/-1->3->2 [4] 4/-1/-1->3->2 [5] 4/-1/-1->3->2 [6] 4/-1/-1->3->2 [7] 4/-1/-1->3->2 [8] 4/-1/-1->3->2 [9] 4/-1/-1->3->2 [10] 4/-1/-1->3->2 [11] 4/-1/-1->3->2 [12] 4/-1/-1->3->2 [13] 4/-1/-1->3->2 [14] 4/-1/-1->3->2 [15] 4/-1/-1->3->2 [16] 4/-1/-1->3->2 [17] 4/-1/-1->3->2 [18] 4/-1/-1->3->2 [19] 4/-1/-1->3->2 [20] 4/-1/-1->3->2 [21] 4/-1/-1->3->2 [22] 4/-1/-1->3->2 [23] 4/-1/-1->3->2\u001b[0m\n",
      "\u001b[34malgo-1:243:986 [5] NCCL INFO P2P Chunksize set to 524288\u001b[0m\n",
      "\u001b[34malgo-1:238:979 [0] NCCL INFO Channel 02/24 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34malgo-1:242:983 [4] NCCL INFO Trees [0] 5/-1/-1->4->3 [1] 5/-1/-1->4->3 [2] 5/-1/-1->4->3 [3] 5/-1/-1->4->3 [4] 5/-1/-1->4->3 [5] 5/-1/-1->4->3 [6] 5/-1/-1->4->3 [7] 5/-1/-1->4->3 [8] 5/-1/-1->4->3 [9] 5/-1/-1->4->3 [10] 5/-1/-1->4->3 [11] 5/-1/-1->4->3 [12] 5/-1/-1->4->3 [13] 5/-1/-1->4->3 [14] 5/-1/-1->4->3 [15] 5/-1/-1->4->3 [16] 5/-1/-1->4->3 [17] 5/-1/-1->4->3 [18] 5/-1/-1->4->3 [19] 5/-1/-1->4->3 [20] 5/-1/-1->4->3 [21] 5/-1/-1->4->3 [22] 5/-1/-1->4->3 [23] 5/-1/-1->4->3\u001b[0m\n",
      "\u001b[34malgo-1:241:980 [3] NCCL INFO P2P Chunksize set to 524288\u001b[0m\n",
      "\u001b[34malgo-1:238:979 [0] NCCL INFO Channel 03/24 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34malgo-1:242:983 [4] NCCL INFO P2P Chunksize set to 524288\u001b[0m\n",
      "\u001b[34malgo-1:240:981 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1 [2] 3/-1/-1->2->1 [3] 3/-1/-1->2->1 [4] 3/-1/-1->2->1 [5] 3/-1/-1->2->1 [6] 3/-1/-1->2->1 [7] 3/-1/-1->2->1 [8] 3/-1/-1->2->1 [9] 3/-1/-1->2->1 [10] 3/-1/-1->2->1 [11] 3/-1/-1->2->1 [12] 3/-1/-1->2->1 [13] 3/-1/-1->2->1 [14] 3/-1/-1->2->1 [15] 3/-1/-1->2->1 [16] 3/-1/-1->2->1 [17] 3/-1/-1->2->1 [18] 3/-1/-1->2->1 [19] 3/-1/-1->2->1 [20] 3/-1/-1->2->1 [21] 3/-1/-1->2->1 [22] 3/-1/-1->2->1 [23] 3/-1/-1->2->1\u001b[0m\n",
      "\u001b[34malgo-1:238:979 [0] NCCL INFO Channel 04/24 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34malgo-1:245:984 [7] NCCL INFO Trees [0] -1/-1/-1->7->6 [1] -1/-1/-1->7->6 [2] -1/-1/-1->7->6 [3] -1/-1/-1->7->6 [4] -1/-1/-1->7->6 [5] -1/-1/-1->7->6 [6] -1/-1/-1->7->6 [7] -1/-1/-1->7->6 [8] -1/-1/-1->7->6 [9] -1/-1/-1->7->6 [10] -1/-1/-1->7->6 [11] -1/-1/-1->7->6 [12] -1/-1/-1->7->6 [13] -1/-1/-1->7->6 [14] -1/-1/-1->7->6 [15] -1/-1/-1->7->6 [16] -1/-1/-1->7->6 [17] -1/-1/-1->7->6 [18] -1/-1/-1->7->6 [19] -1/-1/-1->7->6 [20] -1/-1/-1->7->6 [21] -1/-1/-1->7->6 [22] -1/-1/-1->7->6 [23] -1/-1/-1->7->6\u001b[0m\n",
      "\u001b[34malgo-1:240:981 [2] NCCL INFO P2P Chunksize set to 524288\u001b[0m\n",
      "\u001b[34malgo-1:238:979 [0] NCCL INFO Channel 05/24 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34malgo-1:245:984 [7] NCCL INFO P2P Chunksize set to 524288\u001b[0m\n",
      "\u001b[34malgo-1:238:979 [0] NCCL INFO Channel 06/24 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34malgo-1:239:982 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0 [2] 2/-1/-1->1->0 [3] 2/-1/-1->1->0 [4] 2/-1/-1->1->0 [5] 2/-1/-1->1->0 [6] 2/-1/-1->1->0 [7] 2/-1/-1->1->0 [8] 2/-1/-1->1->0 [9] 2/-1/-1->1->0 [10] 2/-1/-1->1->0 [11] 2/-1/-1->1->0 [12] 2/-1/-1->1->0 [13] 2/-1/-1->1->0 [14] 2/-1/-1->1->0 [15] 2/-1/-1->1->0 [16] 2/-1/-1->1->0 [17] 2/-1/-1->1->0 [18] 2/-1/-1->1->0 [19] 2/-1/-1->1->0 [20] 2/-1/-1->1->0 [21] 2/-1/-1->1->0 [22] 2/-1/-1->1->0 [23] 2/-1/-1->1->0\u001b[0m\n",
      "\u001b[34malgo-1:239:982 [1] NCCL INFO P2P Chunksize set to 524288\u001b[0m\n",
      "\u001b[34malgo-1:238:979 [0] NCCL INFO Channel 07/24 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34malgo-1:238:979 [0] NCCL INFO Channel 08/24 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34malgo-1:238:979 [0] NCCL INFO Channel 09/24 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34malgo-1:238:979 [0] NCCL INFO Channel 10/24 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34malgo-1:238:979 [0] NCCL INFO Channel 11/24 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34malgo-1:238:979 [0] NCCL INFO Channel 12/24 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34malgo-1:238:979 [0] NCCL INFO Channel 13/24 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34malgo-1:238:979 [0] NCCL INFO Channel 14/24 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34malgo-1:238:979 [0] NCCL INFO Channel 15/24 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34malgo-1:238:979 [0] NCCL INFO Channel 16/24 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34malgo-1:238:979 [0] NCCL INFO Channel 17/24 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34malgo-1:238:979 [0] NCCL INFO Channel 18/24 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34malgo-1:238:979 [0] NCCL INFO Channel 19/24 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34malgo-1:238:979 [0] NCCL INFO Channel 20/24 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34malgo-1:238:979 [0] NCCL INFO Channel 21/24 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34malgo-1:238:979 [0] NCCL INFO Channel 22/24 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34malgo-1:238:979 [0] NCCL INFO Channel 23/24 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34malgo-1:238:979 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1 [2] 1/-1/-1->0->-1 [3] 1/-1/-1->0->-1 [4] 1/-1/-1->0->-1 [5] 1/-1/-1->0->-1 [6] 1/-1/-1->0->-1 [7] 1/-1/-1->0->-1 [8] 1/-1/-1->0->-1 [9] 1/-1/-1->0->-1 [10] 1/-1/-1->0->-1 [11] 1/-1/-1->0->-1 [12] 1/-1/-1->0->-1 [13] 1/-1/-1->0->-1 [14] 1/-1/-1->0->-1 [15] 1/-1/-1->0->-1 [16] 1/-1/-1->0->-1 [17] 1/-1/-1->0->-1 [18] 1/-1/-1->0->-1 [19] 1/-1/-1->0->-1 [20] 1/-1/-1->0->-1 [21] 1/-1/-1->0->-1 [22] 1/-1/-1->0->-1 [23] 1/-1/-1->0->-1\u001b[0m\n",
      "\u001b[34malgo-1:238:979 [0] NCCL INFO P2P Chunksize set to 524288\u001b[0m\n",
      "\u001b[34malgo-1:241:980 [3] NCCL INFO Channel 00/0 : 3[3] -> 4[4] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:240:981 [2] NCCL INFO Channel 00/0 : 2[2] -> 3[3] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:244:985 [6] NCCL INFO Channel 00/0 : 6[6] -> 7[7] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:243:986 [5] NCCL INFO Channel 00/0 : 5[5] -> 6[6] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:242:983 [4] NCCL INFO Channel 00/0 : 4[4] -> 5[5] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:238:979 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[1] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:241:980 [3] NCCL INFO Channel 01/0 : 3[3] -> 4[4] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:240:981 [2] NCCL INFO Channel 01/0 : 2[2] -> 3[3] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:239:982 [1] NCCL INFO Channel 00/0 : 1[1] -> 2[2] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:244:985 [6] NCCL INFO Channel 01/0 : 6[6] -> 7[7] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:245:984 [7] NCCL INFO Channel 00/0 : 7[7] -> 0[0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:243:986 [5] NCCL INFO Channel 01/0 : 5[5] -> 6[6] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:242:983 [4] NCCL INFO Channel 01/0 : 4[4] -> 5[5] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:238:979 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[1] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:241:980 [3] NCCL INFO Channel 02/0 : 3[3] -> 4[4] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:240:981 [2] NCCL INFO Channel 02/0 : 2[2] -> 3[3] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:239:982 [1] NCCL INFO Channel 01/0 : 1[1] -> 2[2] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:244:985 [6] NCCL INFO Channel 02/0 : 6[6] -> 7[7] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:245:984 [7] NCCL INFO Channel 01/0 : 7[7] -> 0[0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:243:986 [5] NCCL INFO Channel 02/0 : 5[5] -> 6[6] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:242:983 [4] NCCL INFO Channel 02/0 : 4[4] -> 5[5] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:238:979 [0] NCCL INFO Channel 02/0 : 0[0] -> 1[1] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:241:980 [3] NCCL INFO Channel 03/0 : 3[3] -> 4[4] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:240:981 [2] NCCL INFO Channel 03/0 : 2[2] -> 3[3] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:239:982 [1] NCCL INFO Channel 02/0 : 1[1] -> 2[2] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:244:985 [6] NCCL INFO Channel 03/0 : 6[6] -> 7[7] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:245:984 [7] NCCL INFO Channel 02/0 : 7[7] -> 0[0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:243:986 [5] NCCL INFO Channel 03/0 : 5[5] -> 6[6] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:242:983 [4] NCCL INFO Channel 03/0 : 4[4] -> 5[5] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:238:979 [0] NCCL INFO Channel 03/0 : 0[0] -> 1[1] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:241:980 [3] NCCL INFO Channel 04/0 : 3[3] -> 4[4] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:240:981 [2] NCCL INFO Channel 04/0 : 2[2] -> 3[3] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:239:982 [1] NCCL INFO Channel 03/0 : 1[1] -> 2[2] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:244:985 [6] NCCL INFO Channel 04/0 : 6[6] -> 7[7] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:245:984 [7] NCCL INFO Channel 03/0 : 7[7] -> 0[0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:243:986 [5] NCCL INFO Channel 04/0 : 5[5] -> 6[6] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:242:983 [4] NCCL INFO Channel 04/0 : 4[4] -> 5[5] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:238:979 [0] NCCL INFO Channel 04/0 : 0[0] -> 1[1] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:241:980 [3] NCCL INFO Channel 05/0 : 3[3] -> 4[4] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:240:981 [2] NCCL INFO Channel 05/0 : 2[2] -> 3[3] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:239:982 [1] NCCL INFO Channel 04/0 : 1[1] -> 2[2] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:244:985 [6] NCCL INFO Channel 05/0 : 6[6] -> 7[7] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:245:984 [7] NCCL INFO Channel 04/0 : 7[7] -> 0[0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:243:986 [5] NCCL INFO Channel 05/0 : 5[5] -> 6[6] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:242:983 [4] NCCL INFO Channel 05/0 : 4[4] -> 5[5] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:241:980 [3] NCCL INFO Channel 06/0 : 3[3] -> 4[4] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:238:979 [0] NCCL INFO Channel 05/0 : 0[0] -> 1[1] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:240:981 [2] NCCL INFO Channel 06/0 : 2[2] -> 3[3] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:239:982 [1] NCCL INFO Channel 05/0 : 1[1] -> 2[2] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:244:985 [6] NCCL INFO Channel 06/0 : 6[6] -> 7[7] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:245:984 [7] NCCL INFO Channel 05/0 : 7[7] -> 0[0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:243:986 [5] NCCL INFO Channel 06/0 : 5[5] -> 6[6] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:242:983 [4] NCCL INFO Channel 06/0 : 4[4] -> 5[5] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:241:980 [3] NCCL INFO Channel 07/0 : 3[3] -> 4[4] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:238:979 [0] NCCL INFO Channel 06/0 : 0[0] -> 1[1] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:240:981 [2] NCCL INFO Channel 07/0 : 2[2] -> 3[3] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:239:982 [1] NCCL INFO Channel 06/0 : 1[1] -> 2[2] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:244:985 [6] NCCL INFO Channel 07/0 : 6[6] -> 7[7] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:245:984 [7] NCCL INFO Channel 06/0 : 7[7] -> 0[0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:243:986 [5] NCCL INFO Channel 07/0 : 5[5] -> 6[6] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:242:983 [4] NCCL INFO Channel 07/0 : 4[4] -> 5[5] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:241:980 [3] NCCL INFO Channel 08/0 : 3[3] -> 4[4] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:238:979 [0] NCCL INFO Channel 07/0 : 0[0] -> 1[1] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:240:981 [2] NCCL INFO Channel 08/0 : 2[2] -> 3[3] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:239:982 [1] NCCL INFO Channel 07/0 : 1[1] -> 2[2] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:244:985 [6] NCCL INFO Channel 08/0 : 6[6] -> 7[7] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:245:984 [7] NCCL INFO Channel 07/0 : 7[7] -> 0[0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:243:986 [5] NCCL INFO Channel 08/0 : 5[5] -> 6[6] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:242:983 [4] NCCL INFO Channel 08/0 : 4[4] -> 5[5] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:241:980 [3] NCCL INFO Channel 09/0 : 3[3] -> 4[4] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:238:979 [0] NCCL INFO Channel 08/0 : 0[0] -> 1[1] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:240:981 [2] NCCL INFO Channel 09/0 : 2[2] -> 3[3] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:239:982 [1] NCCL INFO Channel 08/0 : 1[1] -> 2[2] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:244:985 [6] NCCL INFO Channel 09/0 : 6[6] -> 7[7] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:245:984 [7] NCCL INFO Channel 08/0 : 7[7] -> 0[0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:243:986 [5] NCCL INFO Channel 09/0 : 5[5] -> 6[6] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:242:983 [4] NCCL INFO Channel 09/0 : 4[4] -> 5[5] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:241:980 [3] NCCL INFO Channel 10/0 : 3[3] -> 4[4] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:240:981 [2] NCCL INFO Channel 10/0 : 2[2] -> 3[3] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:238:979 [0] NCCL INFO Channel 09/0 : 0[0] -> 1[1] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:239:982 [1] NCCL INFO Channel 09/0 : 1[1] -> 2[2] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:244:985 [6] NCCL INFO Channel 10/0 : 6[6] -> 7[7] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:245:984 [7] NCCL INFO Channel 09/0 : 7[7] -> 0[0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:243:986 [5] NCCL INFO Channel 10/0 : 5[5] -> 6[6] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:242:983 [4] NCCL INFO Channel 10/0 : 4[4] -> 5[5] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:241:980 [3] NCCL INFO Channel 11/0 : 3[3] -> 4[4] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:238:979 [0] NCCL INFO Channel 10/0 : 0[0] -> 1[1] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:240:981 [2] NCCL INFO Channel 11/0 : 2[2] -> 3[3] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:239:982 [1] NCCL INFO Channel 10/0 : 1[1] -> 2[2] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:244:985 [6] NCCL INFO Channel 11/0 : 6[6] -> 7[7] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:245:984 [7] NCCL INFO Channel 10/0 : 7[7] -> 0[0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:243:986 [5] NCCL INFO Channel 11/0 : 5[5] -> 6[6] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:242:983 [4] NCCL INFO Channel 11/0 : 4[4] -> 5[5] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:241:980 [3] NCCL INFO Channel 12/0 : 3[3] -> 4[4] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:238:979 [0] NCCL INFO Channel 11/0 : 0[0] -> 1[1] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:240:981 [2] NCCL INFO Channel 12/0 : 2[2] -> 3[3] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:239:982 [1] NCCL INFO Channel 11/0 : 1[1] -> 2[2] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:244:985 [6] NCCL INFO Channel 12/0 : 6[6] -> 7[7] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:245:984 [7] NCCL INFO Channel 11/0 : 7[7] -> 0[0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:243:986 [5] NCCL INFO Channel 12/0 : 5[5] -> 6[6] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:242:983 [4] NCCL INFO Channel 12/0 : 4[4] -> 5[5] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:241:980 [3] NCCL INFO Channel 13/0 : 3[3] -> 4[4] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:238:979 [0] NCCL INFO Channel 12/0 : 0[0] -> 1[1] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:240:981 [2] NCCL INFO Channel 13/0 : 2[2] -> 3[3] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:239:982 [1] NCCL INFO Channel 12/0 : 1[1] -> 2[2] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:244:985 [6] NCCL INFO Channel 13/0 : 6[6] -> 7[7] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:245:984 [7] NCCL INFO Channel 12/0 : 7[7] -> 0[0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:243:986 [5] NCCL INFO Channel 13/0 : 5[5] -> 6[6] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:242:983 [4] NCCL INFO Channel 13/0 : 4[4] -> 5[5] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:241:980 [3] NCCL INFO Channel 14/0 : 3[3] -> 4[4] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:238:979 [0] NCCL INFO Channel 13/0 : 0[0] -> 1[1] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:240:981 [2] NCCL INFO Channel 14/0 : 2[2] -> 3[3] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:239:982 [1] NCCL INFO Channel 13/0 : 1[1] -> 2[2] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:244:985 [6] NCCL INFO Channel 14/0 : 6[6] -> 7[7] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:245:984 [7] NCCL INFO Channel 13/0 : 7[7] -> 0[0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:243:986 [5] NCCL INFO Channel 14/0 : 5[5] -> 6[6] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:242:983 [4] NCCL INFO Channel 14/0 : 4[4] -> 5[5] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:241:980 [3] NCCL INFO Channel 15/0 : 3[3] -> 4[4] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:238:979 [0] NCCL INFO Channel 14/0 : 0[0] -> 1[1] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:240:981 [2] NCCL INFO Channel 15/0 : 2[2] -> 3[3] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:239:982 [1] NCCL INFO Channel 14/0 : 1[1] -> 2[2] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:244:985 [6] NCCL INFO Channel 15/0 : 6[6] -> 7[7] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:245:984 [7] NCCL INFO Channel 14/0 : 7[7] -> 0[0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:243:986 [5] NCCL INFO Channel 15/0 : 5[5] -> 6[6] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:242:983 [4] NCCL INFO Channel 15/0 : 4[4] -> 5[5] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:241:980 [3] NCCL INFO Channel 16/0 : 3[3] -> 4[4] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:238:979 [0] NCCL INFO Channel 15/0 : 0[0] -> 1[1] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:240:981 [2] NCCL INFO Channel 16/0 : 2[2] -> 3[3] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:239:982 [1] NCCL INFO Channel 15/0 : 1[1] -> 2[2] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:244:985 [6] NCCL INFO Channel 16/0 : 6[6] -> 7[7] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:245:984 [7] NCCL INFO Channel 15/0 : 7[7] -> 0[0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:243:986 [5] NCCL INFO Channel 16/0 : 5[5] -> 6[6] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:242:983 [4] NCCL INFO Channel 16/0 : 4[4] -> 5[5] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:241:980 [3] NCCL INFO Channel 17/0 : 3[3] -> 4[4] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:238:979 [0] NCCL INFO Channel 16/0 : 0[0] -> 1[1] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:240:981 [2] NCCL INFO Channel 17/0 : 2[2] -> 3[3] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:239:982 [1] NCCL INFO Channel 16/0 : 1[1] -> 2[2] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:244:985 [6] NCCL INFO Channel 17/0 : 6[6] -> 7[7] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:245:984 [7] NCCL INFO Channel 16/0 : 7[7] -> 0[0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:243:986 [5] NCCL INFO Channel 17/0 : 5[5] -> 6[6] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:242:983 [4] NCCL INFO Channel 17/0 : 4[4] -> 5[5] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:238:979 [0] NCCL INFO Channel 17/0 : 0[0] -> 1[1] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:241:980 [3] NCCL INFO Channel 18/0 : 3[3] -> 4[4] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:240:981 [2] NCCL INFO Channel 18/0 : 2[2] -> 3[3] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:239:982 [1] NCCL INFO Channel 17/0 : 1[1] -> 2[2] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:244:985 [6] NCCL INFO Channel 18/0 : 6[6] -> 7[7] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:245:984 [7] NCCL INFO Channel 17/0 : 7[7] -> 0[0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:243:986 [5] NCCL INFO Channel 18/0 : 5[5] -> 6[6] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:242:983 [4] NCCL INFO Channel 18/0 : 4[4] -> 5[5] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:241:980 [3] NCCL INFO Channel 19/0 : 3[3] -> 4[4] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:238:979 [0] NCCL INFO Channel 18/0 : 0[0] -> 1[1] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:240:981 [2] NCCL INFO Channel 19/0 : 2[2] -> 3[3] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:239:982 [1] NCCL INFO Channel 18/0 : 1[1] -> 2[2] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:244:985 [6] NCCL INFO Channel 19/0 : 6[6] -> 7[7] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:245:984 [7] NCCL INFO Channel 18/0 : 7[7] -> 0[0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:243:986 [5] NCCL INFO Channel 19/0 : 5[5] -> 6[6] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:242:983 [4] NCCL INFO Channel 19/0 : 4[4] -> 5[5] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:241:980 [3] NCCL INFO Channel 20/0 : 3[3] -> 4[4] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:238:979 [0] NCCL INFO Channel 19/0 : 0[0] -> 1[1] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:240:981 [2] NCCL INFO Channel 20/0 : 2[2] -> 3[3] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:239:982 [1] NCCL INFO Channel 19/0 : 1[1] -> 2[2] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:244:985 [6] NCCL INFO Channel 20/0 : 6[6] -> 7[7] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:245:984 [7] NCCL INFO Channel 19/0 : 7[7] -> 0[0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:243:986 [5] NCCL INFO Channel 20/0 : 5[5] -> 6[6] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:242:983 [4] NCCL INFO Channel 20/0 : 4[4] -> 5[5] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:238:979 [0] NCCL INFO Channel 20/0 : 0[0] -> 1[1] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:241:980 [3] NCCL INFO Channel 21/0 : 3[3] -> 4[4] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:240:981 [2] NCCL INFO Channel 21/0 : 2[2] -> 3[3] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:239:982 [1] NCCL INFO Channel 20/0 : 1[1] -> 2[2] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:244:985 [6] NCCL INFO Channel 21/0 : 6[6] -> 7[7] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:245:984 [7] NCCL INFO Channel 20/0 : 7[7] -> 0[0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:243:986 [5] NCCL INFO Channel 21/0 : 5[5] -> 6[6] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:242:983 [4] NCCL INFO Channel 21/0 : 4[4] -> 5[5] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:238:979 [0] NCCL INFO Channel 21/0 : 0[0] -> 1[1] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:240:981 [2] NCCL INFO Channel 22/0 : 2[2] -> 3[3] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:241:980 [3] NCCL INFO Channel 22/0 : 3[3] -> 4[4] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:239:982 [1] NCCL INFO Channel 21/0 : 1[1] -> 2[2] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:244:985 [6] NCCL INFO Channel 22/0 : 6[6] -> 7[7] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:245:984 [7] NCCL INFO Channel 21/0 : 7[7] -> 0[0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:243:986 [5] NCCL INFO Channel 22/0 : 5[5] -> 6[6] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:242:983 [4] NCCL INFO Channel 22/0 : 4[4] -> 5[5] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:238:979 [0] NCCL INFO Channel 22/0 : 0[0] -> 1[1] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:240:981 [2] NCCL INFO Channel 23/0 : 2[2] -> 3[3] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:241:980 [3] NCCL INFO Channel 23/0 : 3[3] -> 4[4] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:239:982 [1] NCCL INFO Channel 22/0 : 1[1] -> 2[2] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:244:985 [6] NCCL INFO Channel 23/0 : 6[6] -> 7[7] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:245:984 [7] NCCL INFO Channel 22/0 : 7[7] -> 0[0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:243:986 [5] NCCL INFO Channel 23/0 : 5[5] -> 6[6] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:242:983 [4] NCCL INFO Channel 23/0 : 4[4] -> 5[5] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:238:979 [0] NCCL INFO Channel 23/0 : 0[0] -> 1[1] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:239:982 [1] NCCL INFO Channel 23/0 : 1[1] -> 2[2] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:245:984 [7] NCCL INFO Channel 23/0 : 7[7] -> 0[0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:241:980 [3] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34malgo-1:243:986 [5] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34malgo-1:242:983 [4] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34malgo-1:244:985 [6] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34malgo-1:240:981 [2] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34malgo-1:238:979 [0] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34malgo-1:245:984 [7] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34malgo-1:245:984 [7] NCCL INFO Channel 00/0 : 7[7] -> 6[6] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:239:982 [1] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34malgo-1:245:984 [7] NCCL INFO Channel 01/0 : 7[7] -> 6[6] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:245:984 [7] NCCL INFO Channel 02/0 : 7[7] -> 6[6] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:245:984 [7] NCCL INFO Channel 03/0 : 7[7] -> 6[6] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:245:984 [7] NCCL INFO Channel 04/0 : 7[7] -> 6[6] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:245:984 [7] NCCL INFO Channel 05/0 : 7[7] -> 6[6] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:245:984 [7] NCCL INFO Channel 06/0 : 7[7] -> 6[6] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:245:984 [7] NCCL INFO Channel 07/0 : 7[7] -> 6[6] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:245:984 [7] NCCL INFO Channel 08/0 : 7[7] -> 6[6] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:245:984 [7] NCCL INFO Channel 09/0 : 7[7] -> 6[6] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:245:984 [7] NCCL INFO Channel 10/0 : 7[7] -> 6[6] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:245:984 [7] NCCL INFO Channel 11/0 : 7[7] -> 6[6] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:245:984 [7] NCCL INFO Channel 12/0 : 7[7] -> 6[6] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:245:984 [7] NCCL INFO Channel 13/0 : 7[7] -> 6[6] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:245:984 [7] NCCL INFO Channel 14/0 : 7[7] -> 6[6] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:245:984 [7] NCCL INFO Channel 15/0 : 7[7] -> 6[6] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:245:984 [7] NCCL INFO Channel 16/0 : 7[7] -> 6[6] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:245:984 [7] NCCL INFO Channel 17/0 : 7[7] -> 6[6] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:245:984 [7] NCCL INFO Channel 18/0 : 7[7] -> 6[6] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:245:984 [7] NCCL INFO Channel 19/0 : 7[7] -> 6[6] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:245:984 [7] NCCL INFO Channel 20/0 : 7[7] -> 6[6] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:245:984 [7] NCCL INFO Channel 21/0 : 7[7] -> 6[6] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:245:984 [7] NCCL INFO Channel 22/0 : 7[7] -> 6[6] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:245:984 [7] NCCL INFO Channel 23/0 : 7[7] -> 6[6] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:241:980 [3] NCCL INFO Channel 00/0 : 3[3] -> 2[2] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:243:986 [5] NCCL INFO Channel 00/0 : 5[5] -> 4[4] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:240:981 [2] NCCL INFO Channel 00/0 : 2[2] -> 1[1] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:242:983 [4] NCCL INFO Channel 00/0 : 4[4] -> 3[3] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:239:982 [1] NCCL INFO Channel 00/0 : 1[1] -> 0[0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:244:985 [6] NCCL INFO Channel 00/0 : 6[6] -> 5[5] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:241:980 [3] NCCL INFO Channel 01/0 : 3[3] -> 2[2] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:243:986 [5] NCCL INFO Channel 01/0 : 5[5] -> 4[4] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:240:981 [2] NCCL INFO Channel 01/0 : 2[2] -> 1[1] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:242:983 [4] NCCL INFO Channel 01/0 : 4[4] -> 3[3] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:239:982 [1] NCCL INFO Channel 01/0 : 1[1] -> 0[0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:244:985 [6] NCCL INFO Channel 01/0 : 6[6] -> 5[5] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:241:980 [3] NCCL INFO Channel 02/0 : 3[3] -> 2[2] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:240:981 [2] NCCL INFO Channel 02/0 : 2[2] -> 1[1] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:243:986 [5] NCCL INFO Channel 02/0 : 5[5] -> 4[4] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:242:983 [4] NCCL INFO Channel 02/0 : 4[4] -> 3[3] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:239:982 [1] NCCL INFO Channel 02/0 : 1[1] -> 0[0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:244:985 [6] NCCL INFO Channel 02/0 : 6[6] -> 5[5] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:241:980 [3] NCCL INFO Channel 03/0 : 3[3] -> 2[2] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:240:981 [2] NCCL INFO Channel 03/0 : 2[2] -> 1[1] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:243:986 [5] NCCL INFO Channel 03/0 : 5[5] -> 4[4] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:242:983 [4] NCCL INFO Channel 03/0 : 4[4] -> 3[3] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:239:982 [1] NCCL INFO Channel 03/0 : 1[1] -> 0[0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:244:985 [6] NCCL INFO Channel 03/0 : 6[6] -> 5[5] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:241:980 [3] NCCL INFO Channel 04/0 : 3[3] -> 2[2] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:240:981 [2] NCCL INFO Channel 04/0 : 2[2] -> 1[1] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:243:986 [5] NCCL INFO Channel 04/0 : 5[5] -> 4[4] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:242:983 [4] NCCL INFO Channel 04/0 : 4[4] -> 3[3] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:239:982 [1] NCCL INFO Channel 04/0 : 1[1] -> 0[0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:244:985 [6] NCCL INFO Channel 04/0 : 6[6] -> 5[5] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:241:980 [3] NCCL INFO Channel 05/0 : 3[3] -> 2[2] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:240:981 [2] NCCL INFO Channel 05/0 : 2[2] -> 1[1] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:243:986 [5] NCCL INFO Channel 05/0 : 5[5] -> 4[4] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:242:983 [4] NCCL INFO Channel 05/0 : 4[4] -> 3[3] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:239:982 [1] NCCL INFO Channel 05/0 : 1[1] -> 0[0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:244:985 [6] NCCL INFO Channel 05/0 : 6[6] -> 5[5] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:241:980 [3] NCCL INFO Channel 06/0 : 3[3] -> 2[2] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:240:981 [2] NCCL INFO Channel 06/0 : 2[2] -> 1[1] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:243:986 [5] NCCL INFO Channel 06/0 : 5[5] -> 4[4] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:242:983 [4] NCCL INFO Channel 06/0 : 4[4] -> 3[3] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:239:982 [1] NCCL INFO Channel 06/0 : 1[1] -> 0[0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:244:985 [6] NCCL INFO Channel 06/0 : 6[6] -> 5[5] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:241:980 [3] NCCL INFO Channel 07/0 : 3[3] -> 2[2] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:240:981 [2] NCCL INFO Channel 07/0 : 2[2] -> 1[1] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:243:986 [5] NCCL INFO Channel 07/0 : 5[5] -> 4[4] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:242:983 [4] NCCL INFO Channel 07/0 : 4[4] -> 3[3] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:239:982 [1] NCCL INFO Channel 07/0 : 1[1] -> 0[0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:241:980 [3] NCCL INFO Channel 08/0 : 3[3] -> 2[2] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:244:985 [6] NCCL INFO Channel 07/0 : 6[6] -> 5[5] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:240:981 [2] NCCL INFO Channel 08/0 : 2[2] -> 1[1] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:243:986 [5] NCCL INFO Channel 08/0 : 5[5] -> 4[4] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:242:983 [4] NCCL INFO Channel 08/0 : 4[4] -> 3[3] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:239:982 [1] NCCL INFO Channel 08/0 : 1[1] -> 0[0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:241:980 [3] NCCL INFO Channel 09/0 : 3[3] -> 2[2] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:244:985 [6] NCCL INFO Channel 08/0 : 6[6] -> 5[5] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:240:981 [2] NCCL INFO Channel 09/0 : 2[2] -> 1[1] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:242:983 [4] NCCL INFO Channel 09/0 : 4[4] -> 3[3] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:243:986 [5] NCCL INFO Channel 09/0 : 5[5] -> 4[4] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:239:982 [1] NCCL INFO Channel 09/0 : 1[1] -> 0[0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:241:980 [3] NCCL INFO Channel 10/0 : 3[3] -> 2[2] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:244:985 [6] NCCL INFO Channel 09/0 : 6[6] -> 5[5] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:240:981 [2] NCCL INFO Channel 10/0 : 2[2] -> 1[1] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:242:983 [4] NCCL INFO Channel 10/0 : 4[4] -> 3[3] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:243:986 [5] NCCL INFO Channel 10/0 : 5[5] -> 4[4] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:239:982 [1] NCCL INFO Channel 10/0 : 1[1] -> 0[0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:241:980 [3] NCCL INFO Channel 11/0 : 3[3] -> 2[2] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:244:985 [6] NCCL INFO Channel 10/0 : 6[6] -> 5[5] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:240:981 [2] NCCL INFO Channel 11/0 : 2[2] -> 1[1] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:242:983 [4] NCCL INFO Channel 11/0 : 4[4] -> 3[3] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:243:986 [5] NCCL INFO Channel 11/0 : 5[5] -> 4[4] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:239:982 [1] NCCL INFO Channel 11/0 : 1[1] -> 0[0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:241:980 [3] NCCL INFO Channel 12/0 : 3[3] -> 2[2] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:244:985 [6] NCCL INFO Channel 11/0 : 6[6] -> 5[5] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:240:981 [2] NCCL INFO Channel 12/0 : 2[2] -> 1[1] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:242:983 [4] NCCL INFO Channel 12/0 : 4[4] -> 3[3] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:243:986 [5] NCCL INFO Channel 12/0 : 5[5] -> 4[4] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:239:982 [1] NCCL INFO Channel 12/0 : 1[1] -> 0[0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:241:980 [3] NCCL INFO Channel 13/0 : 3[3] -> 2[2] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:244:985 [6] NCCL INFO Channel 12/0 : 6[6] -> 5[5] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:240:981 [2] NCCL INFO Channel 13/0 : 2[2] -> 1[1] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:243:986 [5] NCCL INFO Channel 13/0 : 5[5] -> 4[4] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:242:983 [4] NCCL INFO Channel 13/0 : 4[4] -> 3[3] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:239:982 [1] NCCL INFO Channel 13/0 : 1[1] -> 0[0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:241:980 [3] NCCL INFO Channel 14/0 : 3[3] -> 2[2] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:244:985 [6] NCCL INFO Channel 13/0 : 6[6] -> 5[5] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:240:981 [2] NCCL INFO Channel 14/0 : 2[2] -> 1[1] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:243:986 [5] NCCL INFO Channel 14/0 : 5[5] -> 4[4] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:242:983 [4] NCCL INFO Channel 14/0 : 4[4] -> 3[3] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:239:982 [1] NCCL INFO Channel 14/0 : 1[1] -> 0[0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:241:980 [3] NCCL INFO Channel 15/0 : 3[3] -> 2[2] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:244:985 [6] NCCL INFO Channel 14/0 : 6[6] -> 5[5] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:240:981 [2] NCCL INFO Channel 15/0 : 2[2] -> 1[1] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:243:986 [5] NCCL INFO Channel 15/0 : 5[5] -> 4[4] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:242:983 [4] NCCL INFO Channel 15/0 : 4[4] -> 3[3] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:239:982 [1] NCCL INFO Channel 15/0 : 1[1] -> 0[0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:241:980 [3] NCCL INFO Channel 16/0 : 3[3] -> 2[2] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:244:985 [6] NCCL INFO Channel 15/0 : 6[6] -> 5[5] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:240:981 [2] NCCL INFO Channel 16/0 : 2[2] -> 1[1] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:243:986 [5] NCCL INFO Channel 16/0 : 5[5] -> 4[4] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:242:983 [4] NCCL INFO Channel 16/0 : 4[4] -> 3[3] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:239:982 [1] NCCL INFO Channel 16/0 : 1[1] -> 0[0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:241:980 [3] NCCL INFO Channel 17/0 : 3[3] -> 2[2] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:244:985 [6] NCCL INFO Channel 16/0 : 6[6] -> 5[5] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:240:981 [2] NCCL INFO Channel 17/0 : 2[2] -> 1[1] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:243:986 [5] NCCL INFO Channel 17/0 : 5[5] -> 4[4] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:242:983 [4] NCCL INFO Channel 17/0 : 4[4] -> 3[3] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:239:982 [1] NCCL INFO Channel 17/0 : 1[1] -> 0[0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:241:980 [3] NCCL INFO Channel 18/0 : 3[3] -> 2[2] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:244:985 [6] NCCL INFO Channel 17/0 : 6[6] -> 5[5] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:240:981 [2] NCCL INFO Channel 18/0 : 2[2] -> 1[1] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:243:986 [5] NCCL INFO Channel 18/0 : 5[5] -> 4[4] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:242:983 [4] NCCL INFO Channel 18/0 : 4[4] -> 3[3] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:239:982 [1] NCCL INFO Channel 18/0 : 1[1] -> 0[0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:241:980 [3] NCCL INFO Channel 19/0 : 3[3] -> 2[2] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:244:985 [6] NCCL INFO Channel 18/0 : 6[6] -> 5[5] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:240:981 [2] NCCL INFO Channel 19/0 : 2[2] -> 1[1] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:243:986 [5] NCCL INFO Channel 19/0 : 5[5] -> 4[4] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:242:983 [4] NCCL INFO Channel 19/0 : 4[4] -> 3[3] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:239:982 [1] NCCL INFO Channel 19/0 : 1[1] -> 0[0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:241:980 [3] NCCL INFO Channel 20/0 : 3[3] -> 2[2] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:244:985 [6] NCCL INFO Channel 19/0 : 6[6] -> 5[5] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:240:981 [2] NCCL INFO Channel 20/0 : 2[2] -> 1[1] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:243:986 [5] NCCL INFO Channel 20/0 : 5[5] -> 4[4] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:242:983 [4] NCCL INFO Channel 20/0 : 4[4] -> 3[3] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:239:982 [1] NCCL INFO Channel 20/0 : 1[1] -> 0[0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:241:980 [3] NCCL INFO Channel 21/0 : 3[3] -> 2[2] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:240:981 [2] NCCL INFO Channel 21/0 : 2[2] -> 1[1] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:244:985 [6] NCCL INFO Channel 20/0 : 6[6] -> 5[5] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:243:986 [5] NCCL INFO Channel 21/0 : 5[5] -> 4[4] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:242:983 [4] NCCL INFO Channel 21/0 : 4[4] -> 3[3] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:239:982 [1] NCCL INFO Channel 21/0 : 1[1] -> 0[0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:241:980 [3] NCCL INFO Channel 22/0 : 3[3] -> 2[2] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:240:981 [2] NCCL INFO Channel 22/0 : 2[2] -> 1[1] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:244:985 [6] NCCL INFO Channel 21/0 : 6[6] -> 5[5] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:243:986 [5] NCCL INFO Channel 22/0 : 5[5] -> 4[4] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:242:983 [4] NCCL INFO Channel 22/0 : 4[4] -> 3[3] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:239:982 [1] NCCL INFO Channel 22/0 : 1[1] -> 0[0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:241:980 [3] NCCL INFO Channel 23/0 : 3[3] -> 2[2] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:240:981 [2] NCCL INFO Channel 23/0 : 2[2] -> 1[1] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:244:985 [6] NCCL INFO Channel 22/0 : 6[6] -> 5[5] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:243:986 [5] NCCL INFO Channel 23/0 : 5[5] -> 4[4] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:242:983 [4] NCCL INFO Channel 23/0 : 4[4] -> 3[3] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:239:982 [1] NCCL INFO Channel 23/0 : 1[1] -> 0[0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:244:985 [6] NCCL INFO Channel 23/0 : 6[6] -> 5[5] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:238:979 [0] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34malgo-1:238:979 [0] NCCL INFO NCCL_PROTO set by environment to simple\u001b[0m\n",
      "\u001b[34malgo-1:238:979 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34malgo-1:238:979 [0] NCCL INFO 24 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer\u001b[0m\n",
      "\u001b[34malgo-1:245:984 [7] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34malgo-1:245:984 [7] NCCL INFO NCCL_PROTO set by environment to simple\u001b[0m\n",
      "\u001b[34malgo-1:245:984 [7] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34malgo-1:245:984 [7] NCCL INFO 24 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer\u001b[0m\n",
      "\u001b[34malgo-1:242:983 [4] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34malgo-1:242:983 [4] NCCL INFO NCCL_PROTO set by environment to simple\u001b[0m\n",
      "\u001b[34malgo-1:242:983 [4] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34malgo-1:242:983 [4] NCCL INFO 24 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer\u001b[0m\n",
      "\u001b[34malgo-1:239:982 [1] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34malgo-1:239:982 [1] NCCL INFO NCCL_PROTO set by environment to simple\u001b[0m\n",
      "\u001b[34malgo-1:239:982 [1] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34malgo-1:239:982 [1] NCCL INFO 24 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer\u001b[0m\n",
      "\u001b[34malgo-1:243:986 [5] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34malgo-1:243:986 [5] NCCL INFO NCCL_PROTO set by environment to simple\u001b[0m\n",
      "\u001b[34malgo-1:243:986 [5] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34malgo-1:243:986 [5] NCCL INFO 24 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer\u001b[0m\n",
      "\u001b[34malgo-1:244:985 [6] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34malgo-1:244:985 [6] NCCL INFO NCCL_PROTO set by environment to simple\u001b[0m\n",
      "\u001b[34malgo-1:244:985 [6] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34malgo-1:244:985 [6] NCCL INFO 24 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer\u001b[0m\n",
      "\u001b[34malgo-1:240:981 [2] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34malgo-1:240:981 [2] NCCL INFO NCCL_PROTO set by environment to simple\u001b[0m\n",
      "\u001b[34malgo-1:240:981 [2] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34malgo-1:240:981 [2] NCCL INFO 24 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer\u001b[0m\n",
      "\u001b[34malgo-1:241:980 [3] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34malgo-1:241:980 [3] NCCL INFO NCCL_PROTO set by environment to simple\u001b[0m\n",
      "\u001b[34malgo-1:241:980 [3] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34malgo-1:241:980 [3] NCCL INFO 24 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer\u001b[0m\n",
      "\u001b[34malgo-1:241:980 [3] NCCL INFO comm 0x7f094005cde0 rank 3 nranks 8 cudaDev 3 nvmlDev 3 busId 201d0 commId 0x16dd7c1664f67960 - Init COMPLETE\u001b[0m\n",
      "\u001b[34malgo-1:243:986 [5] NCCL INFO comm 0x7fc48405d250 rank 5 nranks 8 cudaDev 5 nvmlDev 5 busId 901d0 commId 0x16dd7c1664f67960 - Init COMPLETE\u001b[0m\n",
      "\u001b[34malgo-1:245:984 [7] NCCL INFO comm 0x7fe32005ce20 rank 7 nranks 8 cudaDev 7 nvmlDev 7 busId a01d0 commId 0x16dd7c1664f67960 - Init COMPLETE\u001b[0m\n",
      "\u001b[34malgo-1:239:982 [1] NCCL INFO comm 0x7f615c05ce60 rank 1 nranks 8 cudaDev 1 nvmlDev 1 busId 101d0 commId 0x16dd7c1664f67960 - Init COMPLETE\u001b[0m\n",
      "\u001b[34malgo-1:244:985 [6] NCCL INFO comm 0x7f54c405cf20 rank 6 nranks 8 cudaDev 6 nvmlDev 6 busId a01c0 commId 0x16dd7c1664f67960 - Init COMPLETE\u001b[0m\n",
      "\u001b[34malgo-1:240:981 [2] NCCL INFO comm 0x7f816405cdd0 rank 2 nranks 8 cudaDev 2 nvmlDev 2 busId 201c0 commId 0x16dd7c1664f67960 - Init COMPLETE\u001b[0m\n",
      "\u001b[34malgo-1:242:983 [4] NCCL INFO comm 0x7f647c05cfd0 rank 4 nranks 8 cudaDev 4 nvmlDev 4 busId 901c0 commId 0x16dd7c1664f67960 - Init COMPLETE\u001b[0m\n",
      "\u001b[34malgo-1:238:979 [0] NCCL INFO comm 0x7f0f1005d700 rank 0 nranks 8 cudaDev 0 nvmlDev 0 busId 101c0 commId 0x16dd7c1664f67960 - Init COMPLETE\u001b[0m\n",
      "\u001b[34m0%|          | 1/337 [00:06<34:59,  6.25s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 2/337 [00:09<24:23,  4.37s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 3/337 [00:12<20:57,  3.76s/it]\u001b[0m\n",
      "\u001b[34m1%|          | 4/337 [00:15<19:19,  3.48s/it]\u001b[0m\n",
      "\u001b[34m1%|▏         | 5/337 [00:18<18:23,  3.32s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 6/337 [00:21<17:48,  3.23s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 7/337 [00:24<17:24,  3.17s/it]\u001b[0m\n",
      "\u001b[34m2%|▏         | 8/337 [00:27<17:08,  3.13s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 9/337 [00:30<16:56,  3.10s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 10/337 [00:33<16:46,  3.08s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.795, 'grad_norm': 2.645256850425004, 'learning_rate': 2.9411764705882355e-06, 'epoch': 0.03}\u001b[0m\n",
      "\u001b[34m3%|▎         | 10/337 [00:33<16:46,  3.08s/it]\u001b[0m\n",
      "\u001b[34m3%|▎         | 11/337 [00:36<16:38,  3.06s/it]\u001b[0m\n",
      "\u001b[34m4%|▎         | 12/337 [00:39<16:32,  3.06s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 13/337 [00:42<16:28,  3.05s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 14/337 [00:45<16:25,  3.05s/it]\u001b[0m\n",
      "\u001b[34m4%|▍         | 15/337 [00:48<16:23,  3.06s/it]\u001b[0m\n",
      "\u001b[34m5%|▍         | 16/337 [00:51<16:21,  3.06s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 17/337 [00:54<16:19,  3.06s/it]\u001b[0m\n",
      "\u001b[34m5%|▌         | 18/337 [00:58<16:16,  3.06s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 19/337 [01:01<16:14,  3.06s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 20/337 [01:04<16:11,  3.06s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.7267, 'grad_norm': 1.5605299598856563, 'learning_rate': 5.882352941176471e-06, 'epoch': 0.06}\u001b[0m\n",
      "\u001b[34m6%|▌         | 20/337 [01:04<16:11,  3.06s/it]\u001b[0m\n",
      "\u001b[34m6%|▌         | 21/337 [01:07<16:08,  3.06s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 22/337 [01:10<16:05,  3.07s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 23/337 [01:13<16:02,  3.06s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 24/337 [01:16<15:59,  3.07s/it]\u001b[0m\n",
      "\u001b[34m7%|▋         | 25/337 [01:19<15:56,  3.07s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 26/337 [01:22<15:53,  3.07s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 27/337 [01:25<15:49,  3.06s/it]\u001b[0m\n",
      "\u001b[34m8%|▊         | 28/337 [01:28<15:45,  3.06s/it]\u001b[0m\n",
      "\u001b[34m9%|▊         | 29/337 [01:31<15:42,  3.06s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 30/337 [01:34<15:37,  3.05s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.6221, 'grad_norm': 1.0585548509268055, 'learning_rate': 8.823529411764707e-06, 'epoch': 0.09}\u001b[0m\n",
      "\u001b[34m9%|▉         | 30/337 [01:34<15:37,  3.05s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 31/337 [01:37<15:33,  3.05s/it]\u001b[0m\n",
      "\u001b[34m9%|▉         | 32/337 [01:40<15:30,  3.05s/it]\u001b[0m\n",
      "\u001b[34m10%|▉         | 33/337 [01:43<15:26,  3.05s/it]\u001b[0m\n",
      "\u001b[34m10%|█         | 34/337 [01:46<15:19,  3.04s/it]\u001b[0m\n",
      "\u001b[34m10%|█         | 35/337 [01:49<15:10,  3.02s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 36/337 [01:52<15:03,  3.00s/it]\u001b[0m\n",
      "\u001b[34m11%|█         | 37/337 [01:55<14:57,  2.99s/it]\u001b[0m\n",
      "\u001b[34m11%|█▏        | 38/337 [01:58<14:52,  2.98s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 39/337 [02:01<14:47,  2.98s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 40/337 [02:04<14:43,  2.97s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.5362, 'grad_norm': 1.1381315210613931, 'learning_rate': 9.990327985667972e-06, 'epoch': 0.12}\u001b[0m\n",
      "\u001b[34m12%|█▏        | 40/337 [02:04<14:43,  2.97s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 41/337 [02:07<14:39,  2.97s/it]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 42/337 [02:10<14:35,  2.97s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 43/337 [02:13<14:32,  2.97s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 44/337 [02:16<14:28,  2.96s/it]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 45/337 [02:19<14:34,  2.99s/it]\u001b[0m\n",
      "\u001b[34m14%|█▎        | 46/337 [02:22<14:29,  2.99s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 47/337 [02:25<14:25,  2.98s/it]\u001b[0m\n",
      "\u001b[34m14%|█▍        | 48/337 [02:28<14:21,  2.98s/it]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 49/337 [02:31<14:17,  2.98s/it]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 50/337 [02:34<14:17,  2.99s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.482, 'grad_norm': 1.0394911555772637, 'learning_rate': 9.93135668670091e-06, 'epoch': 0.15}\u001b[0m\n",
      "\u001b[34m15%|█▍        | 50/337 [02:34<14:17,  2.99s/it]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 51/337 [02:37<14:20,  3.01s/it]\u001b[0m\n",
      "\u001b[34m15%|█▌        | 52/337 [02:40<14:21,  3.02s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 53/337 [02:43<14:20,  3.03s/it]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 54/337 [02:46<14:19,  3.04s/it]\u001b[0m\n",
      "\u001b[34m16%|█▋        | 55/337 [02:49<14:17,  3.04s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 56/337 [02:52<14:15,  3.05s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 57/337 [02:55<14:13,  3.05s/it]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 58/337 [02:58<14:11,  3.05s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 59/337 [03:02<14:08,  3.05s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 60/337 [03:05<14:05,  3.05s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.49, 'grad_norm': 1.0558708483894208, 'learning_rate': 9.819420042307091e-06, 'epoch': 0.18}\u001b[0m\n",
      "\u001b[34m18%|█▊        | 60/337 [03:05<14:05,  3.05s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 61/337 [03:08<14:03,  3.06s/it]\u001b[0m\n",
      "\u001b[34m18%|█▊        | 62/337 [03:11<14:00,  3.06s/it]\u001b[0m\n",
      "\u001b[34m19%|█▊        | 63/337 [03:14<13:57,  3.06s/it]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 64/337 [03:17<13:54,  3.06s/it]\u001b[0m\n",
      "\u001b[34m19%|█▉        | 65/337 [03:20<13:51,  3.06s/it]\u001b[0m\n",
      "\u001b[34m20%|█▉        | 66/337 [03:23<13:48,  3.06s/it]\u001b[0m\n",
      "\u001b[34m20%|█▉        | 67/337 [03:26<13:45,  3.06s/it]\u001b[0m\n",
      "\u001b[34m20%|██        | 68/337 [03:29<13:42,  3.06s/it]\u001b[0m\n",
      "\u001b[34m20%|██        | 69/337 [03:32<13:39,  3.06s/it]\u001b[0m\n",
      "\u001b[34m21%|██        | 70/337 [03:35<13:37,  3.06s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.4895, 'grad_norm': 0.9213737945435816, 'learning_rate': 9.655720310488298e-06, 'epoch': 0.21}\u001b[0m\n",
      "\u001b[34m21%|██        | 70/337 [03:35<13:37,  3.06s/it]\u001b[0m\n",
      "\u001b[34m21%|██        | 71/337 [03:38<13:33,  3.06s/it]\u001b[0m\n",
      "\u001b[34m21%|██▏       | 72/337 [03:41<13:30,  3.06s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 73/337 [03:44<13:27,  3.06s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 74/337 [03:47<13:24,  3.06s/it]\u001b[0m\n",
      "\u001b[34m22%|██▏       | 75/337 [03:50<13:21,  3.06s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 76/337 [03:54<13:17,  3.06s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 77/337 [03:57<13:14,  3.06s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 78/337 [04:00<13:13,  3.06s/it]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 79/337 [04:03<13:02,  3.03s/it]\u001b[0m\n",
      "\u001b[34m24%|██▎       | 80/337 [04:06<12:53,  3.01s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.4616, 'grad_norm': 0.8768328570129899, 'learning_rate': 9.442015711830246e-06, 'epoch': 0.24}\u001b[0m\n",
      "\u001b[34m24%|██▎       | 80/337 [04:06<12:53,  3.01s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 81/337 [04:09<12:46,  3.00s/it]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 82/337 [04:11<12:40,  2.98s/it]\u001b[0m\n",
      "\u001b[34m25%|██▍       | 83/337 [04:14<12:36,  2.98s/it]\u001b[0m\n",
      "\u001b[34m25%|██▍       | 84/337 [04:17<12:31,  2.97s/it]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 85/337 [04:20<12:27,  2.97s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 86/337 [04:23<12:24,  2.97s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 87/337 [04:26<12:20,  2.96s/it]\u001b[0m\n",
      "\u001b[34m26%|██▌       | 88/337 [04:29<12:17,  2.96s/it]\u001b[0m\n",
      "\u001b[34m26%|██▋       | 89/337 [04:32<12:15,  2.97s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 90/337 [04:35<12:14,  2.97s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.4497, 'grad_norm': 0.9415649381144444, 'learning_rate': 9.180601545295535e-06, 'epoch': 0.27}\u001b[0m\n",
      "\u001b[34m27%|██▋       | 90/337 [04:35<12:14,  2.97s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 91/337 [04:38<12:09,  2.97s/it]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 92/337 [04:41<12:05,  2.96s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 93/337 [04:44<12:01,  2.96s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 94/337 [04:47<11:57,  2.95s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 95/337 [04:50<11:53,  2.95s/it]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 96/337 [04:53<11:50,  2.95s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 97/337 [04:56<11:48,  2.95s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 98/337 [04:59<11:47,  2.96s/it]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 99/337 [05:02<11:45,  2.97s/it]\u001b[0m\n",
      "\u001b[34m30%|██▉       | 100/337 [05:05<11:43,  2.97s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.4716, 'grad_norm': 0.9625851299578964, 'learning_rate': 8.87428553551445e-06, 'epoch': 0.3}\u001b[0m\n",
      "\u001b[34m30%|██▉       | 100/337 [05:05<11:43,  2.97s/it]\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:4258] 2025-03-18 10:46:42,478 >> \u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:4258] 2025-03-18 10:46:42,478 >> \u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:4260] 2025-03-18 10:46:42,478 >>   Num examples = 1200\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:4260] 2025-03-18 10:46:42,478 >>   Num examples = 1200\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:4263] 2025-03-18 10:46:42,478 >>   Batch size = 2\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:4263] 2025-03-18 10:46:42,478 >>   Batch size = 2\u001b[0m\n",
      "\u001b[34m0%|          | 0/75 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m3%|▎         | 2/75 [00:00<00:07,  9.48it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m4%|▍         | 3/75 [00:00<00:11,  6.16it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m5%|▌         | 4/75 [00:00<00:13,  5.27it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m7%|▋         | 5/75 [00:00<00:14,  4.89it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m8%|▊         | 6/75 [00:01<00:14,  4.68it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m9%|▉         | 7/75 [00:01<00:14,  4.56it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m11%|█         | 8/75 [00:01<00:15,  4.44it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m12%|█▏        | 9/75 [00:01<00:15,  4.40it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m13%|█▎        | 10/75 [00:02<00:14,  4.36it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m15%|█▍        | 11/75 [00:02<00:14,  4.35it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m16%|█▌        | 12/75 [00:02<00:14,  4.31it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m17%|█▋        | 13/75 [00:02<00:14,  4.31it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m19%|█▊        | 14/75 [00:03<00:14,  4.31it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m20%|██        | 15/75 [00:03<00:13,  4.32it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m21%|██▏       | 16/75 [00:03<00:13,  4.29it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m23%|██▎       | 17/75 [00:03<00:13,  4.30it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m24%|██▍       | 18/75 [00:03<00:13,  4.30it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m25%|██▌       | 19/75 [00:04<00:12,  4.31it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m27%|██▋       | 20/75 [00:04<00:12,  4.32it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m28%|██▊       | 21/75 [00:04<00:12,  4.29it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m29%|██▉       | 22/75 [00:04<00:12,  4.30it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m31%|███       | 23/75 [00:05<00:12,  4.31it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m32%|███▏      | 24/75 [00:05<00:11,  4.31it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m33%|███▎      | 25/75 [00:05<00:11,  4.29it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m35%|███▍      | 26/75 [00:05<00:11,  4.29it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m36%|███▌      | 27/75 [00:06<00:11,  4.30it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m37%|███▋      | 28/75 [00:06<00:10,  4.31it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m39%|███▊      | 29/75 [00:06<00:10,  4.29it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m40%|████      | 30/75 [00:06<00:10,  4.30it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m41%|████▏     | 31/75 [00:06<00:10,  4.29it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m43%|████▎     | 32/75 [00:07<00:10,  4.30it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m44%|████▍     | 33/75 [00:07<00:09,  4.30it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m45%|████▌     | 34/75 [00:07<00:09,  4.28it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m47%|████▋     | 35/75 [00:07<00:09,  4.29it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m48%|████▊     | 36/75 [00:08<00:09,  4.30it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m49%|████▉     | 37/75 [00:08<00:08,  4.31it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m51%|█████     | 38/75 [00:08<00:08,  4.28it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 39/75 [00:08<00:08,  4.28it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 40/75 [00:09<00:08,  4.29it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 41/75 [00:09<00:07,  4.30it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 42/75 [00:09<00:07,  4.28it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 43/75 [00:09<00:07,  4.29it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m59%|█████▊    | 44/75 [00:10<00:07,  4.30it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m60%|██████    | 45/75 [00:10<00:06,  4.30it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m61%|██████▏   | 46/75 [00:10<00:06,  4.31it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 47/75 [00:10<00:06,  4.30it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 48/75 [00:10<00:06,  4.30it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m65%|██████▌   | 49/75 [00:11<00:06,  4.31it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 50/75 [00:11<00:05,  4.32it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 51/75 [00:11<00:05,  4.29it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 52/75 [00:11<00:05,  4.29it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m71%|███████   | 53/75 [00:12<00:05,  4.30it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 54/75 [00:12<00:04,  4.31it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 55/75 [00:12<00:04,  4.29it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m75%|███████▍  | 56/75 [00:12<00:04,  4.30it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 57/75 [00:13<00:04,  4.31it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 58/75 [00:13<00:03,  4.32it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m79%|███████▊  | 59/75 [00:13<00:03,  4.31it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m80%|████████  | 60/75 [00:13<00:03,  4.31it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m81%|████████▏ | 61/75 [00:13<00:03,  4.32it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 62/75 [00:14<00:03,  4.32it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 63/75 [00:14<00:02,  4.33it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 64/75 [00:14<00:02,  4.29it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 65/75 [00:14<00:02,  4.30it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 66/75 [00:15<00:02,  4.31it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 67/75 [00:15<00:01,  4.32it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m91%|█████████ | 68/75 [00:15<00:01,  4.29it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 69/75 [00:15<00:01,  4.30it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 70/75 [00:16<00:01,  4.31it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m95%|█████████▍| 71/75 [00:16<00:00,  4.32it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 72/75 [00:16<00:00,  4.31it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 73/75 [00:16<00:00,  4.32it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m99%|█████████▊| 74/75 [00:16<00:00,  4.30it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m100%|██████████| 75/75 [00:17<00:00,  4.30it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.4581771790981293, 'eval_runtime': 17.4213, 'eval_samples_per_second': 68.881, 'eval_steps_per_second': 4.305, 'epoch': 0.3}\u001b[0m\n",
      "\u001b[34m30%|██▉       | 100/337 [05:22<11:43,  2.97s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 75/75 [00:17<00:00,  4.30it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3942] 2025-03-18 10:47:09,406 >> Saving model checkpoint to /tmp/finetuned_model/checkpoint-100\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3942] 2025-03-18 10:47:09,406 >> Saving model checkpoint to /tmp/finetuned_model/checkpoint-100\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:423] 2025-03-18 10:47:09,408 >> Configuration saved in /tmp/finetuned_model/checkpoint-100/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:423] 2025-03-18 10:47:09,408 >> Configuration saved in /tmp/finetuned_model/checkpoint-100/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:909] 2025-03-18 10:47:09,409 >> Configuration saved in /tmp/finetuned_model/checkpoint-100/generation_config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:909] 2025-03-18 10:47:09,409 >> Configuration saved in /tmp/finetuned_model/checkpoint-100/generation_config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:3048] 2025-03-18 10:47:25,916 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 3 checkpoint shards. You can find where each parameters has been saved in the index located at /tmp/finetuned_model/checkpoint-100/model.safetensors.index.json.\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:3048] 2025-03-18 10:47:25,916 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 3 checkpoint shards. You can find where each parameters has been saved in the index located at /tmp/finetuned_model/checkpoint-100/model.safetensors.index.json.\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2500] 2025-03-18 10:47:25,917 >> tokenizer config file saved in /tmp/finetuned_model/checkpoint-100/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2500] 2025-03-18 10:47:25,917 >> tokenizer config file saved in /tmp/finetuned_model/checkpoint-100/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2509] 2025-03-18 10:47:25,917 >> Special tokens file saved in /tmp/finetuned_model/checkpoint-100/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2509] 2025-03-18 10:47:25,917 >> Special tokens file saved in /tmp/finetuned_model/checkpoint-100/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:47:25,947] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step100 is about to be saved!\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:47:25,955] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /tmp/finetuned_model/checkpoint-100/global_step100/zero_pp_rank_0_mp_rank_00_model_states.pt\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:47:25,955] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /tmp/finetuned_model/checkpoint-100/global_step100/zero_pp_rank_0_mp_rank_00_model_states.pt...\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:47:25,966] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /tmp/finetuned_model/checkpoint-100/global_step100/zero_pp_rank_0_mp_rank_00_model_states.pt.\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:47:25,972] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /tmp/finetuned_model/checkpoint-100/global_step100/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:47:48,398] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /tmp/finetuned_model/checkpoint-100/global_step100/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:47:48,399] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /tmp/finetuned_model/checkpoint-100/global_step100/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:47:49,400] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step100 is ready now!\u001b[0m\n",
      "\u001b[34m30%|██▉       | 101/337 [06:15<1:30:46, 23.08s/it]\u001b[0m\n",
      "\u001b[34m30%|███       | 102/337 [06:18<1:06:51, 17.07s/it]\u001b[0m\n",
      "\u001b[34m31%|███       | 103/337 [06:21<50:09, 12.86s/it]\u001b[0m\n",
      "\u001b[34m31%|███       | 104/337 [06:24<38:30,  9.92s/it]\u001b[0m\n",
      "\u001b[34m31%|███       | 105/337 [06:27<30:30,  7.89s/it]\u001b[0m\n",
      "\u001b[34m31%|███▏      | 106/337 [06:30<24:49,  6.45s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 107/337 [06:33<20:50,  5.44s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 108/337 [06:36<18:02,  4.73s/it]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 109/337 [06:39<16:03,  4.22s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 110/337 [06:42<14:38,  3.87s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.449, 'grad_norm': 2.384828303910772, 'learning_rate': 8.526357676356538e-06, 'epoch': 0.33}\u001b[0m\n",
      "\u001b[34m33%|███▎      | 110/337 [06:42<14:38,  3.87s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 111/337 [06:45<13:34,  3.60s/it]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 112/337 [06:48<12:48,  3.42s/it]\u001b[0m\n",
      "\u001b[34m34%|███▎      | 113/337 [06:51<12:16,  3.29s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 114/337 [06:54<11:52,  3.20s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 115/337 [06:57<11:35,  3.13s/it]\u001b[0m\n",
      "\u001b[34m34%|███▍      | 116/337 [07:00<11:22,  3.09s/it]\u001b[0m\n",
      "\u001b[34m35%|███▍      | 117/337 [07:03<11:11,  3.05s/it]\u001b[0m\n",
      "\u001b[34m35%|███▌      | 118/337 [07:06<11:03,  3.03s/it]\u001b[0m\n",
      "\u001b[34m35%|███▌      | 119/337 [07:09<10:57,  3.02s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 120/337 [07:12<10:51,  3.00s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.4561, 'grad_norm': 0.7821960753160659, 'learning_rate': 8.14055489467878e-06, 'epoch': 0.36}\u001b[0m\n",
      "\u001b[34m36%|███▌      | 120/337 [07:12<10:51,  3.00s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 121/337 [07:15<10:47,  3.00s/it]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 122/337 [07:18<10:43,  2.99s/it]\u001b[0m\n",
      "\u001b[34m36%|███▋      | 123/337 [07:21<10:39,  2.99s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 124/337 [07:24<10:35,  2.98s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 125/337 [07:27<10:32,  2.98s/it]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 126/337 [07:30<10:28,  2.98s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 127/337 [07:33<10:25,  2.98s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 128/337 [07:36<10:32,  3.03s/it]\u001b[0m\n",
      "\u001b[34m38%|███▊      | 129/337 [07:39<10:40,  3.08s/it]\u001b[0m\n",
      "\u001b[34m39%|███▊      | 130/337 [07:43<10:44,  3.12s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.4594, 'grad_norm': 0.9558491044519761, 'learning_rate': 7.721020913780137e-06, 'epoch': 0.39}\u001b[0m\n",
      "\u001b[34m39%|███▊      | 130/337 [07:43<10:44,  3.12s/it]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 131/337 [07:46<10:31,  3.07s/it]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 132/337 [07:48<10:21,  3.03s/it]\u001b[0m\n",
      "\u001b[34m39%|███▉      | 133/337 [07:51<10:12,  3.00s/it]\u001b[0m\n",
      "\u001b[34m40%|███▉      | 134/337 [07:54<10:06,  2.99s/it]\u001b[0m\n",
      "\u001b[34m40%|████      | 135/337 [07:57<10:01,  2.98s/it]\u001b[0m\n",
      "\u001b[34m40%|████      | 136/337 [08:00<09:56,  2.97s/it]\u001b[0m\n",
      "\u001b[34m41%|████      | 137/337 [08:03<09:53,  2.97s/it]\u001b[0m\n",
      "\u001b[34m41%|████      | 138/337 [08:06<09:50,  2.97s/it]\u001b[0m\n",
      "\u001b[34m41%|████      | 139/337 [08:09<09:48,  2.97s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 140/337 [08:12<09:45,  2.97s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.4322, 'grad_norm': 0.9225643107634512, 'learning_rate': 7.272261747649922e-06, 'epoch': 0.41}\u001b[0m\n",
      "\u001b[34m42%|████▏     | 140/337 [08:12<09:45,  2.97s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 141/337 [08:15<09:42,  2.97s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 142/337 [08:18<09:39,  2.97s/it]\u001b[0m\n",
      "\u001b[34m42%|████▏     | 143/337 [08:21<09:36,  2.97s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 144/337 [08:24<09:34,  2.98s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 145/337 [08:27<09:31,  2.98s/it]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 146/337 [08:30<09:29,  2.98s/it]\u001b[0m\n",
      "\u001b[34m44%|████▎     | 147/337 [08:33<09:26,  2.98s/it]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 148/337 [08:36<09:23,  2.98s/it]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 149/337 [08:39<09:23,  2.99s/it]\u001b[0m\n",
      "\u001b[34m45%|████▍     | 150/337 [08:42<09:19,  2.99s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.4495, 'grad_norm': 0.9222935629529914, 'learning_rate': 6.7990973040250055e-06, 'epoch': 0.44}\u001b[0m\n",
      "\u001b[34m45%|████▍     | 150/337 [08:42<09:19,  2.99s/it]\u001b[0m\n",
      "\u001b[34m45%|████▍     | 151/337 [08:45<09:15,  2.99s/it]\u001b[0m\n",
      "\u001b[34m45%|████▌     | 152/337 [08:48<09:11,  2.98s/it]\u001b[0m\n",
      "\u001b[34m45%|████▌     | 153/337 [08:51<09:07,  2.98s/it]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 154/337 [08:54<09:04,  2.98s/it]\u001b[0m\n",
      "\u001b[34m46%|████▌     | 155/337 [08:57<09:01,  2.97s/it]\u001b[0m\n",
      "\u001b[34m46%|████▋     | 156/337 [09:00<08:57,  2.97s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 157/337 [09:03<08:54,  2.97s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 158/337 [09:06<08:50,  2.96s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 159/337 [09:09<08:47,  2.96s/it]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 160/337 [09:12<08:44,  2.96s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.4619, 'grad_norm': 0.8890284069591667, 'learning_rate': 6.306609616064304e-06, 'epoch': 0.47}\u001b[0m\n",
      "\u001b[34m47%|████▋     | 160/337 [09:12<08:44,  2.96s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 161/337 [09:15<08:41,  2.96s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 162/337 [09:18<08:38,  2.96s/it]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 163/337 [09:21<08:35,  2.96s/it]\u001b[0m\n",
      "\u001b[34m49%|████▊     | 164/337 [09:24<08:32,  2.96s/it]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 165/337 [09:26<08:29,  2.96s/it]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 166/337 [09:29<08:27,  2.97s/it]\u001b[0m\n",
      "\u001b[34m50%|████▉     | 167/337 [09:32<08:24,  2.97s/it]\u001b[0m\n",
      "\u001b[34m50%|████▉     | 168/337 [09:35<08:21,  2.97s/it]\u001b[0m\n",
      "\u001b[34m50%|█████     | 169/337 [09:38<08:18,  2.97s/it]\u001b[0m\n",
      "\u001b[34m50%|█████     | 170/337 [09:41<08:16,  2.97s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.4374, 'grad_norm': 0.9373751207190233, 'learning_rate': 5.800088258659371e-06, 'epoch': 0.5}\u001b[0m\n",
      "\u001b[34m50%|█████     | 170/337 [09:41<08:16,  2.97s/it]\u001b[0m\n",
      "\u001b[34m51%|█████     | 171/337 [09:44<08:13,  2.98s/it]\u001b[0m\n",
      "\u001b[34m51%|█████     | 172/337 [09:47<08:11,  2.98s/it]\u001b[0m\n",
      "\u001b[34m51%|█████▏    | 173/337 [09:50<08:07,  2.98s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 174/337 [09:53<08:04,  2.97s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 175/337 [09:56<08:01,  2.97s/it]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 176/337 [09:59<07:58,  2.97s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 177/337 [10:02<07:55,  2.97s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 178/337 [10:05<07:53,  2.98s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 179/337 [10:08<07:49,  2.97s/it]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 180/337 [10:11<07:46,  2.97s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.4167, 'grad_norm': 0.9368674933403283, 'learning_rate': 5.284973535638424e-06, 'epoch': 0.53}\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 180/337 [10:11<07:46,  2.97s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▎    | 181/337 [10:14<07:43,  2.97s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 182/337 [10:17<07:39,  2.97s/it]\u001b[0m\n",
      "\u001b[34m54%|█████▍    | 183/337 [10:20<07:37,  2.97s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 184/337 [10:23<07:36,  2.99s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 185/337 [10:26<07:44,  3.05s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 186/337 [10:29<07:48,  3.10s/it]\u001b[0m\n",
      "\u001b[34m55%|█████▌    | 187/337 [10:33<07:50,  3.13s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 188/337 [10:36<07:50,  3.16s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 189/337 [10:39<07:49,  3.17s/it]\u001b[0m\n",
      "\u001b[34m56%|█████▋    | 190/337 [10:42<07:48,  3.19s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.4775, 'grad_norm': 1.1522958547954634, 'learning_rate': 4.766798048062913e-06, 'epoch': 0.56}\u001b[0m\n",
      "\u001b[34m56%|█████▋    | 190/337 [10:42<07:48,  3.19s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 191/337 [10:45<07:46,  3.20s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 192/337 [10:49<07:44,  3.20s/it]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 193/337 [10:52<07:41,  3.20s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 194/337 [10:55<07:38,  3.21s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 195/337 [10:58<07:35,  3.21s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 196/337 [11:02<07:32,  3.21s/it]\u001b[0m\n",
      "\u001b[34m58%|█████▊    | 197/337 [11:05<07:29,  3.21s/it]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 198/337 [11:08<07:26,  3.21s/it]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 199/337 [11:11<07:22,  3.21s/it]\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 200/337 [11:14<07:19,  3.21s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.4418, 'grad_norm': 0.832787551199728, 'learning_rate': 4.251127271203593e-06, 'epoch': 0.59}\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 200/337 [11:14<07:19,  3.21s/it]\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:4258] 2025-03-18 10:52:52,096 >> \u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:4258] 2025-03-18 10:52:52,096 >> \u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:4260] 2025-03-18 10:52:52,096 >>   Num examples = 1200\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:4260] 2025-03-18 10:52:52,096 >>   Num examples = 1200\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:4263] 2025-03-18 10:52:52,096 >>   Batch size = 2\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:4263] 2025-03-18 10:52:52,096 >>   Batch size = 2\u001b[0m\n",
      "\u001b[34m0%|          | 0/75 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m3%|▎         | 2/75 [00:00<00:07,  9.46it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m4%|▍         | 3/75 [00:00<00:10,  6.68it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m5%|▌         | 4/75 [00:00<00:12,  5.79it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m7%|▋         | 5/75 [00:00<00:13,  5.38it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m8%|▊         | 6/75 [00:01<00:13,  5.10it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m9%|▉         | 7/75 [00:01<00:13,  4.94it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m11%|█         | 8/75 [00:01<00:13,  4.87it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m12%|█▏        | 9/75 [00:01<00:13,  4.83it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m13%|█▎        | 10/75 [00:01<00:13,  4.80it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m15%|█▍        | 11/75 [00:02<00:13,  4.74it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m16%|█▌        | 12/75 [00:02<00:13,  4.71it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m17%|█▋        | 13/75 [00:02<00:13,  4.71it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m19%|█▊        | 14/75 [00:02<00:12,  4.72it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m20%|██        | 15/75 [00:02<00:12,  4.68it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m21%|██▏       | 16/75 [00:03<00:12,  4.66it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m23%|██▎       | 17/75 [00:03<00:12,  4.68it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m24%|██▍       | 18/75 [00:03<00:12,  4.70it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m25%|██▌       | 19/75 [00:03<00:11,  4.70it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m27%|██▋       | 20/75 [00:04<00:11,  4.67it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m28%|██▊       | 21/75 [00:04<00:11,  4.66it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m29%|██▉       | 22/75 [00:04<00:11,  4.68it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m31%|███       | 23/75 [00:04<00:11,  4.69it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m32%|███▏      | 24/75 [00:04<00:10,  4.70it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m33%|███▎      | 25/75 [00:05<00:10,  4.68it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m35%|███▍      | 26/75 [00:05<00:10,  4.66it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m36%|███▌      | 27/75 [00:05<00:10,  4.68it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m37%|███▋      | 28/75 [00:05<00:10,  4.70it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m39%|███▊      | 29/75 [00:05<00:09,  4.69it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m40%|████      | 30/75 [00:06<00:09,  4.66it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m41%|████▏     | 31/75 [00:06<00:09,  4.68it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m43%|████▎     | 32/75 [00:06<00:09,  4.70it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m44%|████▍     | 33/75 [00:06<00:08,  4.71it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m45%|████▌     | 34/75 [00:07<00:08,  4.67it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m47%|████▋     | 35/75 [00:07<00:08,  4.65it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m48%|████▊     | 36/75 [00:07<00:08,  4.67it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m49%|████▉     | 37/75 [00:07<00:08,  4.69it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m51%|█████     | 38/75 [00:07<00:07,  4.70it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 39/75 [00:08<00:07,  4.68it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 40/75 [00:08<00:07,  4.66it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 41/75 [00:08<00:07,  4.68it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 42/75 [00:08<00:07,  4.70it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 43/75 [00:08<00:06,  4.70it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m59%|█████▊    | 44/75 [00:09<00:06,  4.68it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m60%|██████    | 45/75 [00:09<00:06,  4.65it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m61%|██████▏   | 46/75 [00:09<00:06,  4.65it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 47/75 [00:09<00:06,  4.66it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 48/75 [00:10<00:05,  4.64it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m65%|██████▌   | 49/75 [00:10<00:05,  4.62it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 50/75 [00:10<00:05,  4.65it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 51/75 [00:10<00:05,  4.68it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 52/75 [00:10<00:04,  4.70it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m71%|███████   | 53/75 [00:11<00:04,  4.64it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 54/75 [00:11<00:04,  4.64it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 55/75 [00:11<00:04,  4.66it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m75%|███████▍  | 56/75 [00:11<00:04,  4.67it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 57/75 [00:11<00:03,  4.66it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 58/75 [00:12<00:03,  4.64it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m79%|███████▊  | 59/75 [00:12<00:03,  4.64it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m80%|████████  | 60/75 [00:12<00:03,  4.66it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m81%|████████▏ | 61/75 [00:12<00:02,  4.68it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 62/75 [00:13<00:02,  4.65it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 63/75 [00:13<00:02,  4.63it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 64/75 [00:13<00:02,  4.66it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 65/75 [00:13<00:02,  4.67it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 66/75 [00:13<00:01,  4.69it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 67/75 [00:14<00:01,  4.66it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m91%|█████████ | 68/75 [00:14<00:01,  4.65it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 69/75 [00:14<00:01,  4.67it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 70/75 [00:14<00:01,  4.68it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m95%|█████████▍| 71/75 [00:14<00:00,  4.69it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 72/75 [00:15<00:00,  4.67it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 73/75 [00:15<00:00,  4.65it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m99%|█████████▊| 74/75 [00:15<00:00,  4.67it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m100%|██████████| 75/75 [00:15<00:00,  4.69it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.4341658055782318, 'eval_runtime': 16.0478, 'eval_samples_per_second': 74.776, 'eval_steps_per_second': 4.674, 'epoch': 0.59}\u001b[0m\n",
      "\u001b[34m59%|█████▉    | 200/337 [11:30<07:19,  3.21s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 75/75 [00:15<00:00,  4.69it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3942] 2025-03-18 10:53:17,513 >> Saving model checkpoint to /tmp/finetuned_model/checkpoint-200\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3942] 2025-03-18 10:53:17,513 >> Saving model checkpoint to /tmp/finetuned_model/checkpoint-200\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:423] 2025-03-18 10:53:17,515 >> Configuration saved in /tmp/finetuned_model/checkpoint-200/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:423] 2025-03-18 10:53:17,515 >> Configuration saved in /tmp/finetuned_model/checkpoint-200/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:909] 2025-03-18 10:53:17,516 >> Configuration saved in /tmp/finetuned_model/checkpoint-200/generation_config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:909] 2025-03-18 10:53:17,516 >> Configuration saved in /tmp/finetuned_model/checkpoint-200/generation_config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:3048] 2025-03-18 10:53:33,819 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 3 checkpoint shards. You can find where each parameters has been saved in the index located at /tmp/finetuned_model/checkpoint-200/model.safetensors.index.json.\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:3048] 2025-03-18 10:53:33,819 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 3 checkpoint shards. You can find where each parameters has been saved in the index located at /tmp/finetuned_model/checkpoint-200/model.safetensors.index.json.\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2500] 2025-03-18 10:53:33,820 >> tokenizer config file saved in /tmp/finetuned_model/checkpoint-200/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2500] 2025-03-18 10:53:33,820 >> tokenizer config file saved in /tmp/finetuned_model/checkpoint-200/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2509] 2025-03-18 10:53:33,820 >> Special tokens file saved in /tmp/finetuned_model/checkpoint-200/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2509] 2025-03-18 10:53:33,820 >> Special tokens file saved in /tmp/finetuned_model/checkpoint-200/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:53:33,849] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step200 is about to be saved!\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:53:33,856] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /tmp/finetuned_model/checkpoint-200/global_step200/zero_pp_rank_0_mp_rank_00_model_states.pt\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:53:33,856] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /tmp/finetuned_model/checkpoint-200/global_step200/zero_pp_rank_0_mp_rank_00_model_states.pt...\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:53:33,867] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /tmp/finetuned_model/checkpoint-200/global_step200/zero_pp_rank_0_mp_rank_00_model_states.pt.\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:53:33,870] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /tmp/finetuned_model/checkpoint-200/global_step200/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:53:56,557] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /tmp/finetuned_model/checkpoint-200/global_step200/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:53:56,559] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /tmp/finetuned_model/checkpoint-200/global_step200/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:53:57,940] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step200 is ready now!\u001b[0m\n",
      "\u001b[34m60%|█████▉    | 201/337 [12:23<52:03, 22.97s/it]\u001b[0m\n",
      "\u001b[34m60%|█████▉    | 202/337 [12:27<38:20, 17.04s/it]\u001b[0m\n",
      "\u001b[34m60%|██████    | 203/337 [12:30<28:48, 12.90s/it]\u001b[0m\n",
      "\u001b[34m61%|██████    | 204/337 [12:33<22:09, 10.00s/it]\u001b[0m\n",
      "\u001b[34m61%|██████    | 205/337 [12:36<17:34,  7.99s/it]\u001b[0m\n",
      "\u001b[34m61%|██████    | 206/337 [12:40<14:24,  6.60s/it]\u001b[0m\n",
      "\u001b[34m61%|██████▏   | 207/337 [12:43<12:08,  5.60s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 208/337 [12:46<10:30,  4.88s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 209/337 [12:49<09:20,  4.38s/it]\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 210/337 [12:53<08:31,  4.03s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.4192, 'grad_norm': 2.8603179405683594, 'learning_rate': 3.743499778430445e-06, 'epoch': 0.62}\u001b[0m\n",
      "\u001b[34m62%|██████▏   | 210/337 [12:53<08:31,  4.03s/it]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 211/337 [12:56<07:56,  3.78s/it]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 212/337 [12:59<07:30,  3.61s/it]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 213/337 [13:02<07:12,  3.49s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▎   | 214/337 [13:06<06:58,  3.40s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 215/337 [13:09<06:47,  3.34s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 216/337 [13:12<06:39,  3.30s/it]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 217/337 [13:15<06:32,  3.27s/it]\u001b[0m\n",
      "\u001b[34m65%|██████▍   | 218/337 [13:18<06:26,  3.25s/it]\u001b[0m\n",
      "\u001b[34m65%|██████▍   | 219/337 [13:21<06:21,  3.23s/it]\u001b[0m\n",
      "\u001b[34m65%|██████▌   | 220/337 [13:25<06:16,  3.22s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.4435, 'grad_norm': 0.9002048506633323, 'learning_rate': 3.249367754043047e-06, 'epoch': 0.65}\u001b[0m\n",
      "\u001b[34m65%|██████▌   | 220/337 [13:25<06:16,  3.22s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 221/337 [13:28<06:12,  3.21s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 222/337 [13:31<06:09,  3.21s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▌   | 223/337 [13:34<06:05,  3.21s/it]\u001b[0m\n",
      "\u001b[34m66%|██████▋   | 224/337 [13:38<06:02,  3.21s/it]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 225/337 [13:41<05:59,  3.21s/it]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 226/337 [13:44<05:55,  3.21s/it]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 227/337 [13:47<05:52,  3.21s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 228/337 [13:50<05:49,  3.21s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 229/337 [13:54<05:46,  3.21s/it]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 230/337 [13:57<05:42,  3.20s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.4388, 'grad_norm': 1.1600823419714827, 'learning_rate': 2.7740384339646655e-06, 'epoch': 0.68}\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 230/337 [13:57<05:42,  3.20s/it]\u001b[0m\n",
      "\u001b[34m69%|██████▊   | 231/337 [14:00<05:39,  3.21s/it]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 232/337 [14:03<05:36,  3.21s/it]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 233/337 [14:06<05:33,  3.21s/it]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 234/337 [14:10<05:30,  3.21s/it]\u001b[0m\n",
      "\u001b[34m70%|██████▉   | 235/337 [14:13<05:27,  3.21s/it]\u001b[0m\n",
      "\u001b[34m70%|███████   | 236/337 [14:16<05:24,  3.21s/it]\u001b[0m\n",
      "\u001b[34m70%|███████   | 237/337 [14:19<05:20,  3.21s/it]\u001b[0m\n",
      "\u001b[34m71%|███████   | 238/337 [14:22<05:17,  3.21s/it]\u001b[0m\n",
      "\u001b[34m71%|███████   | 239/337 [14:26<05:14,  3.21s/it]\u001b[0m\n",
      "\u001b[34m71%|███████   | 240/337 [14:29<05:11,  3.21s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.4257, 'grad_norm': 0.8229341672497159, 'learning_rate': 2.3226171032575856e-06, 'epoch': 0.71}\u001b[0m\n",
      "\u001b[34m71%|███████   | 240/337 [14:29<05:11,  3.21s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 241/337 [14:32<05:07,  3.21s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 242/337 [14:35<05:04,  3.21s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 243/337 [14:38<05:01,  3.21s/it]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 244/337 [14:42<04:58,  3.21s/it]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 245/337 [14:45<04:55,  3.21s/it]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 246/337 [14:48<04:52,  3.21s/it]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 247/337 [14:51<04:48,  3.21s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▎  | 248/337 [14:54<04:41,  3.16s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 249/337 [14:57<04:33,  3.10s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 250/337 [15:00<04:26,  3.06s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.4172, 'grad_norm': 0.7654056860825884, 'learning_rate': 1.8999522626961254e-06, 'epoch': 0.74}\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 250/337 [15:00<04:26,  3.06s/it]\u001b[0m\n",
      "\u001b[34m74%|███████▍  | 251/337 [15:03<04:21,  3.04s/it]\u001b[0m\n",
      "\u001b[34m75%|███████▍  | 252/337 [15:06<04:16,  3.02s/it]\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 253/337 [15:09<04:12,  3.01s/it]\u001b[0m\n",
      "\u001b[34m75%|███████▌  | 254/337 [15:12<04:08,  2.99s/it]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 255/337 [15:15<04:04,  2.98s/it]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 256/337 [15:18<04:00,  2.97s/it]\u001b[0m\n",
      "\u001b[34m76%|███████▋  | 257/337 [15:21<03:57,  2.97s/it]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 258/337 [15:24<03:54,  2.97s/it]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 259/337 [15:27<03:51,  2.97s/it]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 260/337 [15:30<03:48,  2.96s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.4174, 'grad_norm': 0.8244397131799326, 'learning_rate': 1.510583553336964e-06, 'epoch': 0.77}\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 260/337 [15:30<03:48,  2.96s/it]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 261/337 [15:33<03:45,  2.96s/it]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 262/337 [15:36<03:42,  2.96s/it]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 263/337 [15:39<03:39,  2.97s/it]\u001b[0m\n",
      "\u001b[34m78%|███████▊  | 264/337 [15:42<03:36,  2.97s/it]\u001b[0m\n",
      "\u001b[34m79%|███████▊  | 265/337 [15:45<03:33,  2.97s/it]\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 266/337 [15:48<03:30,  2.97s/it]\u001b[0m\n",
      "\u001b[34m79%|███████▉  | 267/337 [15:51<03:27,  2.97s/it]\u001b[0m\n",
      "\u001b[34m80%|███████▉  | 268/337 [15:54<03:24,  2.97s/it]\u001b[0m\n",
      "\u001b[34m80%|███████▉  | 269/337 [15:57<03:22,  2.97s/it]\u001b[0m\n",
      "\u001b[34m80%|████████  | 270/337 [16:00<03:19,  2.97s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.4483, 'grad_norm': 0.7991611560553277, 'learning_rate': 1.1586929984040974e-06, 'epoch': 0.8}\u001b[0m\n",
      "\u001b[34m80%|████████  | 270/337 [16:00<03:19,  2.97s/it]\u001b[0m\n",
      "\u001b[34m80%|████████  | 271/337 [16:03<03:16,  2.97s/it]\u001b[0m\n",
      "\u001b[34m81%|████████  | 272/337 [16:06<03:13,  2.97s/it]\u001b[0m\n",
      "\u001b[34m81%|████████  | 273/337 [16:09<03:10,  2.97s/it]\u001b[0m\n",
      "\u001b[34m81%|████████▏ | 274/337 [16:12<03:07,  2.97s/it]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 275/337 [16:14<03:04,  2.97s/it]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 276/337 [16:17<03:01,  2.97s/it]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 277/337 [16:20<02:58,  2.97s/it]\u001b[0m\n",
      "\u001b[34m82%|████████▏ | 278/337 [16:23<02:55,  2.97s/it]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 279/337 [16:26<02:52,  2.97s/it]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 280/337 [16:29<02:49,  2.97s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.3886, 'grad_norm': 0.9419408064980781, 'learning_rate': 8.480600861760124e-07, 'epoch': 0.83}\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 280/337 [16:29<02:49,  2.97s/it]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 281/337 [16:32<02:46,  2.97s/it]\u001b[0m\n",
      "\u001b[34m84%|████████▎ | 282/337 [16:35<02:43,  2.97s/it]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 283/337 [16:38<02:40,  2.97s/it]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 284/337 [16:41<02:37,  2.97s/it]\u001b[0m\n",
      "\u001b[34m85%|████████▍ | 285/337 [16:44<02:34,  2.97s/it]\u001b[0m\n",
      "\u001b[34m85%|████████▍ | 286/337 [16:47<02:31,  2.97s/it]\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 287/337 [16:50<02:28,  2.97s/it]\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 288/337 [16:53<02:25,  2.97s/it]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 289/337 [16:56<02:22,  2.97s/it]\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 290/337 [16:59<02:19,  2.97s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.4628, 'grad_norm': 0.9606830031020892, 'learning_rate': 5.820211763083494e-07, 'epoch': 0.86}\u001b[0m\n",
      "\u001b[34m86%|████████▌ | 290/337 [16:59<02:19,  2.97s/it]\u001b[0m\n",
      "\u001b[34m86%|████████▋ | 291/337 [17:02<02:16,  2.97s/it]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 292/337 [17:05<02:13,  2.97s/it]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 293/337 [17:08<02:10,  2.97s/it]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 294/337 [17:11<02:07,  2.96s/it]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 295/337 [17:14<02:04,  2.95s/it]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 296/337 [17:17<02:01,  2.95s/it]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 297/337 [17:20<01:58,  2.95s/it]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 298/337 [17:23<01:55,  2.95s/it]\u001b[0m\n",
      "\u001b[34m89%|████████▊ | 299/337 [17:26<01:52,  2.95s/it]\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 300/337 [17:29<01:49,  2.95s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.4459, 'grad_norm': 0.8693512556396076, 'learning_rate': 3.634336655893189e-07, 'epoch': 0.89}\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 300/337 [17:29<01:49,  2.95s/it]\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:4258] 2025-03-18 10:59:06,264 >> \u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:4258] 2025-03-18 10:59:06,264 >> \u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:4260] 2025-03-18 10:59:06,264 >>   Num examples = 1200\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:4260] 2025-03-18 10:59:06,264 >>   Num examples = 1200\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:4263] 2025-03-18 10:59:06,264 >>   Batch size = 2\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:4263] 2025-03-18 10:59:06,264 >>   Batch size = 2\u001b[0m\n",
      "\u001b[34m0%|          | 0/75 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m3%|▎         | 2/75 [00:00<00:07,  9.59it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m4%|▍         | 3/75 [00:00<00:10,  6.72it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m5%|▌         | 4/75 [00:00<00:12,  5.80it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m7%|▋         | 5/75 [00:00<00:13,  5.32it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m8%|▊         | 6/75 [00:01<00:13,  5.11it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m9%|▉         | 7/75 [00:01<00:13,  4.99it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m11%|█         | 8/75 [00:01<00:13,  4.91it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m12%|█▏        | 9/75 [00:01<00:13,  4.82it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m13%|█▎        | 10/75 [00:01<00:13,  4.76it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m15%|█▍        | 11/75 [00:02<00:13,  4.75it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m16%|█▌        | 12/75 [00:02<00:13,  4.75it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m17%|█▋        | 13/75 [00:02<00:13,  4.75it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m19%|█▊        | 14/75 [00:02<00:12,  4.70it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m20%|██        | 15/75 [00:02<00:12,  4.70it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m21%|██▏       | 16/75 [00:03<00:12,  4.71it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m23%|██▎       | 17/75 [00:03<00:12,  4.68it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m24%|██▍       | 18/75 [00:03<00:12,  4.69it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m25%|██▌       | 19/75 [00:03<00:12,  4.67it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m27%|██▋       | 20/75 [00:04<00:11,  4.69it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m28%|██▊       | 21/75 [00:04<00:11,  4.70it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m29%|██▉       | 22/75 [00:04<00:11,  4.72it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m31%|███       | 23/75 [00:04<00:11,  4.69it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m32%|███▏      | 24/75 [00:04<00:10,  4.67it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m33%|███▎      | 25/75 [00:05<00:10,  4.69it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m35%|███▍      | 26/75 [00:05<00:10,  4.70it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m36%|███▌      | 27/75 [00:05<00:10,  4.72it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m37%|███▋      | 28/75 [00:05<00:10,  4.68it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m39%|███▊      | 29/75 [00:05<00:09,  4.67it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m40%|████      | 30/75 [00:06<00:09,  4.69it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m41%|████▏     | 31/75 [00:06<00:09,  4.71it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m43%|████▎     | 32/75 [00:06<00:09,  4.72it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m44%|████▍     | 33/75 [00:06<00:08,  4.68it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m45%|████▌     | 34/75 [00:07<00:08,  4.70it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m47%|████▋     | 35/75 [00:07<00:08,  4.71it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m48%|████▊     | 36/75 [00:07<00:08,  4.72it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m49%|████▉     | 37/75 [00:07<00:08,  4.72it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m51%|█████     | 38/75 [00:07<00:07,  4.69it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 39/75 [00:08<00:07,  4.70it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 40/75 [00:08<00:07,  4.69it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 41/75 [00:08<00:07,  4.71it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 42/75 [00:08<00:07,  4.68it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 43/75 [00:08<00:06,  4.66it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m59%|█████▊    | 44/75 [00:09<00:06,  4.69it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m60%|██████    | 45/75 [00:09<00:06,  4.71it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m61%|██████▏   | 46/75 [00:09<00:06,  4.72it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 47/75 [00:09<00:05,  4.68it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 48/75 [00:10<00:05,  4.69it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m65%|██████▌   | 49/75 [00:10<00:05,  4.71it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 50/75 [00:10<00:05,  4.72it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 51/75 [00:10<00:05,  4.73it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 52/75 [00:10<00:04,  4.69it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m71%|███████   | 53/75 [00:11<00:04,  4.71it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 54/75 [00:11<00:04,  4.72it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 55/75 [00:11<00:04,  4.73it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m75%|███████▍  | 56/75 [00:11<00:04,  4.72it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 57/75 [00:11<00:03,  4.70it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 58/75 [00:12<00:03,  4.71it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m79%|███████▊  | 59/75 [00:12<00:03,  4.72it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m80%|████████  | 60/75 [00:12<00:03,  4.73it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m81%|████████▏ | 61/75 [00:12<00:02,  4.69it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 62/75 [00:12<00:02,  4.65it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 63/75 [00:13<00:02,  4.66it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 64/75 [00:13<00:02,  4.67it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 65/75 [00:13<00:02,  4.69it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 66/75 [00:13<00:01,  4.66it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 67/75 [00:14<00:01,  4.66it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m91%|█████████ | 68/75 [00:14<00:01,  4.68it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 69/75 [00:14<00:01,  4.70it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 70/75 [00:14<00:01,  4.72it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m95%|█████████▍| 71/75 [00:14<00:00,  4.69it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 72/75 [00:15<00:00,  4.70it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 73/75 [00:15<00:00,  4.71it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m99%|█████████▊| 74/75 [00:15<00:00,  4.72it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m100%|██████████| 75/75 [00:15<00:00,  4.70it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m{'eval_loss': 0.4267697036266327, 'eval_runtime': 15.9658, 'eval_samples_per_second': 75.161, 'eval_steps_per_second': 4.698, 'epoch': 0.89}\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 300/337 [17:45<01:49,  2.95s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 75/75 [00:15<00:00,  4.70it/s]\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m#033[A\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3942] 2025-03-18 10:59:31,746 >> Saving model checkpoint to /tmp/finetuned_model/checkpoint-300\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3942] 2025-03-18 10:59:31,746 >> Saving model checkpoint to /tmp/finetuned_model/checkpoint-300\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:423] 2025-03-18 10:59:31,749 >> Configuration saved in /tmp/finetuned_model/checkpoint-300/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:423] 2025-03-18 10:59:31,749 >> Configuration saved in /tmp/finetuned_model/checkpoint-300/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:909] 2025-03-18 10:59:31,749 >> Configuration saved in /tmp/finetuned_model/checkpoint-300/generation_config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:909] 2025-03-18 10:59:31,749 >> Configuration saved in /tmp/finetuned_model/checkpoint-300/generation_config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:3048] 2025-03-18 10:59:48,263 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 3 checkpoint shards. You can find where each parameters has been saved in the index located at /tmp/finetuned_model/checkpoint-300/model.safetensors.index.json.\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:3048] 2025-03-18 10:59:48,263 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 3 checkpoint shards. You can find where each parameters has been saved in the index located at /tmp/finetuned_model/checkpoint-300/model.safetensors.index.json.\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2500] 2025-03-18 10:59:48,264 >> tokenizer config file saved in /tmp/finetuned_model/checkpoint-300/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2500] 2025-03-18 10:59:48,264 >> tokenizer config file saved in /tmp/finetuned_model/checkpoint-300/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2509] 2025-03-18 10:59:48,264 >> Special tokens file saved in /tmp/finetuned_model/checkpoint-300/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2509] 2025-03-18 10:59:48,264 >> Special tokens file saved in /tmp/finetuned_model/checkpoint-300/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:59:48,727] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step300 is about to be saved!\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:59:48,734] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /tmp/finetuned_model/checkpoint-300/global_step300/zero_pp_rank_0_mp_rank_00_model_states.pt\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:59:48,734] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /tmp/finetuned_model/checkpoint-300/global_step300/zero_pp_rank_0_mp_rank_00_model_states.pt...\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:59:48,746] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /tmp/finetuned_model/checkpoint-300/global_step300/zero_pp_rank_0_mp_rank_00_model_states.pt.\u001b[0m\n",
      "\u001b[34m[2025-03-18 10:59:48,749] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /tmp/finetuned_model/checkpoint-300/global_step300/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...\u001b[0m\n",
      "\u001b[34m[2025-03-18 11:00:12,063] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /tmp/finetuned_model/checkpoint-300/global_step300/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.\u001b[0m\n",
      "\u001b[34m[2025-03-18 11:00:12,064] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /tmp/finetuned_model/checkpoint-300/global_step300/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt\u001b[0m\n",
      "\u001b[34m[2025-03-18 11:00:12,601] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step300 is ready now!\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 301/337 [18:38<13:42, 22.85s/it]\u001b[0m\n",
      "\u001b[34m90%|████████▉ | 302/337 [18:41<09:50, 16.88s/it]\u001b[0m\n",
      "\u001b[34m90%|████████▉ | 303/337 [18:44<07:12, 12.73s/it]\u001b[0m\n",
      "\u001b[34m90%|█████████ | 304/337 [18:47<05:23,  9.80s/it]\u001b[0m\n",
      "\u001b[34m91%|█████████ | 305/337 [18:50<04:08,  7.77s/it]\u001b[0m\n",
      "\u001b[34m91%|█████████ | 306/337 [18:53<03:17,  6.38s/it]\u001b[0m\n",
      "\u001b[34m91%|█████████ | 307/337 [18:56<02:43,  5.43s/it]\u001b[0m\n",
      "\u001b[34m91%|█████████▏| 308/337 [18:59<02:16,  4.69s/it]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 309/337 [19:02<01:56,  4.17s/it]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 310/337 [19:05<01:42,  3.80s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.4419, 'grad_norm': 1.0346462775170635, 'learning_rate': 1.9464529800637731e-07, 'epoch': 0.92}\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 310/337 [19:05<01:42,  3.80s/it]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 311/337 [19:08<01:32,  3.55s/it]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 312/337 [19:11<01:24,  3.36s/it]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 313/337 [19:14<01:17,  3.24s/it]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 314/337 [19:17<01:12,  3.15s/it]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 315/337 [19:20<01:07,  3.09s/it]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 316/337 [19:23<01:03,  3.05s/it]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 317/337 [19:26<01:00,  3.02s/it]\u001b[0m\n",
      "\u001b[34m94%|█████████▍| 318/337 [19:29<00:56,  3.00s/it]\u001b[0m\n",
      "\u001b[34m95%|█████████▍| 319/337 [19:32<00:53,  2.98s/it]\u001b[0m\n",
      "\u001b[34m95%|█████████▍| 320/337 [19:35<00:50,  2.97s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.4361, 'grad_norm': 1.0182268444876206, 'learning_rate': 7.746894875007016e-08, 'epoch': 0.95}\u001b[0m\n",
      "\u001b[34m95%|█████████▍| 320/337 [19:35<00:50,  2.97s/it]\u001b[0m\n",
      "\u001b[34m95%|█████████▌| 321/337 [19:37<00:47,  2.97s/it]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 322/337 [19:40<00:44,  2.96s/it]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 323/337 [19:43<00:41,  2.96s/it]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 324/337 [19:46<00:38,  2.95s/it]\u001b[0m\n",
      "\u001b[34m96%|█████████▋| 325/337 [19:49<00:35,  2.95s/it]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 326/337 [19:52<00:32,  2.95s/it]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 327/337 [19:55<00:29,  2.94s/it]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 328/337 [19:58<00:26,  2.94s/it]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 329/337 [20:01<00:23,  2.94s/it]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 330/337 [20:04<00:20,  2.94s/it]\u001b[0m\n",
      "\u001b[34m{'loss': 0.4261, 'grad_norm': 0.9077870505687403, 'learning_rate': 1.3163152988000527e-08, 'epoch': 0.98}\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 330/337 [20:04<00:20,  2.94s/it]\u001b[0m\n",
      "\u001b[34m98%|█████████▊| 331/337 [20:07<00:17,  2.93s/it]\u001b[0m\n",
      "\u001b[34m99%|█████████▊| 332/337 [20:10<00:14,  2.93s/it]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 333/337 [20:13<00:11,  2.93s/it]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 334/337 [20:16<00:08,  2.93s/it]\u001b[0m\n",
      "\u001b[34m99%|█████████▉| 335/337 [20:19<00:05,  2.94s/it]\u001b[0m\n",
      "\u001b[34m100%|█████████▉| 336/337 [20:22<00:02,  2.94s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 337/337 [20:25<00:00,  2.94s/it]\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3942] 2025-03-18 11:02:11,614 >> Saving model checkpoint to /tmp/finetuned_model/checkpoint-337\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3942] 2025-03-18 11:02:11,614 >> Saving model checkpoint to /tmp/finetuned_model/checkpoint-337\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:423] 2025-03-18 11:02:11,617 >> Configuration saved in /tmp/finetuned_model/checkpoint-337/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:423] 2025-03-18 11:02:11,617 >> Configuration saved in /tmp/finetuned_model/checkpoint-337/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:909] 2025-03-18 11:02:11,618 >> Configuration saved in /tmp/finetuned_model/checkpoint-337/generation_config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:909] 2025-03-18 11:02:11,618 >> Configuration saved in /tmp/finetuned_model/checkpoint-337/generation_config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:3048] 2025-03-18 11:02:27,999 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 3 checkpoint shards. You can find where each parameters has been saved in the index located at /tmp/finetuned_model/checkpoint-337/model.safetensors.index.json.\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:3048] 2025-03-18 11:02:27,999 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 3 checkpoint shards. You can find where each parameters has been saved in the index located at /tmp/finetuned_model/checkpoint-337/model.safetensors.index.json.\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2500] 2025-03-18 11:02:28,000 >> tokenizer config file saved in /tmp/finetuned_model/checkpoint-337/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2500] 2025-03-18 11:02:28,000 >> tokenizer config file saved in /tmp/finetuned_model/checkpoint-337/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2509] 2025-03-18 11:02:28,001 >> Special tokens file saved in /tmp/finetuned_model/checkpoint-337/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2509] 2025-03-18 11:02:28,001 >> Special tokens file saved in /tmp/finetuned_model/checkpoint-337/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[2025-03-18 11:02:28,477] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step337 is about to be saved!\u001b[0m\n",
      "\u001b[34m[2025-03-18 11:02:28,484] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: /tmp/finetuned_model/checkpoint-337/global_step337/zero_pp_rank_0_mp_rank_00_model_states.pt\u001b[0m\n",
      "\u001b[34m[2025-03-18 11:02:28,484] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /tmp/finetuned_model/checkpoint-337/global_step337/zero_pp_rank_0_mp_rank_00_model_states.pt...\u001b[0m\n",
      "\u001b[34m[2025-03-18 11:02:28,495] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /tmp/finetuned_model/checkpoint-337/global_step337/zero_pp_rank_0_mp_rank_00_model_states.pt.\u001b[0m\n",
      "\u001b[34m[2025-03-18 11:02:28,498] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /tmp/finetuned_model/checkpoint-337/global_step337/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...\u001b[0m\n",
      "\u001b[34m[2025-03-18 11:02:50,925] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /tmp/finetuned_model/checkpoint-337/global_step337/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.\u001b[0m\n",
      "\u001b[34m[2025-03-18 11:02:50,926] [INFO] [engine.py:3536:_save_zero_checkpoint] zero checkpoint saved /tmp/finetuned_model/checkpoint-337/global_step337/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt\u001b[0m\n",
      "\u001b[34m[2025-03-18 11:02:52,489] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step337 is ready now!\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2657] 2025-03-18 11:02:52,492 >> \u001b[0m\n",
      "\u001b[34mTraining completed. Do not forget to share your model on huggingface.co/models =)\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2657] 2025-03-18 11:02:52,492 >> \u001b[0m\n",
      "\u001b[34mTraining completed. Do not forget to share your model on huggingface.co/models =)\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2895] 2025-03-18 11:02:52,493 >> Loading best model from /tmp/finetuned_model/checkpoint-300 (score: 0.4267697036266327).\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:2895] 2025-03-18 11:02:52,493 >> Loading best model from /tmp/finetuned_model/checkpoint-300 (score: 0.4267697036266327).\u001b[0m\n",
      "\u001b[34m[INFO|deepspeed.py:436] 2025-03-18 11:02:52,494 >> Attempting to resume from /tmp/finetuned_model/checkpoint-300\u001b[0m\n",
      "\u001b[34m[INFO|deepspeed.py:436] 2025-03-18 11:02:52,494 >> Attempting to resume from /tmp/finetuned_model/checkpoint-300\u001b[0m\n",
      "\u001b[34m[2025-03-18 11:02:52,503] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /tmp/finetuned_model/checkpoint-300/global_step300/zero_pp_rank_0_mp_rank_00_model_states.pt...\u001b[0m\n",
      "\u001b[34m[2025-03-18 11:02:52,510] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /tmp/finetuned_model/checkpoint-300/global_step300/zero_pp_rank_0_mp_rank_00_model_states.pt.\u001b[0m\n",
      "\u001b[34m[2025-03-18 11:02:52,511] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /tmp/finetuned_model/checkpoint-300/global_step300/zero_pp_rank_0_mp_rank_00_model_states.pt...\u001b[0m\n",
      "\u001b[34m[2025-03-18 11:02:52,517] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /tmp/finetuned_model/checkpoint-300/global_step300/zero_pp_rank_0_mp_rank_00_model_states.pt.\u001b[0m\n",
      "\u001b[34m[2025-03-18 11:02:52,525] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /tmp/finetuned_model/checkpoint-300/global_step300/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...\u001b[0m\n",
      "\u001b[34m[2025-03-18 11:02:57,684] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /tmp/finetuned_model/checkpoint-300/global_step300/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.\u001b[0m\n",
      "\u001b[34m[2025-03-18 11:02:57,685] [INFO] [engine.py:3076:_get_all_zero_checkpoint_state_dicts] successfully read 8 ZeRO state_dicts for rank 0\u001b[0m\n",
      "\u001b[34m[2025-03-18 11:03:00,063] [INFO] [engine.py:3026:_load_zero_checkpoint] loading 8 zero partition checkpoints for rank 0\u001b[0m\n",
      "\u001b[34m{'train_runtime': 1284.2704, 'train_samples_per_second': 8.409, 'train_steps_per_second': 0.262, 'train_loss': 0.47276945425424094, 'epoch': 1.0}\u001b[0m\n",
      "\u001b[34m100%|██████████| 337/337 [21:23<00:00,  2.94s/it]\u001b[0m\n",
      "\u001b[34m100%|██████████| 337/337 [21:23<00:00,  3.81s/it]\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3942] 2025-03-18 11:03:10,222 >> Saving model checkpoint to /tmp/finetuned_model\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:3942] 2025-03-18 11:03:10,222 >> Saving model checkpoint to /tmp/finetuned_model\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:423] 2025-03-18 11:03:10,225 >> Configuration saved in /tmp/finetuned_model/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:423] 2025-03-18 11:03:10,225 >> Configuration saved in /tmp/finetuned_model/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:909] 2025-03-18 11:03:10,226 >> Configuration saved in /tmp/finetuned_model/generation_config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:909] 2025-03-18 11:03:10,226 >> Configuration saved in /tmp/finetuned_model/generation_config.json\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:3048] 2025-03-18 11:03:26,569 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 3 checkpoint shards. You can find where each parameters has been saved in the index located at /tmp/finetuned_model/model.safetensors.index.json.\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:3048] 2025-03-18 11:03:26,569 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 3 checkpoint shards. You can find where each parameters has been saved in the index located at /tmp/finetuned_model/model.safetensors.index.json.\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2500] 2025-03-18 11:03:26,570 >> tokenizer config file saved in /tmp/finetuned_model/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2500] 2025-03-18 11:03:26,570 >> tokenizer config file saved in /tmp/finetuned_model/tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2509] 2025-03-18 11:03:26,571 >> Special tokens file saved in /tmp/finetuned_model/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2509] 2025-03-18 11:03:26,571 >> Special tokens file saved in /tmp/finetuned_model/special_tokens_map.json\u001b[0m\n",
      "\u001b[34m***** train metrics *****\u001b[0m\n",
      "\u001b[34mepoch                    =     0.9985\u001b[0m\n",
      "\u001b[34mtotal_flos               =     1859GF\u001b[0m\n",
      "\u001b[34mtrain_loss               =     0.4728\u001b[0m\n",
      "\u001b[34mtrain_runtime            = 0:21:24.27\u001b[0m\n",
      "\u001b[34mtrain_samples_per_second =      8.409\u001b[0m\n",
      "\u001b[34mtrain_steps_per_second   =      0.262\u001b[0m\n",
      "\u001b[34mFigure saved at:\u001b[0m\n",
      "\u001b[34m/tmp/finetuned_model/training_loss.png\u001b[0m\n",
      "\u001b[34mFigure saved at:\u001b[0m\n",
      "\u001b[34m/tmp/finetuned_model/training_eval_loss.png\u001b[0m\n",
      "\u001b[34m[WARNING|2025-03-18 11:03:27] llamafactory.extras.ploting:162 >> No metric eval_accuracy to plot.\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:4258] 2025-03-18 11:03:27,268 >> \u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:4258] 2025-03-18 11:03:27,268 >> \u001b[0m\n",
      "\u001b[34m***** Running Evaluation *****\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:4260] 2025-03-18 11:03:27,268 >>   Num examples = 1200\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:4260] 2025-03-18 11:03:27,268 >>   Num examples = 1200\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:4263] 2025-03-18 11:03:27,268 >>   Batch size = 2\u001b[0m\n",
      "\u001b[34m[INFO|trainer.py:4263] 2025-03-18 11:03:27,268 >>   Batch size = 2\u001b[0m\n",
      "\u001b[34m0%|          | 0/75 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m3%|▎         | 2/75 [00:00<00:07,  9.50it/s]\u001b[0m\n",
      "\u001b[34m4%|▍         | 3/75 [00:00<00:10,  6.68it/s]\u001b[0m\n",
      "\u001b[34m5%|▌         | 4/75 [00:00<00:12,  5.75it/s]\u001b[0m\n",
      "\u001b[34m7%|▋         | 5/75 [00:00<00:13,  5.34it/s]\u001b[0m\n",
      "\u001b[34m8%|▊         | 6/75 [00:01<00:13,  5.11it/s]\u001b[0m\n",
      "\u001b[34m9%|▉         | 7/75 [00:01<00:13,  4.95it/s]\u001b[0m\n",
      "\u001b[34m11%|█         | 8/75 [00:01<00:13,  4.88it/s]\u001b[0m\n",
      "\u001b[34m12%|█▏        | 9/75 [00:01<00:13,  4.79it/s]\u001b[0m\n",
      "\u001b[34m13%|█▎        | 10/75 [00:01<00:13,  4.76it/s]\u001b[0m\n",
      "\u001b[34m15%|█▍        | 11/75 [00:02<00:13,  4.74it/s]\u001b[0m\n",
      "\u001b[34m16%|█▌        | 12/75 [00:02<00:13,  4.71it/s]\u001b[0m\n",
      "\u001b[34m17%|█▋        | 13/75 [00:02<00:13,  4.68it/s]\u001b[0m\n",
      "\u001b[34m19%|█▊        | 14/75 [00:02<00:13,  4.69it/s]\u001b[0m\n",
      "\u001b[34m20%|██        | 15/75 [00:02<00:12,  4.70it/s]\u001b[0m\n",
      "\u001b[34m21%|██▏       | 16/75 [00:03<00:12,  4.67it/s]\u001b[0m\n",
      "\u001b[34m23%|██▎       | 17/75 [00:03<00:12,  4.69it/s]\u001b[0m\n",
      "\u001b[34m24%|██▍       | 18/75 [00:03<00:12,  4.67it/s]\u001b[0m\n",
      "\u001b[34m25%|██▌       | 19/75 [00:03<00:11,  4.68it/s]\u001b[0m\n",
      "\u001b[34m27%|██▋       | 20/75 [00:04<00:11,  4.69it/s]\u001b[0m\n",
      "\u001b[34m28%|██▊       | 21/75 [00:04<00:11,  4.68it/s]\u001b[0m\n",
      "\u001b[34m29%|██▉       | 22/75 [00:04<00:11,  4.69it/s]\u001b[0m\n",
      "\u001b[34m31%|███       | 23/75 [00:04<00:11,  4.67it/s]\u001b[0m\n",
      "\u001b[34m32%|███▏      | 24/75 [00:04<00:10,  4.68it/s]\u001b[0m\n",
      "\u001b[34m33%|███▎      | 25/75 [00:05<00:10,  4.69it/s]\u001b[0m\n",
      "\u001b[34m35%|███▍      | 26/75 [00:05<00:10,  4.68it/s]\u001b[0m\n",
      "\u001b[34m36%|███▌      | 27/75 [00:05<00:10,  4.69it/s]\u001b[0m\n",
      "\u001b[34m37%|███▋      | 28/75 [00:05<00:10,  4.67it/s]\u001b[0m\n",
      "\u001b[34m39%|███▊      | 29/75 [00:05<00:09,  4.69it/s]\u001b[0m\n",
      "\u001b[34m40%|████      | 30/75 [00:06<00:09,  4.70it/s]\u001b[0m\n",
      "\u001b[34m41%|████▏     | 31/75 [00:06<00:09,  4.71it/s]\u001b[0m\n",
      "\u001b[34m43%|████▎     | 32/75 [00:06<00:09,  4.68it/s]\u001b[0m\n",
      "\u001b[34m44%|████▍     | 33/75 [00:06<00:08,  4.69it/s]\u001b[0m\n",
      "\u001b[34m45%|████▌     | 34/75 [00:07<00:08,  4.70it/s]\u001b[0m\n",
      "\u001b[34m47%|████▋     | 35/75 [00:07<00:08,  4.67it/s]\u001b[0m\n",
      "\u001b[34m48%|████▊     | 36/75 [00:07<00:08,  4.69it/s]\u001b[0m\n",
      "\u001b[34m49%|████▉     | 37/75 [00:07<00:08,  4.67it/s]\u001b[0m\n",
      "\u001b[34m51%|█████     | 38/75 [00:07<00:07,  4.68it/s]\u001b[0m\n",
      "\u001b[34m52%|█████▏    | 39/75 [00:08<00:07,  4.69it/s]\u001b[0m\n",
      "\u001b[34m53%|█████▎    | 40/75 [00:08<00:07,  4.68it/s]\u001b[0m\n",
      "\u001b[34m55%|█████▍    | 41/75 [00:08<00:07,  4.69it/s]\u001b[0m\n",
      "\u001b[34m56%|█████▌    | 42/75 [00:08<00:07,  4.67it/s]\u001b[0m\n",
      "\u001b[34m57%|█████▋    | 43/75 [00:08<00:06,  4.67it/s]\u001b[0m\n",
      "\u001b[34m59%|█████▊    | 44/75 [00:09<00:06,  4.69it/s]\u001b[0m\n",
      "\u001b[34m60%|██████    | 45/75 [00:09<00:06,  4.67it/s]\u001b[0m\n",
      "\u001b[34m61%|██████▏   | 46/75 [00:09<00:06,  4.66it/s]\u001b[0m\n",
      "\u001b[34m63%|██████▎   | 47/75 [00:09<00:05,  4.67it/s]\u001b[0m\n",
      "\u001b[34m64%|██████▍   | 48/75 [00:10<00:05,  4.68it/s]\u001b[0m\n",
      "\u001b[34m65%|██████▌   | 49/75 [00:10<00:05,  4.67it/s]\u001b[0m\n",
      "\u001b[34m67%|██████▋   | 50/75 [00:10<00:05,  4.69it/s]\u001b[0m\n",
      "\u001b[34m68%|██████▊   | 51/75 [00:10<00:05,  4.67it/s]\u001b[0m\n",
      "\u001b[34m69%|██████▉   | 52/75 [00:10<00:04,  4.68it/s]\u001b[0m\n",
      "\u001b[34m71%|███████   | 53/75 [00:11<00:04,  4.70it/s]\u001b[0m\n",
      "\u001b[34m72%|███████▏  | 54/75 [00:11<00:04,  4.68it/s]\u001b[0m\n",
      "\u001b[34m73%|███████▎  | 55/75 [00:11<00:04,  4.70it/s]\u001b[0m\n",
      "\u001b[34m75%|███████▍  | 56/75 [00:11<00:04,  4.67it/s]\u001b[0m\n",
      "\u001b[34m76%|███████▌  | 57/75 [00:11<00:03,  4.68it/s]\u001b[0m\n",
      "\u001b[34m77%|███████▋  | 58/75 [00:12<00:03,  4.69it/s]\u001b[0m\n",
      "\u001b[34m79%|███████▊  | 59/75 [00:12<00:03,  4.67it/s]\u001b[0m\n",
      "\u001b[34m80%|████████  | 60/75 [00:12<00:03,  4.68it/s]\u001b[0m\n",
      "\u001b[34m81%|████████▏ | 61/75 [00:12<00:02,  4.69it/s]\u001b[0m\n",
      "\u001b[34m83%|████████▎ | 62/75 [00:13<00:02,  4.70it/s]\u001b[0m\n",
      "\u001b[34m84%|████████▍ | 63/75 [00:13<00:02,  4.69it/s]\u001b[0m\n",
      "\u001b[34m85%|████████▌ | 64/75 [00:13<00:02,  4.70it/s]\u001b[0m\n",
      "\u001b[34m87%|████████▋ | 65/75 [00:13<00:02,  4.68it/s]\u001b[0m\n",
      "\u001b[34m88%|████████▊ | 66/75 [00:13<00:01,  4.69it/s]\u001b[0m\n",
      "\u001b[34m89%|████████▉ | 67/75 [00:14<00:01,  4.70it/s]\u001b[0m\n",
      "\u001b[34m91%|█████████ | 68/75 [00:14<00:01,  4.68it/s]\u001b[0m\n",
      "\u001b[34m92%|█████████▏| 69/75 [00:14<00:01,  4.69it/s]\u001b[0m\n",
      "\u001b[34m93%|█████████▎| 70/75 [00:14<00:01,  4.67it/s]\u001b[0m\n",
      "\u001b[34m95%|█████████▍| 71/75 [00:14<00:00,  4.67it/s]\u001b[0m\n",
      "\u001b[34m96%|█████████▌| 72/75 [00:15<00:00,  4.68it/s]\u001b[0m\n",
      "\u001b[34m97%|█████████▋| 73/75 [00:15<00:00,  4.67it/s]\u001b[0m\n",
      "\u001b[34m99%|█████████▊| 74/75 [00:15<00:00,  4.69it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 75/75 [00:15<00:00,  4.65it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 75/75 [00:15<00:00,  4.74it/s]\u001b[0m\n",
      "\u001b[34m***** eval metrics *****\u001b[0m\n",
      "\u001b[34mepoch                   =     0.9985\u001b[0m\n",
      "\u001b[34meval_loss               =     0.4268\u001b[0m\n",
      "\u001b[34meval_runtime            = 0:00:16.02\u001b[0m\n",
      "\u001b[34meval_samples_per_second =     74.877\u001b[0m\n",
      "\u001b[34meval_steps_per_second   =       4.68\u001b[0m\n",
      "\u001b[34m[INFO|modelcard.py:449] 2025-03-18 11:03:43,295 >> Dropping the following result as it does not have all the necessary fields:\u001b[0m\n",
      "\u001b[34m{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\u001b[0m\n",
      "\u001b[34m[INFO|modelcard.py:449] 2025-03-18 11:03:43,295 >> Dropping the following result as it does not have all the necessary fields:\u001b[0m\n",
      "\u001b[34m{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\u001b[0m\n",
      "\u001b[34m#033[1;34mwandb#033[0m: \u001b[0m\n",
      "\u001b[34m#033[1;34mwandb#033[0m: 🚀 View run #033[33m/tmp/finetuned_model#033[0m at: #033[34mhttps://wandb.ai/407383787/llamafactory/runs/deepseek6-7B-finetune-2025-03-18-10-29-00-527-3pztry-algo-1#033[0m\u001b[0m\n",
      "\u001b[34m#033[1;34mwandb#033[0m: Find logs at: #033[1;35mwandb/run-20250318_104136-deepseek6-7B-finetune-2025-03-18-10-29-00-527-3pztry-algo-1/logs#033[0m\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker.pytorch import PyTorch\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "instance_count = 1\n",
    "instance_type = 'ml.p4d.24xlarge'\n",
    "# instance_type = 'ml.g5.48xlarge'  ## 8*24G\n",
    "max_time = 200000\n",
    "\n",
    "# Get the current time\n",
    "current_time = datetime.now()\n",
    "\n",
    "wandb.sagemaker_auth(path=\"llama_factory/\")\n",
    "# Format the current time as a string\n",
    "formatted_time = current_time.strftime(\"%Y%m%d%H%M%S\")\n",
    "print(formatted_time)\n",
    "\n",
    "base_job_name = 'deepseek6-7B-finetune'\n",
    "environment = {\n",
    "    'NODE_NUMBER':str(instance_count),\n",
    "    'MODEL_S3_PATH': f's3://{sagemaker_default_bucket}/Foundation-Models/deepseek_coder', # source model files\n",
    "    'MODEL_LOCAL_PATH': '/tmp/pretrain_model',\n",
    "    'OUTPUT_MODEL_S3_PATH': f's3://{sagemaker_default_bucket}/deepseek-coder-6.7b-base/finetuned_model/', # destination\n",
    "}\n",
    "\n",
    "estimator = PyTorch(entry_point='entry.py',\n",
    "                            source_dir='llama_factory/',\n",
    "                            role=role,\n",
    "                            base_job_name=base_job_name,\n",
    "                            environment=environment,\n",
    "                            framework_version='2.1.0',\n",
    "                            py_version='py310',\n",
    "                            script_mode=True,\n",
    "                            instance_count=instance_count,\n",
    "                            instance_type=instance_type,\n",
    "                            max_run=max_time)\n",
    "\n",
    "# # data in channel will be automatically copied to each node - /opt/ml/input/data/train1\n",
    "#input_channel = {'train': f's3://{sagemaker_default_bucket}/datasets/qiandao/{version}/train.json'}\n",
    "estimator.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe231c9-ffe5-4ed7-99ff-431730c2a22a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
