{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f78e3de0-d9f2-4099-88ad-d2932fe952d1",
   "metadata": {},
   "source": [
    "# Multi-Node Training on SageMaker Training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1465fc2e-11a6-49fa-a0ff-84b6e0ea1501",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sagemaker in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (2.231.0)\n",
      "Collecting sagemaker\n",
      "  Using cached sagemaker-2.232.1-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting huggingface_hub\n",
      "  Using cached huggingface_hub-0.25.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: attrs<24,>=23.1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (23.2.0)\n",
      "Requirement already satisfied: boto3<2.0,>=1.34.142 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (1.35.16)\n",
      "Requirement already satisfied: cloudpickle==2.2.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (2.2.1)\n",
      "Requirement already satisfied: docker in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (7.1.0)\n",
      "Requirement already satisfied: google-pasta in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (0.2.0)\n",
      "Requirement already satisfied: importlib-metadata<7.0,>=1.4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (6.11.0)\n",
      "Requirement already satisfied: jsonschema in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (4.23.0)\n",
      "Requirement already satisfied: numpy<2.0,>=1.9.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (1.22.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (21.3)\n",
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (2.2.2)\n",
      "Requirement already satisfied: pathos in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (0.3.2)\n",
      "Requirement already satisfied: platformdirs in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (4.2.2)\n",
      "Requirement already satisfied: protobuf<5.0,>=3.12 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (4.25.4)\n",
      "Requirement already satisfied: psutil in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (6.0.0)\n",
      "Requirement already satisfied: pyyaml~=6.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (6.0.1)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (2.32.3)\n",
      "Requirement already satisfied: sagemaker-core<2.0.0,>=1.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (1.0.4)\n",
      "Requirement already satisfied: schema in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (0.7.7)\n",
      "Requirement already satisfied: smdebug-rulesconfig==1.0.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (1.0.1)\n",
      "Requirement already satisfied: tblib<4,>=1.7.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (3.0.0)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (4.66.4)\n",
      "Requirement already satisfied: urllib3<3.0.0,>=1.26.8 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker) (2.2.2)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from huggingface_hub) (3.15.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from huggingface_hub) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from huggingface_hub) (4.12.2)\n",
      "Requirement already satisfied: botocore<1.36.0,>=1.35.16 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from boto3<2.0,>=1.34.142->sagemaker) (1.35.16)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from boto3<2.0,>=1.34.142->sagemaker) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from boto3<2.0,>=1.34.142->sagemaker) (0.10.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from importlib-metadata<7.0,>=1.4.0->sagemaker) (3.19.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from packaging>=20.0->sagemaker) (3.1.2)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=1.7.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker-core<2.0.0,>=1.0.0->sagemaker) (2.9.1)\n",
      "Requirement already satisfied: rich<14.0.0,>=13.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker-core<2.0.0,>=1.0.0->sagemaker) (13.7.1)\n",
      "Requirement already satisfied: mock<5.0,>4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sagemaker-core<2.0.0,>=1.0.0->sagemaker) (4.0.3)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from jsonschema->sagemaker) (2023.12.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from jsonschema->sagemaker) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from jsonschema->sagemaker) (0.19.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests->sagemaker) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests->sagemaker) (3.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests->sagemaker) (2024.7.4)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from google-pasta->sagemaker) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pandas->sagemaker) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pandas->sagemaker) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pandas->sagemaker) (2024.1)\n",
      "Requirement already satisfied: ppft>=1.7.6.8 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pathos->sagemaker) (1.7.6.8)\n",
      "Requirement already satisfied: dill>=0.3.8 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pathos->sagemaker) (0.3.8)\n",
      "Requirement already satisfied: pox>=0.3.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pathos->sagemaker) (0.3.4)\n",
      "Requirement already satisfied: multiprocess>=0.70.16 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pathos->sagemaker) (0.70.16)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pydantic<3.0.0,>=1.7.0->sagemaker-core<2.0.0,>=1.0.0->sagemaker) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pydantic<3.0.0,>=1.7.0->sagemaker-core<2.0.0,>=1.0.0->sagemaker) (2.23.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from rich<14.0.0,>=13.0.0->sagemaker-core<2.0.0,>=1.0.0->sagemaker) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from rich<14.0.0,>=13.0.0->sagemaker-core<2.0.0,>=1.0.0->sagemaker) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=13.0.0->sagemaker-core<2.0.0,>=1.0.0->sagemaker) (0.1.2)\n",
      "Using cached sagemaker-2.232.1-py3-none-any.whl (1.6 MB)\n",
      "Using cached huggingface_hub-0.25.1-py3-none-any.whl (436 kB)\n",
      "Installing collected packages: huggingface_hub, sagemaker\n",
      "  Attempting uninstall: sagemaker\n",
      "    Found existing installation: sagemaker 2.231.0\n",
      "    Uninstalling sagemaker-2.231.0:\n",
      "      Successfully uninstalled sagemaker-2.231.0\n",
      "Successfully installed huggingface_hub-0.25.1 sagemaker-2.232.1\n"
     ]
    }
   ],
   "source": [
    "# ## Update sagemaker python sdk version\n",
    "!pip install -U sagemaker huggingface_hub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4fc2fef-6ba1-4df5-9c19-169f2de789d6",
   "metadata": {},
   "source": [
    "## Set model, Code and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25d3e133-95da-4751-bffd-71f4c1aa9a2b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n",
      "sagemaker_default_bucket: sagemaker-us-east-1-596899493901\n",
      "sagemaker_region: us-east-1\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "sagemaker_default_bucket = sess.default_bucket()\n",
    "region = sess.boto_session.region_name\n",
    "print(\"sagemaker_default_bucket:\", sagemaker_default_bucket)\n",
    "print(\"sagemaker_region:\", region)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3da8a94",
   "metadata": {},
   "source": [
    "## upload pretrain models to s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d471f6ad-47a5-4a56-a752-541373de9a13",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f805097a9284d818ab49c3558f20128",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 14 files:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73fe554716bf4f8b8fd047fe94e03672",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       ".gitattributes:   0%|          | 0.00/1.52k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3e4d769bf9746c1afb2895f3cc1382c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/243 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "627a7e75c729407db82f30ff7538e62e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9da51bb64ca84c44963c43e28d3cab3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/6.00k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfc6d4c07def43d1a44459a7cbeeac20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "059893e334674595ac26177ded92aed9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/3.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ee990d7b54d40bd9925a1f903d7eb38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/663 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b013c89494d44a36a04995a6a2ccb95b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "LICENSE:   0%|          | 0.00/11.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4131a13a041a4364b1ee3e8e0c8fac98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/3.86G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc9c33bf2246483fa76ec92f327b1431",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/27.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53aad69ec7ca47dda7e73bd13fb4e4eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/3.56G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7417fe82ab94c709f3a728fbe5ed8e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd8d22fd2bb542e1b29eae4aec6f756d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/7.30k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d7b771cdd5c43d892f090169dd99dab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Code language: python\n",
    "from huggingface_hub import snapshot_download\n",
    "from pathlib import Path\n",
    "\n",
    "local_cache_path = Path(\"./Qwen2.5_7B\")\n",
    "local_cache_path.mkdir(exist_ok=True)\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "hf_token = \"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\"\n",
    "# Only download pytorch checkpoint files\n",
    "allow_patterns = [\"*\"]\n",
    "\n",
    "model_download_path = snapshot_download(\n",
    "    repo_id=model_name,\n",
    "    cache_dir=local_cache_path,\n",
    "    allow_patterns=allow_patterns,\n",
    "    token = hf_token\n",
    ")\n",
    "model_snapshot_path = list(local_cache_path.glob(\"**/snapshots/*\"))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "372a7552-e0be-4db5-8fb2-145e0b8d0bfc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: Qwen2.5_7B/models--Qwen--Qwen2.5-7B-Instruct/snapshots/bb46c15ee4bb56c5b63245ef50fd7637234d6f75/generation_config.json to s3://sagemaker-us-east-1-596899493901/Foundation-Models/Qwen2.5_7B_it/generation_config.json\n",
      "upload: Qwen2.5_7B/models--Qwen--Qwen2.5-7B-Instruct/snapshots/bb46c15ee4bb56c5b63245ef50fd7637234d6f75/config.json to s3://sagemaker-us-east-1-596899493901/Foundation-Models/Qwen2.5_7B_it/config.json\n",
      "upload: Qwen2.5_7B/models--Qwen--Qwen2.5-7B-Instruct/snapshots/bb46c15ee4bb56c5b63245ef50fd7637234d6f75/.gitattributes to s3://sagemaker-us-east-1-596899493901/Foundation-Models/Qwen2.5_7B_it/.gitattributes\n",
      "upload: Qwen2.5_7B/models--Qwen--Qwen2.5-7B-Instruct/snapshots/bb46c15ee4bb56c5b63245ef50fd7637234d6f75/README.md to s3://sagemaker-us-east-1-596899493901/Foundation-Models/Qwen2.5_7B_it/README.md\n",
      "upload: Qwen2.5_7B/models--Qwen--Qwen2.5-7B-Instruct/snapshots/bb46c15ee4bb56c5b63245ef50fd7637234d6f75/LICENSE to s3://sagemaker-us-east-1-596899493901/Foundation-Models/Qwen2.5_7B_it/LICENSE\n",
      "upload: Qwen2.5_7B/models--Qwen--Qwen2.5-7B-Instruct/snapshots/bb46c15ee4bb56c5b63245ef50fd7637234d6f75/model.safetensors.index.json to s3://sagemaker-us-east-1-596899493901/Foundation-Models/Qwen2.5_7B_it/model.safetensors.index.json\n",
      "upload: Qwen2.5_7B/models--Qwen--Qwen2.5-7B-Instruct/snapshots/bb46c15ee4bb56c5b63245ef50fd7637234d6f75/merges.txt to s3://sagemaker-us-east-1-596899493901/Foundation-Models/Qwen2.5_7B_it/merges.txt\n",
      "upload: Qwen2.5_7B/models--Qwen--Qwen2.5-7B-Instruct/snapshots/bb46c15ee4bb56c5b63245ef50fd7637234d6f75/tokenizer_config.json to s3://sagemaker-us-east-1-596899493901/Foundation-Models/Qwen2.5_7B_it/tokenizer_config.json\n",
      "upload: Qwen2.5_7B/models--Qwen--Qwen2.5-7B-Instruct/snapshots/bb46c15ee4bb56c5b63245ef50fd7637234d6f75/tokenizer.json to s3://sagemaker-us-east-1-596899493901/Foundation-Models/Qwen2.5_7B_it/tokenizer.json\n",
      "upload: Qwen2.5_7B/models--Qwen--Qwen2.5-7B-Instruct/snapshots/bb46c15ee4bb56c5b63245ef50fd7637234d6f75/vocab.json to s3://sagemaker-us-east-1-596899493901/Foundation-Models/Qwen2.5_7B_it/vocab.json\n",
      "upload: Qwen2.5_7B/models--Qwen--Qwen2.5-7B-Instruct/snapshots/bb46c15ee4bb56c5b63245ef50fd7637234d6f75/model-00001-of-00004.safetensors to s3://sagemaker-us-east-1-596899493901/Foundation-Models/Qwen2.5_7B_it/model-00001-of-00004.safetensors\n",
      "upload: Qwen2.5_7B/models--Qwen--Qwen2.5-7B-Instruct/snapshots/bb46c15ee4bb56c5b63245ef50fd7637234d6f75/model-00003-of-00004.safetensors to s3://sagemaker-us-east-1-596899493901/Foundation-Models/Qwen2.5_7B_it/model-00003-of-00004.safetensors\n",
      "upload: Qwen2.5_7B/models--Qwen--Qwen2.5-7B-Instruct/snapshots/bb46c15ee4bb56c5b63245ef50fd7637234d6f75/model-00002-of-00004.safetensors to s3://sagemaker-us-east-1-596899493901/Foundation-Models/Qwen2.5_7B_it/model-00002-of-00004.safetensors\n",
      "upload: Qwen2.5_7B/models--Qwen--Qwen2.5-7B-Instruct/snapshots/bb46c15ee4bb56c5b63245ef50fd7637234d6f75/model-00004-of-00004.safetensors to s3://sagemaker-us-east-1-596899493901/Foundation-Models/Qwen2.5_7B_it/model-00004-of-00004.safetensors\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp {model_snapshot_path} s3://{sagemaker_default_bucket}/Foundation-Models/Qwen2.5_7B_it --recursive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "995092ad-b6b3-4aad-90cc-1a3c660317b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!rm -rf ./Qwen2.5_7B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11311c58",
   "metadata": {},
   "source": [
    "## Setup for wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a09b3ad4-909e-48ce-8823-bc600d563dcd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wandb\n",
      "  Downloading wandb-0.18.3-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.7 kB)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from wandb) (8.1.7)\n",
      "Collecting docker-pycreds>=0.4.0 (from wandb)\n",
      "  Using cached docker_pycreds-0.4.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting gitpython!=3.1.29,>=1.0.0 (from wandb)\n",
      "  Using cached GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: platformdirs in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from wandb) (4.2.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from wandb) (4.25.4)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from wandb) (6.0.0)\n",
      "Requirement already satisfied: pyyaml in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from wandb) (6.0.1)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from wandb) (2.32.3)\n",
      "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
      "  Downloading sentry_sdk-2.15.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
      "Collecting setproctitle (from wandb)\n",
      "  Using cached setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.9 kB)\n",
      "Requirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from wandb) (71.0.4)\n",
      "Requirement already satisfied: six>=1.4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb)\n",
      "  Using cached gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2024.7.4)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb)\n",
      "  Using cached smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Downloading wandb-0.18.3-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m113.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hUsing cached docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Using cached GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
      "Downloading sentry_sdk-2.15.0-py2.py3-none-any.whl (310 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.0/311.0 kB\u001b[0m \u001b[31m43.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
      "Using cached gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
      "Using cached smmap-5.0.1-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, gitpython, wandb\n",
      "Successfully installed docker-pycreds-0.4.0 gitdb-4.0.11 gitpython-3.1.43 sentry-sdk-2.15.0 setproctitle-1.3.3 smmap-5.0.1 wandb-0.18.3\n"
     ]
    }
   ],
   "source": [
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d3bafe1-097f-410b-9919-9e402887f398",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33m407383787\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a54ecc7",
   "metadata": {},
   "source": [
    "## Submit Training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b165769a-0949-493e-99e4-c627f5fecf98",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20241008090843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:image_uri is not presented, retrieving image_uri based on instance_type, framework etc.\n",
      "INFO:sagemaker:Creating training-job with name: Qwen25-7B-it-finetune-2024-10-08-09-08-43-528\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-10-08 09:08:50 Starting - Starting the training job\n",
      "2024-10-08 09:08:50 Pending - Training job waiting for capacity......\n",
      "2024-10-08 09:09:37 Pending - Preparing the instances for training..............................\n",
      "2024-10-08 09:14:37 Downloading - Downloading input data...\n",
      "2024-10-08 09:15:24 Downloading - Downloading the training image..................\n",
      "2024-10-08 09:18:16 Training - Training image download completed. Training in progress......\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/paramiko/pkey.py:100: CryptographyDeprecationWarning: TripleDES has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.TripleDES and will be removed from this module in 48.0.0.\n",
      "  \"cipher\": algorithms.TripleDES,\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.10/site-packages/paramiko/transport.py:259: CryptographyDeprecationWarning: TripleDES has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.TripleDES and will be removed from this module in 48.0.0.\n",
      "  \"class\": algorithms.TripleDES,\u001b[0m\n",
      "\u001b[34m2024-10-08 09:19:04,447 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2024-10-08 09:19:04,560 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-10-08 09:19:04,570 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2024-10-08 09:19:04,571 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2024-10-08 09:19:07,026 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt\u001b[0m\n",
      "\u001b[34mCollecting transformers<=4.45.0,>=4.41.2 (from -r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading transformers-4.45.0-py3-none-any.whl.metadata (44 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 44.4/44.4 kB 5.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting datasets<=2.21.0,>=2.16.0 (from -r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading datasets-2.21.0-py3-none-any.whl.metadata (21 kB)\u001b[0m\n",
      "\u001b[34mCollecting accelerate==0.34.0 (from -r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading accelerate-0.34.0-py3-none-any.whl.metadata (19 kB)\u001b[0m\n",
      "\u001b[34mCollecting peft<=0.12.0,>=0.11.1 (from -r requirements.txt (line 4))\u001b[0m\n",
      "\u001b[34mDownloading peft-0.12.0-py3-none-any.whl.metadata (13 kB)\u001b[0m\n",
      "\u001b[34mCollecting trl<=0.9.6,>=0.8.6 (from -r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mDownloading trl-0.9.6-py3-none-any.whl.metadata (12 kB)\u001b[0m\n",
      "\u001b[34mCollecting gradio>=4.0.0 (from -r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mDownloading gradio-4.44.1-py3-none-any.whl.metadata (15 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 7)) (2.2.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 8)) (1.14.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: einops in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 9)) (0.8.0)\u001b[0m\n",
      "\u001b[34mCollecting sentencepiece (from -r requirements.txt (line 10))\u001b[0m\n",
      "\u001b[34mDownloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\u001b[0m\n",
      "\u001b[34mCollecting tiktoken (from -r requirements.txt (line 11))\u001b[0m\n",
      "\u001b[34mDownloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 12)) (3.20.3)\u001b[0m\n",
      "\u001b[34mCollecting uvicorn (from -r requirements.txt (line 13))\u001b[0m\n",
      "\u001b[34mDownloading uvicorn-0.31.0-py3-none-any.whl.metadata (6.6 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pydantic in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 14)) (2.8.2)\u001b[0m\n",
      "\u001b[34mCollecting fastapi (from -r requirements.txt (line 15))\u001b[0m\n",
      "\u001b[34mDownloading fastapi-0.115.0-py3-none-any.whl.metadata (27 kB)\u001b[0m\n",
      "\u001b[34mCollecting sse-starlette (from -r requirements.txt (line 16))\u001b[0m\n",
      "\u001b[34mDownloading sse_starlette-2.1.3-py3-none-any.whl.metadata (5.8 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: matplotlib>=3.7.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 17)) (3.9.1)\u001b[0m\n",
      "\u001b[34mCollecting fire (from -r requirements.txt (line 18))\u001b[0m\n",
      "\u001b[34mDownloading fire-0.7.0.tar.gz (87 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 87.2/87.2 kB 10.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 19)) (23.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 20)) (6.0.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy<2.0.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 21)) (1.26.4)\u001b[0m\n",
      "\u001b[34mCollecting deepspeed (from -r requirements.txt (line 22))\u001b[0m\n",
      "\u001b[34mDownloading deepspeed-0.15.1.tar.gz (1.4 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.4/1.4 MB 60.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34mPreparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCollecting wandb (from -r requirements.txt (line 23))\u001b[0m\n",
      "\u001b[34mDownloading wandb-0.18.3-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.7 kB)\u001b[0m\n",
      "\u001b[34mCollecting bitsandbytes>=0.37.0 (from -r requirements.txt (line 24))\u001b[0m\n",
      "\u001b[34mDownloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate==0.34.0->-r requirements.txt (line 3)) (6.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.34.0->-r requirements.txt (line 3)) (2.1.0)\u001b[0m\n",
      "\u001b[34mCollecting huggingface-hub>=0.21.0 (from accelerate==0.34.0->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.25.1-py3-none-any.whl.metadata (13 kB)\u001b[0m\n",
      "\u001b[34mCollecting safetensors>=0.4.3 (from accelerate==0.34.0->-r requirements.txt (line 3))\u001b[0m\n",
      "\u001b[34mDownloading safetensors-0.4.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers<=4.45.0,>=4.41.2->-r requirements.txt (line 1)) (3.15.4)\u001b[0m\n",
      "\u001b[34mCollecting regex!=2019.12.17 (from transformers<=4.45.0,>=4.41.2->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading regex-2024.9.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 40.5/40.5 kB 6.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers<=4.45.0,>=4.41.2->-r requirements.txt (line 1)) (2.32.3)\u001b[0m\n",
      "\u001b[34mCollecting tokenizers<0.21,>=0.20 (from transformers<=4.45.0,>=4.41.2->-r requirements.txt (line 1))\u001b[0m\n",
      "\u001b[34mDownloading tokenizers-0.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers<=4.45.0,>=4.41.2->-r requirements.txt (line 1)) (4.66.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets<=2.21.0,>=2.16.0->-r requirements.txt (line 2)) (17.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets<=2.21.0,>=2.16.0->-r requirements.txt (line 2)) (0.3.8)\u001b[0m\n",
      "\u001b[34mCollecting xxhash (from datasets<=2.21.0,>=2.16.0->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets<=2.21.0,>=2.16.0->-r requirements.txt (line 2)) (0.70.16)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets<=2.21.0,>=2.16.0->-r requirements.txt (line 2)) (2024.6.1)\u001b[0m\n",
      "\u001b[34mCollecting aiohttp (from datasets<=2.21.0,>=2.16.0->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading aiohttp-3.10.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.6 kB)\u001b[0m\n",
      "\u001b[34mCollecting tyro>=0.5.11 (from trl<=0.9.6,>=0.8.6->-r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mDownloading tyro-0.8.11-py3-none-any.whl.metadata (8.4 kB)\u001b[0m\n",
      "\u001b[34mCollecting aiofiles<24.0,>=22.0 (from gradio>=4.0.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mDownloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\u001b[0m\n",
      "\u001b[34mCollecting anyio<5.0,>=3.0 (from gradio>=4.0.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mDownloading anyio-4.6.0-py3-none-any.whl.metadata (4.6 kB)\u001b[0m\n",
      "\u001b[34mCollecting ffmpy (from gradio>=4.0.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mDownloading ffmpy-0.4.0-py3-none-any.whl.metadata (2.9 kB)\u001b[0m\n",
      "\u001b[34mCollecting gradio-client==1.3.0 (from gradio>=4.0.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mDownloading gradio_client-1.3.0-py3-none-any.whl.metadata (7.1 kB)\u001b[0m\n",
      "\u001b[34mCollecting httpx>=0.24.1 (from gradio>=4.0.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mDownloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\u001b[0m\n",
      "\u001b[34mCollecting importlib-resources<7.0,>=1.3 (from gradio>=4.0.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mDownloading importlib_resources-6.4.5-py3-none-any.whl.metadata (4.0 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: jinja2<4.0 in /opt/conda/lib/python3.10/site-packages (from gradio>=4.0.0->-r requirements.txt (line 6)) (3.1.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: markupsafe~=2.0 in /opt/conda/lib/python3.10/site-packages (from gradio>=4.0.0->-r requirements.txt (line 6)) (2.1.5)\u001b[0m\n",
      "\u001b[34mCollecting orjson~=3.0 (from gradio>=4.0.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mDownloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50.4/50.4 kB 7.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pillow<11.0,>=8.0 in /opt/conda/lib/python3.10/site-packages (from gradio>=4.0.0->-r requirements.txt (line 6)) (10.3.0)\u001b[0m\n",
      "\u001b[34mCollecting pydub (from gradio>=4.0.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mDownloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\u001b[0m\n",
      "\u001b[34mCollecting python-multipart>=0.0.9 (from gradio>=4.0.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mDownloading python_multipart-0.0.12-py3-none-any.whl.metadata (1.9 kB)\u001b[0m\n",
      "\u001b[34mCollecting ruff>=0.2.2 (from gradio>=4.0.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mDownloading ruff-0.6.9-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\u001b[0m\n",
      "\u001b[34mCollecting semantic-version~=2.0 (from gradio>=4.0.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mDownloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\u001b[0m\n",
      "\u001b[34mCollecting tomlkit==0.12.0 (from gradio>=4.0.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mDownloading tomlkit-0.12.0-py3-none-any.whl.metadata (2.7 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typer<1.0,>=0.12 in /opt/conda/lib/python3.10/site-packages (from gradio>=4.0.0->-r requirements.txt (line 6)) (0.12.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions~=4.0 in /opt/conda/lib/python3.10/site-packages (from gradio>=4.0.0->-r requirements.txt (line 6)) (4.12.2)\u001b[0m\n",
      "\u001b[34mCollecting urllib3~=2.0 (from gradio>=4.0.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mDownloading urllib3-2.2.3-py3-none-any.whl.metadata (6.5 kB)\u001b[0m\n",
      "\u001b[34mCollecting websockets<13.0,>=10.0 (from gradio-client==1.3.0->gradio>=4.0.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mDownloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas>=2.0.0->-r requirements.txt (line 7)) (2.9.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=2.0.0->-r requirements.txt (line 7)) (2024.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas>=2.0.0->-r requirements.txt (line 7)) (2024.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: click>=7.0 in /opt/conda/lib/python3.10/site-packages (from uvicorn->-r requirements.txt (line 13)) (8.1.7)\u001b[0m\n",
      "\u001b[34mCollecting h11>=0.8 (from uvicorn->-r requirements.txt (line 13))\u001b[0m\n",
      "\u001b[34mDownloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic->-r requirements.txt (line 14)) (0.7.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pydantic-core==2.20.1 in /opt/conda/lib/python3.10/site-packages (from pydantic->-r requirements.txt (line 14)) (2.20.1)\u001b[0m\n",
      "\u001b[34mCollecting starlette<0.39.0,>=0.37.2 (from fastapi->-r requirements.txt (line 15))\u001b[0m\n",
      "\u001b[34mDownloading starlette-0.38.6-py3-none-any.whl.metadata (6.0 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 17)) (1.2.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 17)) (0.12.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 17)) (4.53.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: kiwisolver>=1.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 17)) (1.4.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib>=3.7.0->-r requirements.txt (line 17)) (3.1.2)\u001b[0m\n",
      "\u001b[34mCollecting termcolor (from fire->-r requirements.txt (line 18))\u001b[0m\n",
      "\u001b[34mDownloading termcolor-2.5.0-py3-none-any.whl.metadata (6.1 kB)\u001b[0m\n",
      "\u001b[34mCollecting hjson (from deepspeed->-r requirements.txt (line 22))\u001b[0m\n",
      "\u001b[34mDownloading hjson-3.1.0-py3-none-any.whl.metadata (2.6 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: ninja in /opt/conda/lib/python3.10/site-packages (from deepspeed->-r requirements.txt (line 22)) (1.11.1.1)\u001b[0m\n",
      "\u001b[34mCollecting py-cpuinfo (from deepspeed->-r requirements.txt (line 22))\u001b[0m\n",
      "\u001b[34mDownloading py_cpuinfo-9.0.0-py3-none-any.whl.metadata (794 bytes)\u001b[0m\n",
      "\u001b[34mCollecting nvidia-ml-py (from deepspeed->-r requirements.txt (line 22))\u001b[0m\n",
      "\u001b[34mDownloading nvidia_ml_py-12.560.30-py3-none-any.whl.metadata (8.6 kB)\u001b[0m\n",
      "\u001b[34mCollecting docker-pycreds>=0.4.0 (from wandb->-r requirements.txt (line 23))\u001b[0m\n",
      "\u001b[34mDownloading docker_pycreds-0.4.0-py2.py3-none-any.whl.metadata (1.8 kB)\u001b[0m\n",
      "\u001b[34mCollecting gitpython!=3.1.29,>=1.0.0 (from wandb->-r requirements.txt (line 23))\u001b[0m\n",
      "\u001b[34mDownloading GitPython-3.1.43-py3-none-any.whl.metadata (13 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: platformdirs in /opt/conda/lib/python3.10/site-packages (from wandb->-r requirements.txt (line 23)) (4.2.2)\u001b[0m\n",
      "\u001b[34mCollecting sentry-sdk>=1.0.0 (from wandb->-r requirements.txt (line 23))\u001b[0m\n",
      "\u001b[34mDownloading sentry_sdk-2.15.0-py2.py3-none-any.whl.metadata (9.7 kB)\u001b[0m\n",
      "\u001b[34mCollecting setproctitle (from wandb->-r requirements.txt (line 23))\u001b[0m\n",
      "\u001b[34mDownloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.9 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from wandb->-r requirements.txt (line 23)) (72.1.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.10/site-packages (from anyio<5.0,>=3.0->gradio>=4.0.0->-r requirements.txt (line 6)) (3.7)\u001b[0m\n",
      "\u001b[34mCollecting sniffio>=1.1 (from anyio<5.0,>=3.0->gradio>=4.0.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mDownloading sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio<5.0,>=3.0->gradio>=4.0.0->-r requirements.txt (line 6)) (1.2.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb->-r requirements.txt (line 23)) (1.16.0)\u001b[0m\n",
      "\u001b[34mCollecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets<=2.21.0,>=2.16.0->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading aiohappyeyeballs-2.4.3-py3-none-any.whl.metadata (6.1 kB)\u001b[0m\n",
      "\u001b[34mCollecting aiosignal>=1.1.2 (from aiohttp->datasets<=2.21.0,>=2.16.0->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets<=2.21.0,>=2.16.0->-r requirements.txt (line 2)) (23.2.0)\u001b[0m\n",
      "\u001b[34mCollecting frozenlist>=1.1.1 (from aiohttp->datasets<=2.21.0,>=2.16.0->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\u001b[0m\n",
      "\u001b[34mCollecting multidict<7.0,>=4.5 (from aiohttp->datasets<=2.21.0,>=2.16.0->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading multidict-6.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.0 kB)\u001b[0m\n",
      "\u001b[34mCollecting yarl<2.0,>=1.12.0 (from aiohttp->datasets<=2.21.0,>=2.16.0->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading yarl-1.13.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 50.8/50.8 kB 7.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mCollecting async-timeout<5.0,>=4.0 (from aiohttp->datasets<=2.21.0,>=2.16.0->-r requirements.txt (line 2))\u001b[0m\n",
      "\u001b[34mDownloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\u001b[0m\n",
      "\u001b[34mCollecting gitdb<5,>=4.0.1 (from gitpython!=3.1.29,>=1.0.0->wandb->-r requirements.txt (line 23))\u001b[0m\n",
      "\u001b[34mDownloading gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx>=0.24.1->gradio>=4.0.0->-r requirements.txt (line 6)) (2024.7.4)\u001b[0m\n",
      "\u001b[34mCollecting httpcore==1.* (from httpx>=0.24.1->gradio>=4.0.0->-r requirements.txt (line 6))\u001b[0m\n",
      "\u001b[34mDownloading httpcore-1.0.6-py3-none-any.whl.metadata (21 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers<=4.45.0,>=4.41.2->-r requirements.txt (line 1)) (3.2.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.34.0->-r requirements.txt (line 3)) (1.13.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.34.0->-r requirements.txt (line 3)) (3.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: shellingham>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from typer<1.0,>=0.12->gradio>=4.0.0->-r requirements.txt (line 6)) (1.5.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: rich>=10.11.0 in /opt/conda/lib/python3.10/site-packages (from typer<1.0,>=0.12->gradio>=4.0.0->-r requirements.txt (line 6)) (13.7.1)\u001b[0m\n",
      "\u001b[34mCollecting docstring-parser>=0.16 (from tyro>=0.5.11->trl<=0.9.6,>=0.8.6->-r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mDownloading docstring_parser-0.16-py3-none-any.whl.metadata (3.0 kB)\u001b[0m\n",
      "\u001b[34mCollecting shtab>=1.5.6 (from tyro>=0.5.11->trl<=0.9.6,>=0.8.6->-r requirements.txt (line 5))\u001b[0m\n",
      "\u001b[34mDownloading shtab-1.7.1-py3-none-any.whl.metadata (7.3 kB)\u001b[0m\n",
      "\u001b[34mCollecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb->-r requirements.txt (line 23))\u001b[0m\n",
      "\u001b[34mDownloading smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio>=4.0.0->-r requirements.txt (line 6)) (3.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio>=4.0.0->-r requirements.txt (line 6)) (2.18.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate==0.34.0->-r requirements.txt (line 3)) (1.3.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio>=4.0.0->-r requirements.txt (line 6)) (0.1.2)\u001b[0m\n",
      "\u001b[34mDownloading accelerate-0.34.0-py3-none-any.whl (324 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 324.3/324.3 kB 34.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading transformers-4.45.0-py3-none-any.whl (9.9 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 9.9/9.9 MB 97.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading datasets-2.21.0-py3-none-any.whl (527 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 527.3/527.3 kB 44.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading peft-0.12.0-py3-none-any.whl (296 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 296.4/296.4 kB 35.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading trl-0.9.6-py3-none-any.whl (245 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 245.8/245.8 kB 26.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading gradio-4.44.1-py3-none-any.whl (18.1 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 18.1/18.1 MB 68.4 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading gradio_client-1.3.0-py3-none-any.whl (318 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 318.7/318.7 kB 41.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\u001b[0m\n",
      "\u001b[34mDownloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.3/1.3 MB 76.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/1.2 MB 73.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading uvicorn-0.31.0-py3-none-any.whl (63 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 63.7/63.7 kB 11.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading fastapi-0.115.0-py3-none-any.whl (94 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 94.6/94.6 kB 16.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading sse_starlette-2.1.3-py3-none-any.whl (9.4 kB)\u001b[0m\n",
      "\u001b[34mDownloading wandb-0.18.3-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.0 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 13.0/13.0 MB 88.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl (122.4 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 122.4/122.4 MB 14.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\u001b[0m\n",
      "\u001b[34mDownloading anyio-4.6.0-py3-none-any.whl (89 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 89.6/89.6 kB 14.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\u001b[0m\n",
      "\u001b[34mDownloading aiohttp-3.10.9-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/1.2 MB 70.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading GitPython-3.1.43-py3-none-any.whl (207 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 207.3/207.3 kB 30.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading h11-0.14.0-py3-none-any.whl (58 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 58.3/58.3 kB 10.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 76.4/76.4 kB 11.8 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading httpcore-1.0.6-py3-none-any.whl (78 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 78.0/78.0 kB 13.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading huggingface_hub-0.25.1-py3-none-any.whl (436 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 436.4/436.4 kB 48.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading importlib_resources-6.4.5-py3-none-any.whl (36 kB)\u001b[0m\n",
      "\u001b[34mDownloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 141.9/141.9 kB 24.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading python_multipart-0.0.12-py3-none-any.whl (23 kB)\u001b[0m\n",
      "\u001b[34mDownloading regex-2024.9.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (782 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 782.7/782.7 kB 63.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading ruff-0.6.9-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.9 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 10.9/10.9 MB 83.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading safetensors-0.4.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (435 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 435.0/435.0 kB 47.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\u001b[0m\n",
      "\u001b[34mDownloading sentry_sdk-2.15.0-py2.py3-none-any.whl (310 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 311.0/311.0 kB 37.9 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading starlette-0.38.6-py3-none-any.whl (71 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 71.5/71.5 kB 13.2 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading tokenizers-0.20.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.9 MB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.9/2.9 MB 90.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading tyro-0.8.11-py3-none-any.whl (105 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 105.9/105.9 kB 16.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading urllib3-2.2.3-py3-none-any.whl (126 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 126.3/126.3 kB 20.5 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading ffmpy-0.4.0-py3-none-any.whl (5.8 kB)\u001b[0m\n",
      "\u001b[34mDownloading hjson-3.1.0-py3-none-any.whl (54 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 54.0/54.0 kB 9.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading nvidia_ml_py-12.560.30-py3-none-any.whl (40 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 40.5/40.5 kB 6.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\u001b[0m\n",
      "\u001b[34mDownloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\u001b[0m\n",
      "\u001b[34mDownloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\u001b[0m\n",
      "\u001b[34mDownloading termcolor-2.5.0-py3-none-any.whl (7.8 kB)\u001b[0m\n",
      "\u001b[34mDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 194.1/194.1 kB 30.3 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading aiohappyeyeballs-2.4.3-py3-none-any.whl (14 kB)\u001b[0m\n",
      "\u001b[34mDownloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\u001b[0m\n",
      "\u001b[34mDownloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\u001b[0m\n",
      "\u001b[34mDownloading docstring_parser-0.16-py3-none-any.whl (36 kB)\u001b[0m\n",
      "\u001b[34mDownloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (239 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 239.5/239.5 kB 36.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading gitdb-4.0.11-py3-none-any.whl (62 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 62.7/62.7 kB 10.6 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading multidict-6.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 124.6/124.6 kB 20.0 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading shtab-1.7.1-py3-none-any.whl (14 kB)\u001b[0m\n",
      "\u001b[34mDownloading sniffio-1.3.1-py3-none-any.whl (10 kB)\u001b[0m\n",
      "\u001b[34mDownloading websockets-12.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 130.2/130.2 kB 18.1 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading yarl-1.13.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (447 kB)\u001b[0m\n",
      "\u001b[34m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 447.9/447.9 kB 50.7 MB/s eta 0:00:00\u001b[0m\n",
      "\u001b[34mDownloading smmap-5.0.1-py3-none-any.whl (24 kB)\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: fire, deepspeed\u001b[0m\n",
      "\u001b[34mBuilding wheel for fire (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for fire (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for fire: filename=fire-0.7.0-py3-none-any.whl size=114250 sha256=12be2f228901fb95801ee8edc52dfd528cd18e8c80358323795fd9583b2c949b\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/19/39/2f/2d3cadc408a8804103f1c34ddd4b9f6a93497b11fa96fe738e\u001b[0m\n",
      "\u001b[34mBuilding wheel for deepspeed (setup.py): started\u001b[0m\n",
      "\u001b[34mBuilding wheel for deepspeed (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mCreated wheel for deepspeed: filename=deepspeed-0.15.1-py3-none-any.whl size=1483865 sha256=b72c4e32a2617bf2589381b3f2009c843d8758f5cb4e218153c00ec4893a1b75\u001b[0m\n",
      "\u001b[34mStored in directory: /root/.cache/pip/wheels/da/cb/14/9cbba50c73df044eb32a7ca29e34844c5f8959e12d22ae8b60\u001b[0m\n",
      "\u001b[34mSuccessfully built fire deepspeed\u001b[0m\n",
      "\u001b[34mInstalling collected packages: sentencepiece, pydub, py-cpuinfo, nvidia-ml-py, hjson, xxhash, websockets, urllib3, tomlkit, termcolor, sniffio, smmap, shtab, setproctitle, semantic-version, safetensors, ruff, regex, python-multipart, orjson, multidict, importlib-resources, h11, frozenlist, ffmpy, docstring-parser, docker-pycreds, async-timeout, aiohappyeyeballs, aiofiles, yarl, uvicorn, sentry-sdk, httpcore, gitdb, fire, anyio, aiosignal, tyro, tiktoken, starlette, huggingface-hub, httpx, gitpython, deepspeed, bitsandbytes, aiohttp, wandb, tokenizers, sse-starlette, gradio-client, fastapi, accelerate, transformers, gradio, datasets, trl, peft\u001b[0m\n",
      "\u001b[34mAttempting uninstall: urllib3\u001b[0m\n",
      "\u001b[34mFound existing installation: urllib3 1.26.19\u001b[0m\n",
      "\u001b[34mUninstalling urllib3-1.26.19:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled urllib3-1.26.19\u001b[0m\n",
      "\u001b[34mAttempting uninstall: accelerate\u001b[0m\n",
      "\u001b[34mFound existing installation: accelerate 0.22.0\u001b[0m\n",
      "\u001b[34mUninstalling accelerate-0.22.0:\u001b[0m\n",
      "\u001b[34mSuccessfully uninstalled accelerate-0.22.0\u001b[0m\n",
      "\u001b[34mSuccessfully installed accelerate-0.34.0 aiofiles-23.2.1 aiohappyeyeballs-2.4.3 aiohttp-3.10.9 aiosignal-1.3.1 anyio-4.6.0 async-timeout-4.0.3 bitsandbytes-0.44.1 datasets-2.21.0 deepspeed-0.15.1 docker-pycreds-0.4.0 docstring-parser-0.16 fastapi-0.115.0 ffmpy-0.4.0 fire-0.7.0 frozenlist-1.4.1 gitdb-4.0.11 gitpython-3.1.43 gradio-4.44.1 gradio-client-1.3.0 h11-0.14.0 hjson-3.1.0 httpcore-1.0.6 httpx-0.27.2 huggingface-hub-0.25.1 importlib-resources-6.4.5 multidict-6.1.0 nvidia-ml-py-12.560.30 orjson-3.10.7 peft-0.12.0 py-cpuinfo-9.0.0 pydub-0.25.1 python-multipart-0.0.12 regex-2024.9.11 ruff-0.6.9 safetensors-0.4.5 semantic-version-2.10.0 sentencepiece-0.2.0 sentry-sdk-2.15.0 setproctitle-1.3.3 shtab-1.7.1 smmap-5.0.1 sniffio-1.3.1 sse-starlette-2.1.3 starlette-0.38.6 termcolor-2.5.0 tiktoken-0.8.0 tokenizers-0.20.0 tomlkit-0.12.0 transformers-4.45.0 trl-0.9.6 tyro-0.8.11 urllib3-2.2.3 uvicorn-0.31.0 wandb-0.18.3 websockets-12.0 xxhash-3.5.0 yarl-1.13.1\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip is available: 24.1.2 -> 24.2\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m2024-10-08 09:19:46,401 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2024-10-08 09:19:46,401 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2024-10-08 09:19:46,543 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-10-08 09:19:46,670 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-10-08 09:19:46,792 sagemaker-training-toolkit INFO     No Neurons detected (normal if no neurons installed)\u001b[0m\n",
      "\u001b[34m2024-10-08 09:19:46,802 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {},\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"current_instance_group\": \"homogeneousCluster\",\n",
      "    \"current_instance_group_hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"current_instance_type\": \"ml.p4d.24xlarge\",\n",
      "    \"distribution_hosts\": [],\n",
      "    \"distribution_instance_groups\": [],\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {},\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {},\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"instance_groups\": [\n",
      "        \"homogeneousCluster\"\n",
      "    ],\n",
      "    \"instance_groups_dict\": {\n",
      "        \"homogeneousCluster\": {\n",
      "            \"instance_group_name\": \"homogeneousCluster\",\n",
      "            \"instance_type\": \"ml.p4d.24xlarge\",\n",
      "            \"hosts\": [\n",
      "                \"algo-1\"\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"is_hetero\": false,\n",
      "    \"is_master\": true,\n",
      "    \"is_modelparallel_enabled\": null,\n",
      "    \"is_smddpmprun_installed\": false,\n",
      "    \"is_smddprun_installed\": true,\n",
      "    \"job_name\": \"Qwen25-7B-it-finetune-2024-10-08-09-08-43-528\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-596899493901/Qwen25-7B-it-finetune-2024-10-08-09-08-43-528/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"entry\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 96,\n",
      "    \"num_gpus\": 8,\n",
      "    \"num_neurons\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.p4d.24xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.p4d.24xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"entry.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=entry.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_TYPE=ml.p4d.24xlarge\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP=homogeneousCluster\u001b[0m\n",
      "\u001b[34mSM_CURRENT_INSTANCE_GROUP_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS=[\"homogeneousCluster\"]\u001b[0m\n",
      "\u001b[34mSM_INSTANCE_GROUPS_DICT={\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}}\u001b[0m\n",
      "\u001b[34mSM_DISTRIBUTION_INSTANCE_GROUPS=[]\u001b[0m\n",
      "\u001b[34mSM_IS_HETERO=false\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=entry\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=96\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_NEURONS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-596899493901/Qwen25-7B-it-finetune-2024-10-08-09-08-43-528/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{},\"current_host\":\"algo-1\",\"current_instance_group\":\"homogeneousCluster\",\"current_instance_group_hosts\":[\"algo-1\"],\"current_instance_type\":\"ml.p4d.24xlarge\",\"distribution_hosts\":[],\"distribution_instance_groups\":[],\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"instance_groups\":[\"homogeneousCluster\"],\"instance_groups_dict\":{\"homogeneousCluster\":{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}},\"is_hetero\":false,\"is_master\":true,\"is_modelparallel_enabled\":null,\"is_smddpmprun_installed\":false,\"is_smddprun_installed\":true,\"job_name\":\"Qwen25-7B-it-finetune-2024-10-08-09-08-43-528\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-596899493901/Qwen25-7B-it-finetune-2024-10-08-09-08-43-528/source/sourcedir.tar.gz\",\"module_name\":\"entry\",\"network_interface_name\":\"eth0\",\"num_cpus\":96,\"num_gpus\":8,\"num_neurons\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.p4d.24xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.p4d.24xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"entry.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python310.zip:/opt/conda/lib/python3.10:/opt/conda/lib/python3.10/lib-dynload:/opt/conda/lib/python3.10/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.10 entry.py\u001b[0m\n",
      "\u001b[34m2024-10-08 09:19:46,803 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker Debugger as it is not installed.\u001b[0m\n",
      "\u001b[34m2024-10-08 09:19:46,803 sagemaker-training-toolkit INFO     Exceptions not imported for SageMaker TF as Tensorflow is not installed.\u001b[0m\n",
      "\u001b[34m*****************start cp pretrain model*****************************\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-596899493901/Foundation-Models/Qwen2.5_7B_it/.gitattributes /tmp/pretrain_model/.gitattributes\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-596899493901/Foundation-Models/Qwen2.5_7B_it/LICENSE /tmp/pretrain_model/LICENSE\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-596899493901/Foundation-Models/Qwen2.5_7B_it/README.md /tmp/pretrain_model/README.md\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-596899493901/Foundation-Models/Qwen2.5_7B_it/generation_config.json /tmp/pretrain_model/generation_config.json\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-596899493901/Foundation-Models/Qwen2.5_7B_it/tokenizer_config.json /tmp/pretrain_model/tokenizer_config.json\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-596899493901/Foundation-Models/Qwen2.5_7B_it/model.safetensors.index.json /tmp/pretrain_model/model.safetensors.index.json\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-596899493901/Foundation-Models/Qwen2.5_7B_it/config.json /tmp/pretrain_model/config.json\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-596899493901/Foundation-Models/Qwen2.5_7B_it/merges.txt /tmp/pretrain_model/merges.txt\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-596899493901/Foundation-Models/Qwen2.5_7B_it/tokenizer.json /tmp/pretrain_model/tokenizer.json\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-596899493901/Foundation-Models/Qwen2.5_7B_it/vocab.json /tmp/pretrain_model/vocab.json\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-596899493901/Foundation-Models/Qwen2.5_7B_it/model-00004-of-00004.safetensors /tmp/pretrain_model/model-00004-of-00004.safetensors\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-596899493901/Foundation-Models/Qwen2.5_7B_it/model-00002-of-00004.safetensors /tmp/pretrain_model/model-00002-of-00004.safetensors\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-596899493901/Foundation-Models/Qwen2.5_7B_it/model-00003-of-00004.safetensors /tmp/pretrain_model/model-00003-of-00004.safetensors\u001b[0m\n",
      "\u001b[34mcp s3://sagemaker-us-east-1-596899493901/Foundation-Models/Qwen2.5_7B_it/model-00001-of-00004.safetensors /tmp/pretrain_model/model-00001-of-00004.safetensors\u001b[0m\n",
      "\u001b[34m-----finished cp-------\u001b[0m\n",
      "\u001b[34m[2024-10-08 09:20:36,660] torch.distributed.run: [WARNING] \u001b[0m\n",
      "\u001b[34m[2024-10-08 09:20:36,660] torch.distributed.run: [WARNING] *****************************************\u001b[0m\n",
      "\u001b[34m[2024-10-08 09:20:36,660] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \u001b[0m\n",
      "\u001b[34m[2024-10-08 09:20:36,660] torch.distributed.run: [WARNING] *****************************************\u001b[0m\n",
      "\u001b[34m[2024-10-08 09:20:53,188] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[2024-10-08 09:20:53,310] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[2024-10-08 09:20:53,313] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34mdf: /root/.triton/autotune: No such file or directory\u001b[0m\n",
      "\u001b[34m[2024-10-08 09:20:53,364] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[2024-10-08 09:20:53,408] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[2024-10-08 09:20:53,416] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[2024-10-08 09:20:53,421] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[2024-10-08 09:20:53,458] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\u001b[0m\n",
      "\u001b[34m[2024-10-08 09:20:54,479] [INFO] [comm.py:652:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34m[2024-10-08 09:20:54,596] [INFO] [comm.py:652:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34m[2024-10-08 09:20:54,598] [INFO] [comm.py:652:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34m[2024-10-08 09:20:54,705] [INFO] [comm.py:652:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34m[2024-10-08 09:20:54,705] [INFO] [comm.py:652:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34m[2024-10-08 09:20:54,727] [INFO] [comm.py:652:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34m[2024-10-08 09:20:54,777] [INFO] [comm.py:652:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34m[2024-10-08 09:20:54,777] [INFO] [comm.py:683:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\u001b[0m\n",
      "\u001b[34m[2024-10-08 09:20:54,777] [INFO] [comm.py:652:init_distributed] cdb=None\u001b[0m\n",
      "\u001b[34m10/08/2024 09:20:55 - INFO - llamafactory.hparams.parser - Process rank: 1, device: cuda:1, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16\u001b[0m\n",
      "\u001b[34m10/08/2024 09:20:55 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:670] 2024-10-08 09:20:55,587 >> loading configuration file /tmp/pretrain_model/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:670] 2024-10-08 09:20:55,587 >> loading configuration file /tmp/pretrain_model/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:739] 2024-10-08 09:20:55,589 >> Model config Qwen2Config {\n",
      "  \"_name_or_path\": \"/tmp/pretrain_model\",\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3584,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 18944,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 28,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 4,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.45.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 152064\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:739] 2024-10-08 09:20:55,589 >> Model config Qwen2Config {\n",
      "  \"_name_or_path\": \"/tmp/pretrain_model\",\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3584,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 18944,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 28,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 4,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.45.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 152064\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2212] 2024-10-08 09:20:55,597 >> loading file vocab.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2212] 2024-10-08 09:20:55,597 >> loading file vocab.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2212] 2024-10-08 09:20:55,597 >> loading file merges.txt\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2212] 2024-10-08 09:20:55,597 >> loading file tokenizer.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2212] 2024-10-08 09:20:55,597 >> loading file added_tokens.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2212] 2024-10-08 09:20:55,597 >> loading file merges.txt\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2212] 2024-10-08 09:20:55,597 >> loading file tokenizer.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2212] 2024-10-08 09:20:55,597 >> loading file added_tokens.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2212] 2024-10-08 09:20:55,597 >> loading file special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2212] 2024-10-08 09:20:55,597 >> loading file tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2212] 2024-10-08 09:20:55,597 >> loading file special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2212] 2024-10-08 09:20:55,597 >> loading file tokenizer_config.json\u001b[0m\n",
      "\u001b[34m10/08/2024 09:20:55 - INFO - llamafactory.hparams.parser - Process rank: 3, device: cuda:3, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16\u001b[0m\n",
      "\u001b[34m10/08/2024 09:20:55 - INFO - llamafactory.hparams.parser - Process rank: 5, device: cuda:5, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16\u001b[0m\n",
      "\u001b[34m10/08/2024 09:20:55 - INFO - llamafactory.hparams.parser - Process rank: 4, device: cuda:4, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16\u001b[0m\n",
      "\u001b[34m10/08/2024 09:20:55 - INFO - llamafactory.hparams.parser - Process rank: 6, device: cuda:6, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16\u001b[0m\n",
      "\u001b[34m10/08/2024 09:20:55 - INFO - llamafactory.hparams.parser - Process rank: 7, device: cuda:7, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16\u001b[0m\n",
      "\u001b[34m10/08/2024 09:20:55 - INFO - llamafactory.hparams.parser - Process rank: 2, device: cuda:2, n_gpu: 1, distributed training: True, compute dtype: torch.bfloat16\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2478] 2024-10-08 09:20:55,934 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2478] 2024-10-08 09:20:55,934 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:670] 2024-10-08 09:20:55,935 >> loading configuration file /tmp/pretrain_model/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:670] 2024-10-08 09:20:55,935 >> loading configuration file /tmp/pretrain_model/config.json\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:739] 2024-10-08 09:20:55,935 >> Model config Qwen2Config {\n",
      "  \"_name_or_path\": \"/tmp/pretrain_model\",\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3584,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 18944,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 28,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 4,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.45.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 152064\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:739] 2024-10-08 09:20:55,935 >> Model config Qwen2Config {\n",
      "  \"_name_or_path\": \"/tmp/pretrain_model\",\n",
      "  \"architectures\": [\n",
      "    \"Qwen2ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3584,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 18944,\n",
      "  \"max_position_embeddings\": 32768,\n",
      "  \"max_window_layers\": 28,\n",
      "  \"model_type\": \"qwen2\",\n",
      "  \"num_attention_heads\": 28,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 4,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000.0,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.45.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 152064\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2212] 2024-10-08 09:20:55,936 >> loading file vocab.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2212] 2024-10-08 09:20:55,936 >> loading file merges.txt\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2212] 2024-10-08 09:20:55,936 >> loading file tokenizer.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2212] 2024-10-08 09:20:55,936 >> loading file added_tokens.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2212] 2024-10-08 09:20:55,936 >> loading file special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2212] 2024-10-08 09:20:55,936 >> loading file tokenizer_config.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2212] 2024-10-08 09:20:55,936 >> loading file vocab.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2212] 2024-10-08 09:20:55,936 >> loading file merges.txt\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2212] 2024-10-08 09:20:55,936 >> loading file tokenizer.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2212] 2024-10-08 09:20:55,936 >> loading file added_tokens.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2212] 2024-10-08 09:20:55,936 >> loading file special_tokens_map.json\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2212] 2024-10-08 09:20:55,936 >> loading file tokenizer_config.json\u001b[0m\n",
      "\u001b[34m10/08/2024 09:20:56 - INFO - llamafactory.data.template - Replace eos token: <|im_end|>\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2478] 2024-10-08 09:20:56,279 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:2478] 2024-10-08 09:20:56,279 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\u001b[0m\n",
      "\u001b[34m10/08/2024 09:20:56 - INFO - llamafactory.data.template - Replace eos token: <|im_end|>\u001b[0m\n",
      "\u001b[34m10/08/2024 09:20:56 - INFO - llamafactory.data.template - Replace eos token: <|im_end|>\u001b[0m\n",
      "\u001b[34m10/08/2024 09:20:56 - INFO - llamafactory.data.template - Replace eos token: <|im_end|>\u001b[0m\n",
      "\u001b[34m10/08/2024 09:20:56 - INFO - llamafactory.data.template - Replace eos token: <|im_end|>\u001b[0m\n",
      "\u001b[34malgo-1:220:220 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0\u001b[0m\n",
      "\u001b[34malgo-1:220:220 [0] NCCL INFO Bootstrap : Using eth0:10.0.213.203<0>\u001b[0m\n",
      "\u001b[34malgo-1:220:220 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.\u001b[0m\n",
      "\u001b[34malgo-1:220:220 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).\u001b[0m\n",
      "\u001b[34malgo-1:220:220 [0] NCCL INFO cudaDriverVersion 12040\u001b[0m\n",
      "\u001b[34m10/08/2024 09:20:56 - INFO - llamafactory.data.template - Replace eos token: <|im_end|>\u001b[0m\n",
      "\u001b[34mNCCL version 2.18.5+cuda12.1\u001b[0m\n",
      "\u001b[34malgo-1:221:221 [1] NCCL INFO cudaDriverVersion 12040\u001b[0m\n",
      "\u001b[34malgo-1:223:223 [3] NCCL INFO cudaDriverVersion 12040\u001b[0m\n",
      "\u001b[34malgo-1:224:224 [4] NCCL INFO cudaDriverVersion 12040\u001b[0m\n",
      "\u001b[34malgo-1:225:225 [5] NCCL INFO cudaDriverVersion 12040\u001b[0m\n",
      "\u001b[34malgo-1:223:223 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0\u001b[0m\n",
      "\u001b[34malgo-1:225:225 [5] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0\u001b[0m\n",
      "\u001b[34malgo-1:224:224 [4] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0\u001b[0m\n",
      "\u001b[34malgo-1:221:221 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0\u001b[0m\n",
      "\u001b[34malgo-1:223:223 [3] NCCL INFO Bootstrap : Using eth0:10.0.213.203<0>\u001b[0m\n",
      "\u001b[34malgo-1:225:225 [5] NCCL INFO Bootstrap : Using eth0:10.0.213.203<0>\u001b[0m\n",
      "\u001b[34malgo-1:224:224 [4] NCCL INFO Bootstrap : Using eth0:10.0.213.203<0>\u001b[0m\n",
      "\u001b[34malgo-1:221:221 [1] NCCL INFO Bootstrap : Using eth0:10.0.213.203<0>\u001b[0m\n",
      "\u001b[34malgo-1:227:227 [7] NCCL INFO cudaDriverVersion 12040\u001b[0m\n",
      "\u001b[34malgo-1:227:227 [7] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0\u001b[0m\n",
      "\u001b[34malgo-1:227:227 [7] NCCL INFO Bootstrap : Using eth0:10.0.213.203<0>\u001b[0m\n",
      "\u001b[34malgo-1:227:227 [7] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.\u001b[0m\n",
      "\u001b[34malgo-1:227:227 [7] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).\u001b[0m\n",
      "\u001b[34malgo-1:221:221 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.\u001b[0m\n",
      "\u001b[34malgo-1:224:224 [4] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.\u001b[0m\n",
      "\u001b[34malgo-1:221:221 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).\u001b[0m\n",
      "\u001b[34malgo-1:224:224 [4] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).\u001b[0m\n",
      "\u001b[34malgo-1:225:225 [5] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.\u001b[0m\n",
      "\u001b[34malgo-1:223:223 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.\u001b[0m\n",
      "\u001b[34malgo-1:225:225 [5] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).\u001b[0m\n",
      "\u001b[34malgo-1:223:223 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).\u001b[0m\n",
      "\u001b[34m10/08/2024 09:20:56 - INFO - llamafactory.data.template - Replace eos token: <|im_end|>\u001b[0m\n",
      "\u001b[34malgo-1:222:222 [2] NCCL INFO cudaDriverVersion 12040\u001b[0m\n",
      "\u001b[34malgo-1:222:222 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0\u001b[0m\n",
      "\u001b[34malgo-1:222:222 [2] NCCL INFO Bootstrap : Using eth0:10.0.213.203<0>\u001b[0m\n",
      "\u001b[34malgo-1:222:222 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.\u001b[0m\n",
      "\u001b[34malgo-1:222:222 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).\u001b[0m\n",
      "\u001b[34m10/08/2024 09:20:56 - INFO - llamafactory.data.template - Replace eos token: <|im_end|>\u001b[0m\n",
      "\u001b[34malgo-1:226:226 [6] NCCL INFO cudaDriverVersion 12040\u001b[0m\n",
      "\u001b[34malgo-1:226:226 [6] NCCL INFO NCCL_SOCKET_IFNAME set by environment to eth0\u001b[0m\n",
      "\u001b[34malgo-1:226:226 [6] NCCL INFO Bootstrap : Using eth0:10.0.213.203<0>\u001b[0m\n",
      "\u001b[34malgo-1:226:226 [6] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.\u001b[0m\n",
      "\u001b[34malgo-1:226:226 [6] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin symbol (v4 or v5).\u001b[0m\n",
      "\u001b[34malgo-1:220:547 [0] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.9.1-aws\u001b[0m\n",
      "\u001b[34malgo-1:220:547 [0] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.9.1-aws\u001b[0m\n",
      "\u001b[34malgo-1:220:547 [0] NCCL INFO NET/OFI Using Libfabric version 1.19\u001b[0m\n",
      "\u001b[34malgo-1:220:547 [0] NCCL INFO NET/OFI Using CUDA driver version 12040\u001b[0m\n",
      "\u001b[34malgo-1:220:547 [0] NCCL INFO NET/OFI Configuring AWS-specific options\u001b[0m\n",
      "\u001b[34malgo-1:220:547 [0] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34malgo-1:220:547 [0] NCCL INFO NET/OFI Setting NCCL_NVLSTREE_MAX_CHUNKSIZE to 512KiB\u001b[0m\n",
      "\u001b[34malgo-1:220:547 [0] NCCL INFO NET/OFI Setting NCCL_NVLS_CHUNKSIZE to 512KiB\u001b[0m\n",
      "\u001b[34malgo-1:220:547 [0] NCCL INFO NET/OFI Running on p4d.24xlarge platform, Setting NCCL_TOPO_FILE environment variable to /opt/conda/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[34malgo-1:220:547 [0] NCCL INFO NET/OFI Internode latency set at 75.0 us\u001b[0m\n",
      "\u001b[34malgo-1:220:547 [0] NCCL INFO NET/OFI Using transport protocol SENDRECV\u001b[0m\n",
      "\u001b[34malgo-1:220:547 [0] NCCL INFO NET/OFI Selected Provider is efa (found 4 nics)\u001b[0m\n",
      "\u001b[34malgo-1:221:552 [1] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.9.1-aws\u001b[0m\n",
      "\u001b[34malgo-1:221:552 [1] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.9.1-aws\u001b[0m\n",
      "\u001b[34malgo-1:221:552 [1] NCCL INFO NET/OFI Using Libfabric version 1.19\u001b[0m\n",
      "\u001b[34malgo-1:221:552 [1] NCCL INFO NET/OFI Using CUDA driver version 12040\u001b[0m\n",
      "\u001b[34malgo-1:221:552 [1] NCCL INFO NET/OFI Configuring AWS-specific options\u001b[0m\n",
      "\u001b[34malgo-1:221:552 [1] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34malgo-1:221:552 [1] NCCL INFO NET/OFI Setting NCCL_NVLSTREE_MAX_CHUNKSIZE to 512KiB\u001b[0m\n",
      "\u001b[34malgo-1:221:552 [1] NCCL INFO NET/OFI Setting NCCL_NVLS_CHUNKSIZE to 512KiB\u001b[0m\n",
      "\u001b[34malgo-1:221:552 [1] NCCL INFO NET/OFI Running on p4d.24xlarge platform, Setting NCCL_TOPO_FILE environment variable to /opt/conda/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[34malgo-1:221:552 [1] NCCL INFO NET/OFI Internode latency set at 75.0 us\u001b[0m\n",
      "\u001b[34malgo-1:221:552 [1] NCCL INFO NET/OFI Using transport protocol SENDRECV\u001b[0m\n",
      "\u001b[34malgo-1:223:551 [3] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.9.1-aws\u001b[0m\n",
      "\u001b[34malgo-1:223:551 [3] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.9.1-aws\u001b[0m\n",
      "\u001b[34malgo-1:223:551 [3] NCCL INFO NET/OFI Using Libfabric version 1.19\u001b[0m\n",
      "\u001b[34malgo-1:223:551 [3] NCCL INFO NET/OFI Using CUDA driver version 12040\u001b[0m\n",
      "\u001b[34malgo-1:223:551 [3] NCCL INFO NET/OFI Configuring AWS-specific options\u001b[0m\n",
      "\u001b[34malgo-1:223:551 [3] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34malgo-1:223:551 [3] NCCL INFO NET/OFI Setting NCCL_NVLSTREE_MAX_CHUNKSIZE to 512KiB\u001b[0m\n",
      "\u001b[34malgo-1:223:551 [3] NCCL INFO NET/OFI Setting NCCL_NVLS_CHUNKSIZE to 512KiB\u001b[0m\n",
      "\u001b[34malgo-1:223:551 [3] NCCL INFO NET/OFI Running on p4d.24xlarge platform, Setting NCCL_TOPO_FILE environment variable to /opt/conda/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[34malgo-1:223:551 [3] NCCL INFO NET/OFI Internode latency set at 75.0 us\u001b[0m\n",
      "\u001b[34malgo-1:223:551 [3] NCCL INFO NET/OFI Using transport protocol SENDRECV\u001b[0m\n",
      "\u001b[34malgo-1:225:549 [5] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.9.1-aws\u001b[0m\n",
      "\u001b[34malgo-1:225:549 [5] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.9.1-aws\u001b[0m\n",
      "\u001b[34malgo-1:225:549 [5] NCCL INFO NET/OFI Using Libfabric version 1.19\u001b[0m\n",
      "\u001b[34malgo-1:225:549 [5] NCCL INFO NET/OFI Using CUDA driver version 12040\u001b[0m\n",
      "\u001b[34malgo-1:225:549 [5] NCCL INFO NET/OFI Configuring AWS-specific options\u001b[0m\n",
      "\u001b[34malgo-1:225:549 [5] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34malgo-1:225:549 [5] NCCL INFO NET/OFI Setting NCCL_NVLSTREE_MAX_CHUNKSIZE to 512KiB\u001b[0m\n",
      "\u001b[34malgo-1:225:549 [5] NCCL INFO NET/OFI Setting NCCL_NVLS_CHUNKSIZE to 512KiB\u001b[0m\n",
      "\u001b[34malgo-1:225:549 [5] NCCL INFO NET/OFI Running on p4d.24xlarge platform, Setting NCCL_TOPO_FILE environment variable to /opt/conda/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[34malgo-1:225:549 [5] NCCL INFO NET/OFI Internode latency set at 75.0 us\u001b[0m\n",
      "\u001b[34malgo-1:225:549 [5] NCCL INFO NET/OFI Using transport protocol SENDRECV\u001b[0m\n",
      "\u001b[34malgo-1:220:547 [0] NCCL INFO NET/OFI Setting FI_OPT_EFA_SENDRECV_IN_ORDER_ALIGNED_128_BYTES not supported.\u001b[0m\n",
      "\u001b[34malgo-1:222:553 [2] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.9.1-aws\u001b[0m\n",
      "\u001b[34malgo-1:222:553 [2] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.9.1-aws\u001b[0m\n",
      "\u001b[34malgo-1:222:553 [2] NCCL INFO NET/OFI Using Libfabric version 1.19\u001b[0m\n",
      "\u001b[34malgo-1:222:553 [2] NCCL INFO NET/OFI Using CUDA driver version 12040\u001b[0m\n",
      "\u001b[34malgo-1:222:553 [2] NCCL INFO NET/OFI Configuring AWS-specific options\u001b[0m\n",
      "\u001b[34malgo-1:222:553 [2] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34malgo-1:222:553 [2] NCCL INFO NET/OFI Setting NCCL_NVLSTREE_MAX_CHUNKSIZE to 512KiB\u001b[0m\n",
      "\u001b[34malgo-1:222:553 [2] NCCL INFO NET/OFI Setting NCCL_NVLS_CHUNKSIZE to 512KiB\u001b[0m\n",
      "\u001b[34malgo-1:222:553 [2] NCCL INFO NET/OFI Running on p4d.24xlarge platform, Setting NCCL_TOPO_FILE environment variable to /opt/conda/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[34malgo-1:222:553 [2] NCCL INFO NET/OFI Internode latency set at 75.0 us\u001b[0m\n",
      "\u001b[34malgo-1:222:553 [2] NCCL INFO NET/OFI Using transport protocol SENDRECV\u001b[0m\n",
      "\u001b[34malgo-1:221:552 [1] NCCL INFO NET/OFI Selected Provider is efa (found 4 nics)\u001b[0m\n",
      "\u001b[34malgo-1:223:551 [3] NCCL INFO NET/OFI Selected Provider is efa (found 4 nics)\u001b[0m\n",
      "\u001b[34malgo-1:225:549 [5] NCCL INFO NET/OFI Selected Provider is efa (found 4 nics)\u001b[0m\n",
      "\u001b[34malgo-1:224:550 [4] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.9.1-aws\u001b[0m\n",
      "\u001b[34malgo-1:224:550 [4] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.9.1-aws\u001b[0m\n",
      "\u001b[34malgo-1:224:550 [4] NCCL INFO NET/OFI Using Libfabric version 1.19\u001b[0m\n",
      "\u001b[34malgo-1:224:550 [4] NCCL INFO NET/OFI Using CUDA driver version 12040\u001b[0m\n",
      "\u001b[34malgo-1:224:550 [4] NCCL INFO NET/OFI Configuring AWS-specific options\u001b[0m\n",
      "\u001b[34malgo-1:224:550 [4] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34malgo-1:224:550 [4] NCCL INFO NET/OFI Setting NCCL_NVLSTREE_MAX_CHUNKSIZE to 512KiB\u001b[0m\n",
      "\u001b[34malgo-1:224:550 [4] NCCL INFO NET/OFI Setting NCCL_NVLS_CHUNKSIZE to 512KiB\u001b[0m\n",
      "\u001b[34malgo-1:224:550 [4] NCCL INFO NET/OFI Running on p4d.24xlarge platform, Setting NCCL_TOPO_FILE environment variable to /opt/conda/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[34malgo-1:224:550 [4] NCCL INFO NET/OFI Internode latency set at 75.0 us\u001b[0m\n",
      "\u001b[34malgo-1:224:550 [4] NCCL INFO NET/OFI Using transport protocol SENDRECV\u001b[0m\n",
      "\u001b[34malgo-1:227:548 [7] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.9.1-aws\u001b[0m\n",
      "\u001b[34malgo-1:227:548 [7] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.9.1-aws\u001b[0m\n",
      "\u001b[34malgo-1:227:548 [7] NCCL INFO NET/OFI Using Libfabric version 1.19\u001b[0m\n",
      "\u001b[34malgo-1:227:548 [7] NCCL INFO NET/OFI Using CUDA driver version 12040\u001b[0m\n",
      "\u001b[34malgo-1:227:548 [7] NCCL INFO NET/OFI Configuring AWS-specific options\u001b[0m\n",
      "\u001b[34malgo-1:227:548 [7] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34malgo-1:227:548 [7] NCCL INFO NET/OFI Setting NCCL_NVLSTREE_MAX_CHUNKSIZE to 512KiB\u001b[0m\n",
      "\u001b[34malgo-1:227:548 [7] NCCL INFO NET/OFI Setting NCCL_NVLS_CHUNKSIZE to 512KiB\u001b[0m\n",
      "\u001b[34malgo-1:227:548 [7] NCCL INFO NET/OFI Running on p4d.24xlarge platform, Setting NCCL_TOPO_FILE environment variable to /opt/conda/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[34malgo-1:227:548 [7] NCCL INFO NET/OFI Internode latency set at 75.0 us\u001b[0m\n",
      "\u001b[34malgo-1:227:548 [7] NCCL INFO NET/OFI Using transport protocol SENDRECV\u001b[0m\n",
      "\u001b[34malgo-1:222:553 [2] NCCL INFO NET/OFI Selected Provider is efa (found 4 nics)\u001b[0m\n",
      "\u001b[34malgo-1:224:550 [4] NCCL INFO NET/OFI Selected Provider is efa (found 4 nics)\u001b[0m\n",
      "\u001b[34malgo-1:221:552 [1] NCCL INFO NET/OFI Setting FI_OPT_EFA_SENDRECV_IN_ORDER_ALIGNED_128_BYTES not supported.\u001b[0m\n",
      "\u001b[34malgo-1:227:548 [7] NCCL INFO NET/OFI Selected Provider is efa (found 4 nics)\u001b[0m\n",
      "\u001b[34malgo-1:223:551 [3] NCCL INFO NET/OFI Setting FI_OPT_EFA_SENDRECV_IN_ORDER_ALIGNED_128_BYTES not supported.\u001b[0m\n",
      "\u001b[34malgo-1:220:547 [0] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34malgo-1:226:554 [6] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.9.1-aws\u001b[0m\n",
      "\u001b[34malgo-1:226:554 [6] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.9.1-aws\u001b[0m\n",
      "\u001b[34malgo-1:226:554 [6] NCCL INFO NET/OFI Using Libfabric version 1.19\u001b[0m\n",
      "\u001b[34malgo-1:225:549 [5] NCCL INFO NET/OFI Setting FI_OPT_EFA_SENDRECV_IN_ORDER_ALIGNED_128_BYTES not supported.\u001b[0m\n",
      "\u001b[34malgo-1:226:554 [6] NCCL INFO NET/OFI Using CUDA driver version 12040\u001b[0m\n",
      "\u001b[34malgo-1:226:554 [6] NCCL INFO NET/OFI Configuring AWS-specific options\u001b[0m\n",
      "\u001b[34malgo-1:226:554 [6] NCCL INFO NET/OFI Setting FI_EFA_FORK_SAFE environment variable to 1\u001b[0m\n",
      "\u001b[34malgo-1:226:554 [6] NCCL INFO NET/OFI Setting NCCL_NVLSTREE_MAX_CHUNKSIZE to 512KiB\u001b[0m\n",
      "\u001b[34malgo-1:226:554 [6] NCCL INFO NET/OFI Setting NCCL_NVLS_CHUNKSIZE to 512KiB\u001b[0m\n",
      "\u001b[34malgo-1:226:554 [6] NCCL INFO NET/OFI Running on p4d.24xlarge platform, Setting NCCL_TOPO_FILE environment variable to /opt/conda/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[34malgo-1:226:554 [6] NCCL INFO NET/OFI Internode latency set at 75.0 us\u001b[0m\n",
      "\u001b[34malgo-1:226:554 [6] NCCL INFO NET/OFI Using transport protocol SENDRECV\u001b[0m\n",
      "\u001b[34malgo-1:222:553 [2] NCCL INFO NET/OFI Setting FI_OPT_EFA_SENDRECV_IN_ORDER_ALIGNED_128_BYTES not supported.\u001b[0m\n",
      "\u001b[34malgo-1:226:554 [6] NCCL INFO NET/OFI Selected Provider is efa (found 4 nics)\u001b[0m\n",
      "\u001b[34malgo-1:223:551 [3] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34malgo-1:221:552 [1] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34malgo-1:224:550 [4] NCCL INFO NET/OFI Setting FI_OPT_EFA_SENDRECV_IN_ORDER_ALIGNED_128_BYTES not supported.\u001b[0m\n",
      "\u001b[34malgo-1:225:549 [5] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34malgo-1:227:548 [7] NCCL INFO NET/OFI Setting FI_OPT_EFA_SENDRECV_IN_ORDER_ALIGNED_128_BYTES not supported.\u001b[0m\n",
      "\u001b[34malgo-1:222:553 [2] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34malgo-1:224:550 [4] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34malgo-1:227:548 [7] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34malgo-1:226:554 [6] NCCL INFO NET/OFI Setting FI_OPT_EFA_SENDRECV_IN_ORDER_ALIGNED_128_BYTES not supported.\u001b[0m\n",
      "\u001b[34malgo-1:226:554 [6] NCCL INFO Using network AWS Libfabric\u001b[0m\n",
      "\u001b[34malgo-1:220:547 [0] NCCL INFO DMA-BUF is available on GPU device 0\u001b[0m\n",
      "\u001b[34malgo-1:223:551 [3] NCCL INFO DMA-BUF is available on GPU device 3\u001b[0m\n",
      "\u001b[34malgo-1:221:552 [1] NCCL INFO DMA-BUF is available on GPU device 1\u001b[0m\n",
      "\u001b[34malgo-1:225:549 [5] NCCL INFO DMA-BUF is available on GPU device 5\u001b[0m\n",
      "\u001b[34malgo-1:222:553 [2] NCCL INFO DMA-BUF is available on GPU device 2\u001b[0m\n",
      "\u001b[34malgo-1:224:550 [4] NCCL INFO DMA-BUF is available on GPU device 4\u001b[0m\n",
      "\u001b[34malgo-1:227:548 [7] NCCL INFO DMA-BUF is available on GPU device 7\u001b[0m\n",
      "\u001b[34malgo-1:226:554 [6] NCCL INFO DMA-BUF is available on GPU device 6\u001b[0m\n",
      "\u001b[34malgo-1:221:552 [1] NCCL INFO comm 0x55fd5268aa80 rank 1 nranks 8 cudaDev 1 nvmlDev 1 busId 101d0 commId 0xee7b35dfe14976ce - Init START\u001b[0m\n",
      "\u001b[34malgo-1:220:547 [0] NCCL INFO comm 0x56235546d1d0 rank 0 nranks 8 cudaDev 0 nvmlDev 0 busId 101c0 commId 0xee7b35dfe14976ce - Init START\u001b[0m\n",
      "\u001b[34malgo-1:227:548 [7] NCCL INFO comm 0x56276a4dc730 rank 7 nranks 8 cudaDev 7 nvmlDev 7 busId a01d0 commId 0xee7b35dfe14976ce - Init START\u001b[0m\n",
      "\u001b[34malgo-1:226:554 [6] NCCL INFO comm 0x5578d34ecac0 rank 6 nranks 8 cudaDev 6 nvmlDev 6 busId a01c0 commId 0xee7b35dfe14976ce - Init START\u001b[0m\n",
      "\u001b[34malgo-1:225:549 [5] NCCL INFO comm 0x55d5267db8f0 rank 5 nranks 8 cudaDev 5 nvmlDev 5 busId 901d0 commId 0xee7b35dfe14976ce - Init START\u001b[0m\n",
      "\u001b[34malgo-1:224:550 [4] NCCL INFO comm 0x55f80ff36b30 rank 4 nranks 8 cudaDev 4 nvmlDev 4 busId 901c0 commId 0xee7b35dfe14976ce - Init START\u001b[0m\n",
      "\u001b[34malgo-1:222:553 [2] NCCL INFO comm 0x5583ca69f830 rank 2 nranks 8 cudaDev 2 nvmlDev 2 busId 201c0 commId 0xee7b35dfe14976ce - Init START\u001b[0m\n",
      "\u001b[34malgo-1:223:551 [3] NCCL INFO comm 0x5616a8c4b2f0 rank 3 nranks 8 cudaDev 3 nvmlDev 3 busId 201d0 commId 0xee7b35dfe14976ce - Init START\u001b[0m\n",
      "\u001b[34malgo-1:222:553 [2] NCCL INFO NCCL_TOPO_FILE set by environment to /opt/conda/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[34malgo-1:227:548 [7] NCCL INFO NCCL_TOPO_FILE set by environment to /opt/conda/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[34malgo-1:224:550 [4] NCCL INFO NCCL_TOPO_FILE set by environment to /opt/conda/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[34malgo-1:226:554 [6] NCCL INFO NCCL_TOPO_FILE set by environment to /opt/conda/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[34malgo-1:220:547 [0] NCCL INFO NCCL_TOPO_FILE set by environment to /opt/conda/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[34malgo-1:225:549 [5] NCCL INFO NCCL_TOPO_FILE set by environment to /opt/conda/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[34malgo-1:221:552 [1] NCCL INFO NCCL_TOPO_FILE set by environment to /opt/conda/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[34malgo-1:223:551 [3] NCCL INFO NCCL_TOPO_FILE set by environment to /opt/conda/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml\u001b[0m\n",
      "\u001b[34malgo-1:222:553 [2] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:222:553 [2] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:222:553 [2] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:222:553 [2] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:227:548 [7] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:227:548 [7] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:227:548 [7] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:227:548 [7] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:224:550 [4] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:224:550 [4] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:224:550 [4] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:224:550 [4] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:226:554 [6] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:226:554 [6] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:226:554 [6] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:226:554 [6] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:220:547 [0] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:220:547 [0] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:220:547 [0] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:220:547 [0] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:225:549 [5] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:225:549 [5] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:225:549 [5] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:225:549 [5] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:221:552 [1] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:221:552 [1] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:221:552 [1] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:221:552 [1] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:223:551 [3] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:223:551 [3] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:223:551 [3] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:223:551 [3] NCCL INFO NET/OFI Libfabric provider associates MRs with domains\u001b[0m\n",
      "\u001b[34malgo-1:222:553 [2] NCCL INFO Setting affinity for GPU 2 to ff,ffff0000,00ffffff\u001b[0m\n",
      "\u001b[34malgo-1:222:553 [2] NCCL INFO NVLS multicast support is not available on dev 2\u001b[0m\n",
      "\u001b[34malgo-1:227:548 [7] NCCL INFO Setting affinity for GPU 7 to ffffff00,0000ffff,ff000000\u001b[0m\n",
      "\u001b[34malgo-1:227:548 [7] NCCL INFO NVLS multicast support is not available on dev 7\u001b[0m\n",
      "\u001b[34malgo-1:225:549 [5] NCCL INFO Setting affinity for GPU 5 to ffffff00,0000ffff,ff000000\u001b[0m\n",
      "\u001b[34malgo-1:225:549 [5] NCCL INFO NVLS multicast support is not available on dev 5\u001b[0m\n",
      "\u001b[34malgo-1:224:550 [4] NCCL INFO Setting affinity for GPU 4 to ffffff00,0000ffff,ff000000\u001b[0m\n",
      "\u001b[34malgo-1:220:547 [0] NCCL INFO Setting affinity for GPU 0 to ff,ffff0000,00ffffff\u001b[0m\n",
      "\u001b[34malgo-1:220:547 [0] NCCL INFO NVLS multicast support is not available on dev 0\u001b[0m\n",
      "\u001b[34malgo-1:221:552 [1] NCCL INFO Setting affinity for GPU 1 to ff,ffff0000,00ffffff\u001b[0m\n",
      "\u001b[34malgo-1:221:552 [1] NCCL INFO NVLS multicast support is not available on dev 1\u001b[0m\n",
      "\u001b[34malgo-1:226:554 [6] NCCL INFO Setting affinity for GPU 6 to ffffff00,0000ffff,ff000000\u001b[0m\n",
      "\u001b[34malgo-1:223:551 [3] NCCL INFO Setting affinity for GPU 3 to ff,ffff0000,00ffffff\u001b[0m\n",
      "\u001b[34malgo-1:223:551 [3] NCCL INFO NVLS multicast support is not available on dev 3\u001b[0m\n",
      "\u001b[34malgo-1:226:554 [6] NCCL INFO NVLS multicast support is not available on dev 6\u001b[0m\n",
      "\u001b[34malgo-1:224:550 [4] NCCL INFO NVLS multicast support is not available on dev 4\u001b[0m\n",
      "\u001b[34malgo-1:224:550 [4] NCCL INFO Trees [0] 5/-1/-1->4->3 [1] 5/-1/-1->4->3 [2] 5/-1/-1->4->3 [3] 5/-1/-1->4->3 [4] 5/-1/-1->4->3 [5] 5/-1/-1->4->3 [6] 5/-1/-1->4->3 [7] 5/-1/-1->4->3 [8] 5/-1/-1->4->3 [9] 5/-1/-1->4->3 [10] 5/-1/-1->4->3 [11] 5/-1/-1->4->3 [12] 5/-1/-1->4->3 [13] 5/-1/-1->4->3 [14] 5/-1/-1->4->3 [15] 5/-1/-1->4->3 [16] 5/-1/-1->4->3 [17] 5/-1/-1->4->3 [18] 5/-1/-1->4->3 [19] 5/-1/-1->4->3 [20] 5/-1/-1->4->3 [21] 5/-1/-1->4->3 [22] 5/-1/-1->4->3 [23] 5/-1/-1->4->3\u001b[0m\n",
      "\u001b[34malgo-1:224:550 [4] NCCL INFO P2P Chunksize set to 524288\u001b[0m\n",
      "\u001b[34malgo-1:225:549 [5] NCCL INFO Trees [0] 6/-1/-1->5->4 [1] 6/-1/-1->5->4 [2] 6/-1/-1->5->4 [3] 6/-1/-1->5->4 [4] 6/-1/-1->5->4 [5] 6/-1/-1->5->4 [6] 6/-1/-1->5->4 [7] 6/-1/-1->5->4 [8] 6/-1/-1->5->4 [9] 6/-1/-1->5->4 [10] 6/-1/-1->5->4 [11] 6/-1/-1->5->4 [12] 6/-1/-1->5->4 [13] 6/-1/-1->5->4 [14] 6/-1/-1->5->4 [15] 6/-1/-1->5->4 [16] 6/-1/-1->5->4 [17] 6/-1/-1->5->4 [18] 6/-1/-1->5->4 [19] 6/-1/-1->5->4 [20] 6/-1/-1->5->4 [21] 6/-1/-1->5->4 [22] 6/-1/-1->5->4 [23] 6/-1/-1->5->4\u001b[0m\n",
      "\u001b[34malgo-1:225:549 [5] NCCL INFO P2P Chunksize set to 524288\u001b[0m\n",
      "\u001b[34malgo-1:224:550 [4] NCCL INFO Channel 00/0 : 4[4] -> 5[5] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:224:550 [4] NCCL INFO Channel 01/0 : 4[4] -> 5[5] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:224:550 [4] NCCL INFO Channel 02/0 : 4[4] -> 5[5] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:224:550 [4] NCCL INFO Channel 03/0 : 4[4] -> 5[5] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:224:550 [4] NCCL INFO Channel 04/0 : 4[4] -> 5[5] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:224:550 [4] NCCL INFO Channel 05/0 : 4[4] -> 5[5] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:224:550 [4] NCCL INFO Channel 06/0 : 4[4] -> 5[5] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:224:550 [4] NCCL INFO Channel 07/0 : 4[4] -> 5[5] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:224:550 [4] NCCL INFO Channel 08/0 : 4[4] -> 5[5] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:224:550 [4] NCCL INFO Channel 09/0 : 4[4] -> 5[5] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:224:550 [4] NCCL INFO Channel 10/0 : 4[4] -> 5[5] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:224:550 [4] NCCL INFO Channel 11/0 : 4[4] -> 5[5] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:224:550 [4] NCCL INFO Channel 12/0 : 4[4] -> 5[5] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:225:549 [5] NCCL INFO Channel 00/0 : 5[5] -> 6[6] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:224:550 [4] NCCL INFO Channel 13/0 : 4[4] -> 5[5] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:224:550 [4] NCCL INFO Channel 14/0 : 4[4] -> 5[5] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:225:549 [5] NCCL INFO Channel 01/0 : 5[5] -> 6[6] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:224:550 [4] NCCL INFO Channel 15/0 : 4[4] -> 5[5] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:225:549 [5] NCCL INFO Channel 02/0 : 5[5] -> 6[6] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:224:550 [4] NCCL INFO Channel 16/0 : 4[4] -> 5[5] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:225:549 [5] NCCL INFO Channel 03/0 : 5[5] -> 6[6] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:224:550 [4] NCCL INFO Channel 17/0 : 4[4] -> 5[5] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:225:549 [5] NCCL INFO Channel 04/0 : 5[5] -> 6[6] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:224:550 [4] NCCL INFO Channel 18/0 : 4[4] -> 5[5] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:224:550 [4] NCCL INFO Channel 19/0 : 4[4] -> 5[5] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:225:549 [5] NCCL INFO Channel 05/0 : 5[5] -> 6[6] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:224:550 [4] NCCL INFO Channel 20/0 : 4[4] -> 5[5] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:225:549 [5] NCCL INFO Channel 06/0 : 5[5] -> 6[6] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:224:550 [4] NCCL INFO Channel 21/0 : 4[4] -> 5[5] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:225:549 [5] NCCL INFO Channel 07/0 : 5[5] -> 6[6] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:224:550 [4] NCCL INFO Channel 22/0 : 4[4] -> 5[5] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:224:550 [4] NCCL INFO Channel 23/0 : 4[4] -> 5[5] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:225:549 [5] NCCL INFO Channel 08/0 : 5[5] -> 6[6] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:225:549 [5] NCCL INFO Channel 09/0 : 5[5] -> 6[6] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:225:549 [5] NCCL INFO Channel 10/0 : 5[5] -> 6[6] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:225:549 [5] NCCL INFO Channel 11/0 : 5[5] -> 6[6] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:225:549 [5] NCCL INFO Channel 12/0 : 5[5] -> 6[6] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:225:549 [5] NCCL INFO Channel 13/0 : 5[5] -> 6[6] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:225:549 [5] NCCL INFO Channel 14/0 : 5[5] -> 6[6] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:225:549 [5] NCCL INFO Channel 15/0 : 5[5] -> 6[6] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:225:549 [5] NCCL INFO Channel 16/0 : 5[5] -> 6[6] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:225:549 [5] NCCL INFO Channel 17/0 : 5[5] -> 6[6] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:225:549 [5] NCCL INFO Channel 18/0 : 5[5] -> 6[6] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:225:549 [5] NCCL INFO Channel 19/0 : 5[5] -> 6[6] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:225:549 [5] NCCL INFO Channel 20/0 : 5[5] -> 6[6] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:223:551 [3] NCCL INFO Trees [0] 4/-1/-1->3->2 [1] 4/-1/-1->3->2 [2] 4/-1/-1->3->2 [3] 4/-1/-1->3->2 [4] 4/-1/-1->3->2 [5] 4/-1/-1->3->2 [6] 4/-1/-1->3->2 [7] 4/-1/-1->3->2 [8] 4/-1/-1->3->2 [9] 4/-1/-1->3->2 [10] 4/-1/-1->3->2 [11] 4/-1/-1->3->2 [12] 4/-1/-1->3->2 [13] 4/-1/-1->3->2 [14] 4/-1/-1->3->2 [15] 4/-1/-1->3->2 [16] 4/-1/-1->3->2 [17] 4/-1/-1->3->2 [18] 4/-1/-1->3->2 [19] 4/-1/-1->3->2 [20] 4/-1/-1->3->2 [21] 4/-1/-1->3->2 [22] 4/-1/-1->3->2 [23] 4/-1/-1->3->2\u001b[0m\n",
      "\u001b[34malgo-1:223:551 [3] NCCL INFO P2P Chunksize set to 524288\u001b[0m\n",
      "\u001b[34malgo-1:222:553 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1 [2] 3/-1/-1->2->1 [3] 3/-1/-1->2->1 [4] 3/-1/-1->2->1 [5] 3/-1/-1->2->1 [6] 3/-1/-1->2->1 [7] 3/-1/-1->2->1 [8] 3/-1/-1->2->1 [9] 3/-1/-1->2->1 [10] 3/-1/-1->2->1 [11] 3/-1/-1->2->1 [12] 3/-1/-1->2->1 [13] 3/-1/-1->2->1 [14] 3/-1/-1->2->1 [15] 3/-1/-1->2->1 [16] 3/-1/-1->2->1 [17] 3/-1/-1->2->1 [18] 3/-1/-1->2->1 [19] 3/-1/-1->2->1 [20] 3/-1/-1->2->1 [21] 3/-1/-1->2->1 [22] 3/-1/-1->2->1 [23] 3/-1/-1->2->1\u001b[0m\n",
      "\u001b[34malgo-1:220:547 [0] NCCL INFO Channel 00/24 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34malgo-1:222:553 [2] NCCL INFO P2P Chunksize set to 524288\u001b[0m\n",
      "\u001b[34malgo-1:220:547 [0] NCCL INFO Channel 01/24 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34malgo-1:220:547 [0] NCCL INFO Channel 02/24 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34malgo-1:220:547 [0] NCCL INFO Channel 03/24 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34malgo-1:220:547 [0] NCCL INFO Channel 04/24 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34malgo-1:221:552 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0 [2] 2/-1/-1->1->0 [3] 2/-1/-1->1->0 [4] 2/-1/-1->1->0 [5] 2/-1/-1->1->0 [6] 2/-1/-1->1->0 [7] 2/-1/-1->1->0 [8] 2/-1/-1->1->0 [9] 2/-1/-1->1->0 [10] 2/-1/-1->1->0 [11] 2/-1/-1->1->0 [12] 2/-1/-1->1->0 [13] 2/-1/-1->1->0 [14] 2/-1/-1->1->0 [15] 2/-1/-1->1->0 [16] 2/-1/-1->1->0 [17] 2/-1/-1->1->0 [18] 2/-1/-1->1->0 [19] 2/-1/-1->1->0 [20] 2/-1/-1->1->0 [21] 2/-1/-1->1->0 [22] 2/-1/-1->1->0 [23] 2/-1/-1->1->0\u001b[0m\n",
      "\u001b[34malgo-1:220:547 [0] NCCL INFO Channel 05/24 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34malgo-1:221:552 [1] NCCL INFO P2P Chunksize set to 524288\u001b[0m\n",
      "\u001b[34malgo-1:220:547 [0] NCCL INFO Channel 06/24 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34malgo-1:226:554 [6] NCCL INFO Trees [0] 7/-1/-1->6->5 [1] 7/-1/-1->6->5 [2] 7/-1/-1->6->5 [3] 7/-1/-1->6->5 [4] 7/-1/-1->6->5 [5] 7/-1/-1->6->5 [6] 7/-1/-1->6->5 [7] 7/-1/-1->6->5 [8] 7/-1/-1->6->5 [9] 7/-1/-1->6->5 [10] 7/-1/-1->6->5 [11] 7/-1/-1->6->5 [12] 7/-1/-1->6->5 [13] 7/-1/-1->6->5 [14] 7/-1/-1->6->5 [15] 7/-1/-1->6->5 [16] 7/-1/-1->6->5 [17] 7/-1/-1->6->5 [18] 7/-1/-1->6->5 [19] 7/-1/-1->6->5 [20] 7/-1/-1->6->5 [21] 7/-1/-1->6->5 [22] 7/-1/-1->6->5 [23] 7/-1/-1->6->5\u001b[0m\n",
      "\u001b[34malgo-1:220:547 [0] NCCL INFO Channel 07/24 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34malgo-1:220:547 [0] NCCL INFO Channel 08/24 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34malgo-1:226:554 [6] NCCL INFO P2P Chunksize set to 524288\u001b[0m\n",
      "\u001b[34malgo-1:220:547 [0] NCCL INFO Channel 09/24 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34malgo-1:220:547 [0] NCCL INFO Channel 10/24 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34malgo-1:220:547 [0] NCCL INFO Channel 11/24 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34malgo-1:220:547 [0] NCCL INFO Channel 12/24 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34malgo-1:220:547 [0] NCCL INFO Channel 13/24 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34malgo-1:220:547 [0] NCCL INFO Channel 14/24 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34malgo-1:220:547 [0] NCCL INFO Channel 15/24 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34malgo-1:220:547 [0] NCCL INFO Channel 16/24 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34malgo-1:220:547 [0] NCCL INFO Channel 17/24 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34malgo-1:227:548 [7] NCCL INFO Trees [0] -1/-1/-1->7->6 [1] -1/-1/-1->7->6 [2] -1/-1/-1->7->6 [3] -1/-1/-1->7->6 [4] -1/-1/-1->7->6 [5] -1/-1/-1->7->6 [6] -1/-1/-1->7->6 [7] -1/-1/-1->7->6 [8] -1/-1/-1->7->6 [9] -1/-1/-1->7->6 [10] -1/-1/-1->7->6 [11] -1/-1/-1->7->6 [12] -1/-1/-1->7->6 [13] -1/-1/-1->7->6 [14] -1/-1/-1->7->6 [15] -1/-1/-1->7->6 [16] -1/-1/-1->7->6 [17] -1/-1/-1->7->6 [18] -1/-1/-1->7->6 [19] -1/-1/-1->7->6 [20] -1/-1/-1->7->6 [21] -1/-1/-1->7->6 [22] -1/-1/-1->7->6 [23] -1/-1/-1->7->6\u001b[0m\n",
      "\u001b[34malgo-1:220:547 [0] NCCL INFO Channel 18/24 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34malgo-1:220:547 [0] NCCL INFO Channel 19/24 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34malgo-1:220:547 [0] NCCL INFO Channel 20/24 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34malgo-1:227:548 [7] NCCL INFO P2P Chunksize set to 524288\u001b[0m\n",
      "\u001b[34malgo-1:220:547 [0] NCCL INFO Channel 21/24 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34malgo-1:220:547 [0] NCCL INFO Channel 22/24 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34malgo-1:220:547 [0] NCCL INFO Channel 23/24 :    0   1   2   3   4   5   6   7\u001b[0m\n",
      "\u001b[34malgo-1:220:547 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1 [2] 1/-1/-1->0->-1 [3] 1/-1/-1->0->-1 [4] 1/-1/-1->0->-1 [5] 1/-1/-1->0->-1 [6] 1/-1/-1->0->-1 [7] 1/-1/-1->0->-1 [8] 1/-1/-1->0->-1 [9] 1/-1/-1->0->-1 [10] 1/-1/-1->0->-1 [11] 1/-1/-1->0->-1 [12] 1/-1/-1->0->-1 [13] 1/-1/-1->0->-1 [14] 1/-1/-1->0->-1 [15] 1/-1/-1->0->-1 [16] 1/-1/-1->0->-1 [17] 1/-1/-1->0->-1 [18] 1/-1/-1->0->-1 [19] 1/-1/-1->0->-1 [20] 1/-1/-1->0->-1 [21] 1/-1/-1->0->-1 [22] 1/-1/-1->0->-1 [23] 1/-1/-1->0->-1\u001b[0m\n",
      "\u001b[34malgo-1:220:547 [0] NCCL INFO P2P Chunksize set to 524288\u001b[0m\n",
      "\u001b[34malgo-1:225:549 [5] NCCL INFO Channel 21/0 : 5[5] -> 6[6] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:225:549 [5] NCCL INFO Channel 22/0 : 5[5] -> 6[6] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:225:549 [5] NCCL INFO Channel 23/0 : 5[5] -> 6[6] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:223:551 [3] NCCL INFO Channel 00/0 : 3[3] -> 4[4] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:222:553 [2] NCCL INFO Channel 00/0 : 2[2] -> 3[3] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:220:547 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[1] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:223:551 [3] NCCL INFO Channel 01/0 : 3[3] -> 4[4] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:221:552 [1] NCCL INFO Channel 00/0 : 1[1] -> 2[2] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:222:553 [2] NCCL INFO Channel 01/0 : 2[2] -> 3[3] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:220:547 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[1] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:223:551 [3] NCCL INFO Channel 02/0 : 3[3] -> 4[4] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:221:552 [1] NCCL INFO Channel 01/0 : 1[1] -> 2[2] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:222:553 [2] NCCL INFO Channel 02/0 : 2[2] -> 3[3] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:220:547 [0] NCCL INFO Channel 02/0 : 0[0] -> 1[1] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:223:551 [3] NCCL INFO Channel 03/0 : 3[3] -> 4[4] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:221:552 [1] NCCL INFO Channel 02/0 : 1[1] -> 2[2] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:222:553 [2] NCCL INFO Channel 03/0 : 2[2] -> 3[3] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:226:554 [6] NCCL INFO Channel 00/0 : 6[6] -> 7[7] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:220:547 [0] NCCL INFO Channel 03/0 : 0[0] -> 1[1] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:223:551 [3] NCCL INFO Channel 04/0 : 3[3] -> 4[4] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:221:552 [1] NCCL INFO Channel 03/0 : 1[1] -> 2[2] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:222:553 [2] NCCL INFO Channel 04/0 : 2[2] -> 3[3] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:226:554 [6] NCCL INFO Channel 01/0 : 6[6] -> 7[7] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:220:547 [0] NCCL INFO Channel 04/0 : 0[0] -> 1[1] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:223:551 [3] NCCL INFO Channel 05/0 : 3[3] -> 4[4] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:221:552 [1] NCCL INFO Channel 04/0 : 1[1] -> 2[2] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:222:553 [2] NCCL INFO Channel 05/0 : 2[2] -> 3[3] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:220:547 [0] NCCL INFO Channel 05/0 : 0[0] -> 1[1] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:223:551 [3] NCCL INFO Channel 06/0 : 3[3] -> 4[4] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:226:554 [6] NCCL INFO Channel 02/0 : 6[6] -> 7[7] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:221:552 [1] NCCL INFO Channel 05/0 : 1[1] -> 2[2] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:227:548 [7] NCCL INFO Channel 00/0 : 7[7] -> 0[0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:222:553 [2] NCCL INFO Channel 06/0 : 2[2] -> 3[3] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:220:547 [0] NCCL INFO Channel 06/0 : 0[0] -> 1[1] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:223:551 [3] NCCL INFO Channel 07/0 : 3[3] -> 4[4] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:226:554 [6] NCCL INFO Channel 03/0 : 6[6] -> 7[7] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:221:552 [1] NCCL INFO Channel 06/0 : 1[1] -> 2[2] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:222:553 [2] NCCL INFO Channel 07/0 : 2[2] -> 3[3] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:220:547 [0] NCCL INFO Channel 07/0 : 0[0] -> 1[1] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:227:548 [7] NCCL INFO Channel 01/0 : 7[7] -> 0[0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:223:551 [3] NCCL INFO Channel 08/0 : 3[3] -> 4[4] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:226:554 [6] NCCL INFO Channel 04/0 : 6[6] -> 7[7] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:221:552 [1] NCCL INFO Channel 07/0 : 1[1] -> 2[2] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:222:553 [2] NCCL INFO Channel 08/0 : 2[2] -> 3[3] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:220:547 [0] NCCL INFO Channel 08/0 : 0[0] -> 1[1] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:227:548 [7] NCCL INFO Channel 02/0 : 7[7] -> 0[0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:223:551 [3] NCCL INFO Channel 09/0 : 3[3] -> 4[4] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:226:554 [6] NCCL INFO Channel 05/0 : 6[6] -> 7[7] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:221:552 [1] NCCL INFO Channel 08/0 : 1[1] -> 2[2] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:222:553 [2] NCCL INFO Channel 09/0 : 2[2] -> 3[3] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:220:547 [0] NCCL INFO Channel 09/0 : 0[0] -> 1[1] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:227:548 [7] NCCL INFO Channel 03/0 : 7[7] -> 0[0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:223:551 [3] NCCL INFO Channel 10/0 : 3[3] -> 4[4] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:226:554 [6] NCCL INFO Channel 06/0 : 6[6] -> 7[7] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:221:552 [1] NCCL INFO Channel 09/0 : 1[1] -> 2[2] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:222:553 [2] NCCL INFO Channel 10/0 : 2[2] -> 3[3] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:220:547 [0] NCCL INFO Channel 10/0 : 0[0] -> 1[1] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:227:548 [7] NCCL INFO Channel 04/0 : 7[7] -> 0[0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:223:551 [3] NCCL INFO Channel 11/0 : 3[3] -> 4[4] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:226:554 [6] NCCL INFO Channel 07/0 : 6[6] -> 7[7] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:221:552 [1] NCCL INFO Channel 10/0 : 1[1] -> 2[2] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:222:553 [2] NCCL INFO Channel 11/0 : 2[2] -> 3[3] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:220:547 [0] NCCL INFO Channel 11/0 : 0[0] -> 1[1] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:223:551 [3] NCCL INFO Channel 12/0 : 3[3] -> 4[4] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:227:548 [7] NCCL INFO Channel 05/0 : 7[7] -> 0[0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:221:552 [1] NCCL INFO Channel 11/0 : 1[1] -> 2[2] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:226:554 [6] NCCL INFO Channel 08/0 : 6[6] -> 7[7] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:222:553 [2] NCCL INFO Channel 12/0 : 2[2] -> 3[3] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:220:547 [0] NCCL INFO Channel 12/0 : 0[0] -> 1[1] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:223:551 [3] NCCL INFO Channel 13/0 : 3[3] -> 4[4] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:227:548 [7] NCCL INFO Channel 06/0 : 7[7] -> 0[0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:221:552 [1] NCCL INFO Channel 12/0 : 1[1] -> 2[2] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:222:553 [2] NCCL INFO Channel 13/0 : 2[2] -> 3[3] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:226:554 [6] NCCL INFO Channel 09/0 : 6[6] -> 7[7] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:220:547 [0] NCCL INFO Channel 13/0 : 0[0] -> 1[1] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:223:551 [3] NCCL INFO Channel 14/0 : 3[3] -> 4[4] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:227:548 [7] NCCL INFO Channel 07/0 : 7[7] -> 0[0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:221:552 [1] NCCL INFO Channel 13/0 : 1[1] -> 2[2] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:222:553 [2] NCCL INFO Channel 14/0 : 2[2] -> 3[3] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:226:554 [6] NCCL INFO Channel 10/0 : 6[6] -> 7[7] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:220:547 [0] NCCL INFO Channel 14/0 : 0[0] -> 1[1] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:223:551 [3] NCCL INFO Channel 15/0 : 3[3] -> 4[4] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:227:548 [7] NCCL INFO Channel 08/0 : 7[7] -> 0[0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:221:552 [1] NCCL INFO Channel 14/0 : 1[1] -> 2[2] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:222:553 [2] NCCL INFO Channel 15/0 : 2[2] -> 3[3] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:220:547 [0] NCCL INFO Channel 15/0 : 0[0] -> 1[1] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:226:554 [6] NCCL INFO Channel 11/0 : 6[6] -> 7[7] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:223:551 [3] NCCL INFO Channel 16/0 : 3[3] -> 4[4] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:227:548 [7] NCCL INFO Channel 09/0 : 7[7] -> 0[0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:221:552 [1] NCCL INFO Channel 15/0 : 1[1] -> 2[2] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:222:553 [2] NCCL INFO Channel 16/0 : 2[2] -> 3[3] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:220:547 [0] NCCL INFO Channel 16/0 : 0[0] -> 1[1] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:226:554 [6] NCCL INFO Channel 12/0 : 6[6] -> 7[7] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:223:551 [3] NCCL INFO Channel 17/0 : 3[3] -> 4[4] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:227:548 [7] NCCL INFO Channel 10/0 : 7[7] -> 0[0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:221:552 [1] NCCL INFO Channel 16/0 : 1[1] -> 2[2] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:222:553 [2] NCCL INFO Channel 17/0 : 2[2] -> 3[3] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:220:547 [0] NCCL INFO Channel 17/0 : 0[0] -> 1[1] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:223:551 [3] NCCL INFO Channel 18/0 : 3[3] -> 4[4] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:226:554 [6] NCCL INFO Channel 13/0 : 6[6] -> 7[7] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:221:552 [1] NCCL INFO Channel 17/0 : 1[1] -> 2[2] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:227:548 [7] NCCL INFO Channel 11/0 : 7[7] -> 0[0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:222:553 [2] NCCL INFO Channel 18/0 : 2[2] -> 3[3] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:220:547 [0] NCCL INFO Channel 18/0 : 0[0] -> 1[1] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:223:551 [3] NCCL INFO Channel 19/0 : 3[3] -> 4[4] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:226:554 [6] NCCL INFO Channel 14/0 : 6[6] -> 7[7] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:221:552 [1] NCCL INFO Channel 18/0 : 1[1] -> 2[2] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:222:553 [2] NCCL INFO Channel 19/0 : 2[2] -> 3[3] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:227:548 [7] NCCL INFO Channel 12/0 : 7[7] -> 0[0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:220:547 [0] NCCL INFO Channel 19/0 : 0[0] -> 1[1] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:223:551 [3] NCCL INFO Channel 20/0 : 3[3] -> 4[4] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:226:554 [6] NCCL INFO Channel 15/0 : 6[6] -> 7[7] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:221:552 [1] NCCL INFO Channel 19/0 : 1[1] -> 2[2] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:222:553 [2] NCCL INFO Channel 20/0 : 2[2] -> 3[3] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:227:548 [7] NCCL INFO Channel 13/0 : 7[7] -> 0[0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:220:547 [0] NCCL INFO Channel 20/0 : 0[0] -> 1[1] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:223:551 [3] NCCL INFO Channel 21/0 : 3[3] -> 4[4] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:226:554 [6] NCCL INFO Channel 16/0 : 6[6] -> 7[7] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:221:552 [1] NCCL INFO Channel 20/0 : 1[1] -> 2[2] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:222:553 [2] NCCL INFO Channel 21/0 : 2[2] -> 3[3] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:227:548 [7] NCCL INFO Channel 14/0 : 7[7] -> 0[0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:220:547 [0] NCCL INFO Channel 21/0 : 0[0] -> 1[1] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:223:551 [3] NCCL INFO Channel 22/0 : 3[3] -> 4[4] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:226:554 [6] NCCL INFO Channel 17/0 : 6[6] -> 7[7] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:221:552 [1] NCCL INFO Channel 21/0 : 1[1] -> 2[2] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:222:553 [2] NCCL INFO Channel 22/0 : 2[2] -> 3[3] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:227:548 [7] NCCL INFO Channel 15/0 : 7[7] -> 0[0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:220:547 [0] NCCL INFO Channel 22/0 : 0[0] -> 1[1] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:223:551 [3] NCCL INFO Channel 23/0 : 3[3] -> 4[4] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:226:554 [6] NCCL INFO Channel 18/0 : 6[6] -> 7[7] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:221:552 [1] NCCL INFO Channel 22/0 : 1[1] -> 2[2] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:222:553 [2] NCCL INFO Channel 23/0 : 2[2] -> 3[3] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:227:548 [7] NCCL INFO Channel 16/0 : 7[7] -> 0[0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:220:547 [0] NCCL INFO Channel 23/0 : 0[0] -> 1[1] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:226:554 [6] NCCL INFO Channel 19/0 : 6[6] -> 7[7] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:221:552 [1] NCCL INFO Channel 23/0 : 1[1] -> 2[2] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:227:548 [7] NCCL INFO Channel 17/0 : 7[7] -> 0[0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:226:554 [6] NCCL INFO Channel 20/0 : 6[6] -> 7[7] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:227:548 [7] NCCL INFO Channel 18/0 : 7[7] -> 0[0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:226:554 [6] NCCL INFO Channel 21/0 : 6[6] -> 7[7] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:227:548 [7] NCCL INFO Channel 19/0 : 7[7] -> 0[0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:226:554 [6] NCCL INFO Channel 22/0 : 6[6] -> 7[7] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:227:548 [7] NCCL INFO Channel 20/0 : 7[7] -> 0[0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:226:554 [6] NCCL INFO Channel 23/0 : 6[6] -> 7[7] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:227:548 [7] NCCL INFO Channel 21/0 : 7[7] -> 0[0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:227:548 [7] NCCL INFO Channel 22/0 : 7[7] -> 0[0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:227:548 [7] NCCL INFO Channel 23/0 : 7[7] -> 0[0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:224:550 [4] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34malgo-1:223:551 [3] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34malgo-1:222:553 [2] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34malgo-1:225:549 [5] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34malgo-1:220:547 [0] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34malgo-1:221:552 [1] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34malgo-1:223:551 [3] NCCL INFO Channel 00/0 : 3[3] -> 2[2] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:223:551 [3] NCCL INFO Channel 01/0 : 3[3] -> 2[2] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:223:551 [3] NCCL INFO Channel 02/0 : 3[3] -> 2[2] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:224:550 [4] NCCL INFO Channel 00/0 : 4[4] -> 3[3] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:223:551 [3] NCCL INFO Channel 03/0 : 3[3] -> 2[2] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:224:550 [4] NCCL INFO Channel 01/0 : 4[4] -> 3[3] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:226:554 [6] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34malgo-1:223:551 [3] NCCL INFO Channel 04/0 : 3[3] -> 2[2] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:227:548 [7] NCCL INFO Connected all rings\u001b[0m\n",
      "\u001b[34malgo-1:227:548 [7] NCCL INFO Channel 00/0 : 7[7] -> 6[6] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:224:550 [4] NCCL INFO Channel 02/0 : 4[4] -> 3[3] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:223:551 [3] NCCL INFO Channel 05/0 : 3[3] -> 2[2] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:227:548 [7] NCCL INFO Channel 01/0 : 7[7] -> 6[6] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:224:550 [4] NCCL INFO Channel 03/0 : 4[4] -> 3[3] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:223:551 [3] NCCL INFO Channel 06/0 : 3[3] -> 2[2] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:227:548 [7] NCCL INFO Channel 02/0 : 7[7] -> 6[6] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:224:550 [4] NCCL INFO Channel 04/0 : 4[4] -> 3[3] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:223:551 [3] NCCL INFO Channel 07/0 : 3[3] -> 2[2] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:227:548 [7] NCCL INFO Channel 03/0 : 7[7] -> 6[6] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:223:551 [3] NCCL INFO Channel 08/0 : 3[3] -> 2[2] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:224:550 [4] NCCL INFO Channel 05/0 : 4[4] -> 3[3] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:227:548 [7] NCCL INFO Channel 04/0 : 7[7] -> 6[6] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:223:551 [3] NCCL INFO Channel 09/0 : 3[3] -> 2[2] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:224:550 [4] NCCL INFO Channel 06/0 : 4[4] -> 3[3] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:227:548 [7] NCCL INFO Channel 05/0 : 7[7] -> 6[6] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:223:551 [3] NCCL INFO Channel 10/0 : 3[3] -> 2[2] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:224:550 [4] NCCL INFO Channel 07/0 : 4[4] -> 3[3] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:222:553 [2] NCCL INFO Channel 00/0 : 2[2] -> 1[1] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:227:548 [7] NCCL INFO Channel 06/0 : 7[7] -> 6[6] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:223:551 [3] NCCL INFO Channel 11/0 : 3[3] -> 2[2] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:224:550 [4] NCCL INFO Channel 08/0 : 4[4] -> 3[3] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:222:553 [2] NCCL INFO Channel 01/0 : 2[2] -> 1[1] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:227:548 [7] NCCL INFO Channel 07/0 : 7[7] -> 6[6] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:223:551 [3] NCCL INFO Channel 12/0 : 3[3] -> 2[2] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:222:553 [2] NCCL INFO Channel 02/0 : 2[2] -> 1[1] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:224:550 [4] NCCL INFO Channel 09/0 : 4[4] -> 3[3] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:227:548 [7] NCCL INFO Channel 08/0 : 7[7] -> 6[6] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:223:551 [3] NCCL INFO Channel 13/0 : 3[3] -> 2[2] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:222:553 [2] NCCL INFO Channel 03/0 : 2[2] -> 1[1] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:224:550 [4] NCCL INFO Channel 10/0 : 4[4] -> 3[3] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:227:548 [7] NCCL INFO Channel 09/0 : 7[7] -> 6[6] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:223:551 [3] NCCL INFO Channel 14/0 : 3[3] -> 2[2] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:222:553 [2] NCCL INFO Channel 04/0 : 2[2] -> 1[1] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:223:551 [3] NCCL INFO Channel 15/0 : 3[3] -> 2[2] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:227:548 [7] NCCL INFO Channel 10/0 : 7[7] -> 6[6] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:224:550 [4] NCCL INFO Channel 11/0 : 4[4] -> 3[3] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:222:553 [2] NCCL INFO Channel 05/0 : 2[2] -> 1[1] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:223:551 [3] NCCL INFO Channel 16/0 : 3[3] -> 2[2] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:227:548 [7] NCCL INFO Channel 11/0 : 7[7] -> 6[6] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:222:553 [2] NCCL INFO Channel 06/0 : 2[2] -> 1[1] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:224:550 [4] NCCL INFO Channel 12/0 : 4[4] -> 3[3] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:223:551 [3] NCCL INFO Channel 17/0 : 3[3] -> 2[2] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:227:548 [7] NCCL INFO Channel 12/0 : 7[7] -> 6[6] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:222:553 [2] NCCL INFO Channel 07/0 : 2[2] -> 1[1] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:225:549 [5] NCCL INFO Channel 00/0 : 5[5] -> 4[4] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:224:550 [4] NCCL INFO Channel 13/0 : 4[4] -> 3[3] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:223:551 [3] NCCL INFO Channel 18/0 : 3[3] -> 2[2] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:227:548 [7] NCCL INFO Channel 13/0 : 7[7] -> 6[6] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:222:553 [2] NCCL INFO Channel 08/0 : 2[2] -> 1[1] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:223:551 [3] NCCL INFO Channel 19/0 : 3[3] -> 2[2] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:225:549 [5] NCCL INFO Channel 01/0 : 5[5] -> 4[4] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:224:550 [4] NCCL INFO Channel 14/0 : 4[4] -> 3[3] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:227:548 [7] NCCL INFO Channel 14/0 : 7[7] -> 6[6] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:222:553 [2] NCCL INFO Channel 09/0 : 2[2] -> 1[1] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:223:551 [3] NCCL INFO Channel 20/0 : 3[3] -> 2[2] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:225:549 [5] NCCL INFO Channel 02/0 : 5[5] -> 4[4] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:224:550 [4] NCCL INFO Channel 15/0 : 4[4] -> 3[3] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:227:548 [7] NCCL INFO Channel 15/0 : 7[7] -> 6[6] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:222:553 [2] NCCL INFO Channel 10/0 : 2[2] -> 1[1] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:221:552 [1] NCCL INFO Channel 00/0 : 1[1] -> 0[0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:223:551 [3] NCCL INFO Channel 21/0 : 3[3] -> 2[2] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:225:549 [5] NCCL INFO Channel 03/0 : 5[5] -> 4[4] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:227:548 [7] NCCL INFO Channel 16/0 : 7[7] -> 6[6] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:222:553 [2] NCCL INFO Channel 11/0 : 2[2] -> 1[1] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:224:550 [4] NCCL INFO Channel 16/0 : 4[4] -> 3[3] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:221:552 [1] NCCL INFO Channel 01/0 : 1[1] -> 0[0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:223:551 [3] NCCL INFO Channel 22/0 : 3[3] -> 2[2] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:225:549 [5] NCCL INFO Channel 04/0 : 5[5] -> 4[4] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:222:553 [2] NCCL INFO Channel 12/0 : 2[2] -> 1[1] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:227:548 [7] NCCL INFO Channel 17/0 : 7[7] -> 6[6] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:224:550 [4] NCCL INFO Channel 17/0 : 4[4] -> 3[3] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:221:552 [1] NCCL INFO Channel 02/0 : 1[1] -> 0[0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:223:551 [3] NCCL INFO Channel 23/0 : 3[3] -> 2[2] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:225:549 [5] NCCL INFO Channel 05/0 : 5[5] -> 4[4] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:222:553 [2] NCCL INFO Channel 13/0 : 2[2] -> 1[1] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:221:552 [1] NCCL INFO Channel 03/0 : 1[1] -> 0[0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:227:548 [7] NCCL INFO Channel 18/0 : 7[7] -> 6[6] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:224:550 [4] NCCL INFO Channel 18/0 : 4[4] -> 3[3] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:225:549 [5] NCCL INFO Channel 06/0 : 5[5] -> 4[4] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:222:553 [2] NCCL INFO Channel 14/0 : 2[2] -> 1[1] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:221:552 [1] NCCL INFO Channel 04/0 : 1[1] -> 0[0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:227:548 [7] NCCL INFO Channel 19/0 : 7[7] -> 6[6] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:224:550 [4] NCCL INFO Channel 19/0 : 4[4] -> 3[3] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:222:553 [2] NCCL INFO Channel 15/0 : 2[2] -> 1[1] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:225:549 [5] NCCL INFO Channel 07/0 : 5[5] -> 4[4] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:221:552 [1] NCCL INFO Channel 05/0 : 1[1] -> 0[0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:227:548 [7] NCCL INFO Channel 20/0 : 7[7] -> 6[6] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:224:550 [4] NCCL INFO Channel 20/0 : 4[4] -> 3[3] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:222:553 [2] NCCL INFO Channel 16/0 : 2[2] -> 1[1] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:221:552 [1] NCCL INFO Channel 06/0 : 1[1] -> 0[0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:225:549 [5] NCCL INFO Channel 08/0 : 5[5] -> 4[4] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:226:554 [6] NCCL INFO Channel 00/0 : 6[6] -> 5[5] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:227:548 [7] NCCL INFO Channel 21/0 : 7[7] -> 6[6] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:224:550 [4] NCCL INFO Channel 21/0 : 4[4] -> 3[3] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:222:553 [2] NCCL INFO Channel 17/0 : 2[2] -> 1[1] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:221:552 [1] NCCL INFO Channel 07/0 : 1[1] -> 0[0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:225:549 [5] NCCL INFO Channel 09/0 : 5[5] -> 4[4] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:226:554 [6] NCCL INFO Channel 01/0 : 6[6] -> 5[5] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:227:548 [7] NCCL INFO Channel 22/0 : 7[7] -> 6[6] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:222:553 [2] NCCL INFO Channel 18/0 : 2[2] -> 1[1] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:224:550 [4] NCCL INFO Channel 22/0 : 4[4] -> 3[3] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:221:552 [1] NCCL INFO Channel 08/0 : 1[1] -> 0[0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:226:554 [6] NCCL INFO Channel 02/0 : 6[6] -> 5[5] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:225:549 [5] NCCL INFO Channel 10/0 : 5[5] -> 4[4] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:227:548 [7] NCCL INFO Channel 23/0 : 7[7] -> 6[6] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:222:553 [2] NCCL INFO Channel 19/0 : 2[2] -> 1[1] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:224:550 [4] NCCL INFO Channel 23/0 : 4[4] -> 3[3] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:221:552 [1] NCCL INFO Channel 09/0 : 1[1] -> 0[0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:226:554 [6] NCCL INFO Channel 03/0 : 6[6] -> 5[5] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:225:549 [5] NCCL INFO Channel 11/0 : 5[5] -> 4[4] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:222:553 [2] NCCL INFO Channel 20/0 : 2[2] -> 1[1] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:221:552 [1] NCCL INFO Channel 10/0 : 1[1] -> 0[0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:226:554 [6] NCCL INFO Channel 04/0 : 6[6] -> 5[5] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:225:549 [5] NCCL INFO Channel 12/0 : 5[5] -> 4[4] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:222:553 [2] NCCL INFO Channel 21/0 : 2[2] -> 1[1] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:221:552 [1] NCCL INFO Channel 11/0 : 1[1] -> 0[0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:226:554 [6] NCCL INFO Channel 05/0 : 6[6] -> 5[5] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:222:553 [2] NCCL INFO Channel 22/0 : 2[2] -> 1[1] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:221:552 [1] NCCL INFO Channel 12/0 : 1[1] -> 0[0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:225:549 [5] NCCL INFO Channel 13/0 : 5[5] -> 4[4] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:226:554 [6] NCCL INFO Channel 06/0 : 6[6] -> 5[5] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:222:553 [2] NCCL INFO Channel 23/0 : 2[2] -> 1[1] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:221:552 [1] NCCL INFO Channel 13/0 : 1[1] -> 0[0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:225:549 [5] NCCL INFO Channel 14/0 : 5[5] -> 4[4] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:226:554 [6] NCCL INFO Channel 07/0 : 6[6] -> 5[5] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:221:552 [1] NCCL INFO Channel 14/0 : 1[1] -> 0[0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:226:554 [6] NCCL INFO Channel 08/0 : 6[6] -> 5[5] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:225:549 [5] NCCL INFO Channel 15/0 : 5[5] -> 4[4] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:221:552 [1] NCCL INFO Channel 15/0 : 1[1] -> 0[0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:226:554 [6] NCCL INFO Channel 09/0 : 6[6] -> 5[5] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:221:552 [1] NCCL INFO Channel 16/0 : 1[1] -> 0[0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:225:549 [5] NCCL INFO Channel 16/0 : 5[5] -> 4[4] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:226:554 [6] NCCL INFO Channel 10/0 : 6[6] -> 5[5] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:221:552 [1] NCCL INFO Channel 17/0 : 1[1] -> 0[0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:225:549 [5] NCCL INFO Channel 17/0 : 5[5] -> 4[4] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:226:554 [6] NCCL INFO Channel 11/0 : 6[6] -> 5[5] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:221:552 [1] NCCL INFO Channel 18/0 : 1[1] -> 0[0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:225:549 [5] NCCL INFO Channel 18/0 : 5[5] -> 4[4] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:226:554 [6] NCCL INFO Channel 12/0 : 6[6] -> 5[5] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:221:552 [1] NCCL INFO Channel 19/0 : 1[1] -> 0[0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:226:554 [6] NCCL INFO Channel 13/0 : 6[6] -> 5[5] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:221:552 [1] NCCL INFO Channel 20/0 : 1[1] -> 0[0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:225:549 [5] NCCL INFO Channel 19/0 : 5[5] -> 4[4] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:226:554 [6] NCCL INFO Channel 14/0 : 6[6] -> 5[5] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:221:552 [1] NCCL INFO Channel 21/0 : 1[1] -> 0[0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:225:549 [5] NCCL INFO Channel 20/0 : 5[5] -> 4[4] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:226:554 [6] NCCL INFO Channel 15/0 : 6[6] -> 5[5] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:221:552 [1] NCCL INFO Channel 22/0 : 1[1] -> 0[0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:225:549 [5] NCCL INFO Channel 21/0 : 5[5] -> 4[4] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:226:554 [6] NCCL INFO Channel 16/0 : 6[6] -> 5[5] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:221:552 [1] NCCL INFO Channel 23/0 : 1[1] -> 0[0] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:225:549 [5] NCCL INFO Channel 22/0 : 5[5] -> 4[4] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:226:554 [6] NCCL INFO Channel 17/0 : 6[6] -> 5[5] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:225:549 [5] NCCL INFO Channel 23/0 : 5[5] -> 4[4] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:226:554 [6] NCCL INFO Channel 18/0 : 6[6] -> 5[5] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:226:554 [6] NCCL INFO Channel 19/0 : 6[6] -> 5[5] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:226:554 [6] NCCL INFO Channel 20/0 : 6[6] -> 5[5] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:226:554 [6] NCCL INFO Channel 21/0 : 6[6] -> 5[5] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:226:554 [6] NCCL INFO Channel 22/0 : 6[6] -> 5[5] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:226:554 [6] NCCL INFO Channel 23/0 : 6[6] -> 5[5] via P2P/IPC/read\u001b[0m\n",
      "\u001b[34malgo-1:220:547 [0] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34malgo-1:220:547 [0] NCCL INFO NCCL_PROTO set by environment to simple\u001b[0m\n",
      "\u001b[34malgo-1:220:547 [0] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34malgo-1:220:547 [0] NCCL INFO 24 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer\u001b[0m\n",
      "\u001b[34malgo-1:223:551 [3] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34malgo-1:223:551 [3] NCCL INFO NCCL_PROTO set by environment to simple\u001b[0m\n",
      "\u001b[34malgo-1:223:551 [3] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34malgo-1:223:551 [3] NCCL INFO 24 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer\u001b[0m\n",
      "\u001b[34malgo-1:227:548 [7] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34malgo-1:227:548 [7] NCCL INFO NCCL_PROTO set by environment to simple\u001b[0m\n",
      "\u001b[34malgo-1:227:548 [7] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34malgo-1:227:548 [7] NCCL INFO 24 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer\u001b[0m\n",
      "\u001b[34malgo-1:222:553 [2] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34malgo-1:222:553 [2] NCCL INFO NCCL_PROTO set by environment to simple\u001b[0m\n",
      "\u001b[34malgo-1:222:553 [2] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34malgo-1:222:553 [2] NCCL INFO 24 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer\u001b[0m\n",
      "\u001b[34malgo-1:221:552 [1] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34malgo-1:221:552 [1] NCCL INFO NCCL_PROTO set by environment to simple\u001b[0m\n",
      "\u001b[34malgo-1:221:552 [1] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34malgo-1:221:552 [1] NCCL INFO 24 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer\u001b[0m\n",
      "\u001b[34malgo-1:224:550 [4] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34malgo-1:224:550 [4] NCCL INFO NCCL_PROTO set by environment to simple\u001b[0m\n",
      "\u001b[34malgo-1:224:550 [4] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34malgo-1:224:550 [4] NCCL INFO 24 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer\u001b[0m\n",
      "\u001b[34malgo-1:226:554 [6] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34malgo-1:226:554 [6] NCCL INFO NCCL_PROTO set by environment to simple\u001b[0m\n",
      "\u001b[34malgo-1:226:554 [6] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34malgo-1:226:554 [6] NCCL INFO 24 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer\u001b[0m\n",
      "\u001b[34malgo-1:225:549 [5] NCCL INFO Connected all trees\u001b[0m\n",
      "\u001b[34malgo-1:225:549 [5] NCCL INFO NCCL_PROTO set by environment to simple\u001b[0m\n",
      "\u001b[34malgo-1:225:549 [5] NCCL INFO threadThresholds 8/8/64 | 64/8/64 | 512 | 512\u001b[0m\n",
      "\u001b[34malgo-1:225:549 [5] NCCL INFO 24 coll channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer\u001b[0m\n",
      "\u001b[34malgo-1:222:553 [2] NCCL INFO comm 0x5583ca69f830 rank 2 nranks 8 cudaDev 2 nvmlDev 2 busId 201c0 commId 0xee7b35dfe14976ce - Init COMPLETE\u001b[0m\n",
      "\u001b[34malgo-1:221:552 [1] NCCL INFO comm 0x55fd5268aa80 rank 1 nranks 8 cudaDev 1 nvmlDev 1 busId 101d0 commId 0xee7b35dfe14976ce - Init COMPLETE\u001b[0m\n",
      "\u001b[34malgo-1:224:550 [4] NCCL INFO comm 0x55f80ff36b30 rank 4 nranks 8 cudaDev 4 nvmlDev 4 busId 901c0 commId 0xee7b35dfe14976ce - Init COMPLETE\u001b[0m\n",
      "\u001b[34malgo-1:225:549 [5] NCCL INFO comm 0x55d5267db8f0 rank 5 nranks 8 cudaDev 5 nvmlDev 5 busId 901d0 commId 0xee7b35dfe14976ce - Init COMPLETE\u001b[0m\n",
      "\u001b[34malgo-1:226:554 [6] NCCL INFO comm 0x5578d34ecac0 rank 6 nranks 8 cudaDev 6 nvmlDev 6 busId a01c0 commId 0xee7b35dfe14976ce - Init COMPLETE\u001b[0m\n",
      "\u001b[34malgo-1:220:547 [0] NCCL INFO comm 0x56235546d1d0 rank 0 nranks 8 cudaDev 0 nvmlDev 0 busId 101c0 commId 0xee7b35dfe14976ce - Init COMPLETE\u001b[0m\n",
      "\u001b[34malgo-1:227:548 [7] NCCL INFO comm 0x56276a4dc730 rank 7 nranks 8 cudaDev 7 nvmlDev 7 busId a01d0 commId 0xee7b35dfe14976ce - Init COMPLETE\u001b[0m\n",
      "\u001b[34malgo-1:223:551 [3] NCCL INFO comm 0x5616a8c4b2f0 rank 3 nranks 8 cudaDev 3 nvmlDev 3 busId 201d0 commId 0xee7b35dfe14976ce - Init COMPLETE\u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\n",
      "  File \"/opt/ml/code/src/llamafactory/data/parser.py\", line 91, in get_dataset_list\u001b[0m\n",
      "\u001b[34mdataset_info = json.load(f)\n",
      "  File \"/opt/conda/lib/python3.10/json/__init__.py\", line 293, in load\u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\n",
      "  File \"/opt/ml/code/src/llamafactory/data/parser.py\", line 91, in get_dataset_list\u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\n",
      "  File \"/opt/ml/code/src/llamafactory/data/parser.py\", line 91, in get_dataset_list\u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\n",
      "  File \"/opt/ml/code/src/llamafactory/data/parser.py\", line 91, in get_dataset_list\n",
      "  File \"/opt/ml/code/src/llamafactory/data/parser.py\", line 91, in get_dataset_list\n",
      "  File \"/opt/ml/code/src/llamafactory/data/parser.py\", line 91, in get_dataset_list\n",
      "  File \"/opt/ml/code/src/llamafactory/data/parser.py\", line 91, in get_dataset_list\n",
      "  File \"/opt/ml/code/src/llamafactory/data/parser.py\", line 91, in get_dataset_list\u001b[0m\n",
      "\u001b[34mreturn loads(fp.read(),\n",
      "  File \"/opt/conda/lib/python3.10/json/__init__.py\", line 346, in loads\n",
      "                dataset_info = json.load(f)dataset_info = json.load(f)dataset_info = json.load(f)dataset_info = json.load(f)    \n",
      "    \u001b[0m\n",
      "\u001b[34mdataset_info = json.load(f)\u001b[0m\n",
      "\u001b[34mdataset_info = json.load(f)      File \"/opt/conda/lib/python3.10/json/__init__.py\", line 293, in load\n",
      "  File \"/opt/conda/lib/python3.10/json/__init__.py\", line 293, in load\n",
      "  File \"/opt/conda/lib/python3.10/json/__init__.py\", line 293, in load\n",
      "  File \"/opt/conda/lib/python3.10/json/__init__.py\", line 293, in load\u001b[0m\n",
      "\u001b[34mdataset_info = json.load(f)  File \"/opt/conda/lib/python3.10/json/__init__.py\", line 293, in load\n",
      "  File \"/opt/conda/lib/python3.10/json/__init__.py\", line 293, in load\n",
      "  File \"/opt/conda/lib/python3.10/json/__init__.py\", line 293, in load\n",
      "    return _default_decoder.decode(s)\n",
      "  File \"/opt/conda/lib/python3.10/json/decoder.py\", line 337, in decode\n",
      "    return loads(fp.read(),\u001b[0m\n",
      "\u001b[34mreturn loads(fp.read(),  File \"/opt/conda/lib/python3.10/json/__init__.py\", line 346, in loads\u001b[0m\n",
      "\u001b[34mreturn loads(fp.read(),return loads(fp.read(),return loads(fp.read(),return loads(fp.read(),\u001b[0m\n",
      "\u001b[34mreturn loads(fp.read(),\n",
      "  File \"/opt/conda/lib/python3.10/json/__init__.py\", line 346, in loads\n",
      "  File \"/opt/conda/lib/python3.10/json/__init__.py\", line 346, in loads\u001b[0m\n",
      "\u001b[34mFile \"/opt/conda/lib/python3.10/json/__init__.py\", line 346, in loads\n",
      "  File \"/opt/conda/lib/python3.10/json/__init__.py\", line 346, in loads\n",
      "  File \"/opt/conda/lib/python3.10/json/__init__.py\", line 346, in loads\n",
      "  File \"/opt/conda/lib/python3.10/json/__init__.py\", line 346, in loads\n",
      "    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n",
      "  File \"/opt/conda/lib/python3.10/json/decoder.py\", line 353, in raw_decode\n",
      "    obj, end = self.scan_once(s, idx)\n",
      "    return _default_decoder.decode(s)json.decoder\n",
      "        .  File \"/opt/conda/lib/python3.10/json/decoder.py\", line 337, in decode\u001b[0m\n",
      "\u001b[34mreturn _default_decoder.decode(s)return _default_decoder.decode(s)JSONDecodeError            \u001b[0m\n",
      "\u001b[34mreturn _default_decoder.decode(s)    return _default_decoder.decode(s)return _default_decoder.decode(s):   File \"/opt/conda/lib/python3.10/json/decoder.py\", line 337, in decode\u001b[0m\n",
      "\u001b[34mreturn _default_decoder.decode(s)  File \"/opt/conda/lib/python3.10/json/decoder.py\", line 337, in decode\u001b[0m\n",
      "\u001b[34mExpecting property name enclosed in double quotes: line 13 column 5 (char 314)\n",
      "  File \"/opt/conda/lib/python3.10/json/decoder.py\", line 337, in decode\n",
      "  File \"/opt/conda/lib/python3.10/json/decoder.py\", line 337, in decode\n",
      "  File \"/opt/conda/lib/python3.10/json/decoder.py\", line 337, in decode\n",
      "  File \"/opt/conda/lib/python3.10/json/decoder.py\", line 337, in decode\u001b[0m\n",
      "\u001b[34mDuring handling of the above exception, another exception occurred:\u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\n",
      "  File \"/opt/ml/code/src/train.py\", line 28, in <module>\n",
      "    main()\n",
      "  File \"/opt/ml/code/src/train.py\", line 19, in main\n",
      "    obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n",
      "  File \"/opt/conda/lib/python3.10/json/decoder.py\", line 353, in raw_decode\n",
      "            obj, end = self.raw_decode(s, idx=_w(s, 0).end())obj, end = self.raw_decode(s, idx=_w(s, 0).end())    obj, end = self.raw_decode(s, idx=_w(s, 0).end())        \u001b[0m\n",
      "\u001b[34mobj, end = self.raw_decode(s, idx=_w(s, 0).end())\u001b[0m\n",
      "\u001b[34mobj, end = self.raw_decode(s, idx=_w(s, 0).end())obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n",
      "  File \"/opt/conda/lib/python3.10/json/decoder.py\", line 353, in raw_decode\n",
      "  File \"/opt/conda/lib/python3.10/json/decoder.py\", line 353, in raw_decode\n",
      "  File \"/opt/conda/lib/python3.10/json/decoder.py\", line 353, in raw_decode\n",
      "  File \"/opt/conda/lib/python3.10/json/decoder.py\", line 353, in raw_decode\n",
      "  File \"/opt/conda/lib/python3.10/json/decoder.py\", line 353, in raw_decode\n",
      "  File \"/opt/conda/lib/python3.10/json/decoder.py\", line 353, in raw_decode\n",
      "    run_exp()\n",
      "  File \"/opt/ml/code/src/llamafactory/train/tuner.py\", line 50, in run_exp\n",
      "    run_sft(model_args, data_args, training_args, finetuning_args, generating_args, callbacks)\n",
      "      File \"/opt/ml/code/src/llamafactory/train/sft/workflow.py\", line 47, in run_sft\u001b[0m\n",
      "\u001b[34mobj, end = self.scan_once(s, idx)\n",
      "        json.decoder    obj, end = self.scan_once(s, idx)            obj, end = self.scan_once(s, idx).obj, end = self.scan_once(s, idx)obj, end = self.scan_once(s, idx)obj, end = self.scan_once(s, idx)obj, end = self.scan_once(s, idx)\u001b[0m\n",
      "\u001b[34mJSONDecodeError\u001b[0m\n",
      "\u001b[34m:\u001b[0m\n",
      "\u001b[34mjson.decoderExpecting property name enclosed in double quotes: line 13 column 5 (char 314)json.decoderjson.decoderjson.decoderjson.decoderjson.decoder..\u001b[0m\n",
      "\u001b[34m....JSONDecodeError\u001b[0m\n",
      "\u001b[34mDuring handling of the above exception, another exception occurred:\u001b[0m\n",
      "\u001b[34mJSONDecodeErrorJSONDecodeErrorJSONDecodeErrorJSONDecodeErrorTraceback (most recent call last):\u001b[0m\n",
      "\u001b[34mJSONDecodeError    : : : :   File \"/opt/ml/code/src/train.py\", line 28, in <module>\u001b[0m\n",
      "\u001b[34mdataset_module = get_dataset(template, model_args, data_args, training_args, stage=\"sft\", **tokenizer_module)Expecting property name enclosed in double quotes: line 13 column 5 (char 314): : Expecting property name enclosed in double quotes: line 13 column 5 (char 314)Expecting property name enclosed in double quotes: line 13 column 5 (char 314)Expecting property name enclosed in double quotes: line 13 column 5 (char 314)\u001b[0m\n",
      "\u001b[34mExpecting property name enclosed in double quotes: line 13 column 5 (char 314)\u001b[0m\n",
      "\u001b[34mDuring handling of the above exception, another exception occurred:\u001b[0m\n",
      "\u001b[34mDuring handling of the above exception, another exception occurred:\u001b[0m\n",
      "\u001b[34mDuring handling of the above exception, another exception occurred:\u001b[0m\n",
      "\u001b[34mExpecting property name enclosed in double quotes: line 13 column 5 (char 314)  File \"/opt/ml/code/src/llamafactory/data/loader.py\", line 248, in get_dataset\u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\u001b[0m\n",
      "\u001b[34mDuring handling of the above exception, another exception occurred:\u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\n",
      "  File \"/opt/ml/code/src/train.py\", line 28, in <module>\u001b[0m\n",
      "\u001b[34mDuring handling of the above exception, another exception occurred:\u001b[0m\n",
      "\u001b[34mDuring handling of the above exception, another exception occurred:\n",
      "  File \"/opt/ml/code/src/train.py\", line 28, in <module>\n",
      "  File \"/opt/ml/code/src/train.py\", line 28, in <module>\n",
      "  File \"/opt/ml/code/src/train.py\", line 28, in <module>\u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\n",
      "  File \"/opt/ml/code/src/train.py\", line 28, in <module>\n",
      "  File \"/opt/ml/code/src/train.py\", line 28, in <module>\n",
      "    main()\n",
      "  File \"/opt/ml/code/src/train.py\", line 19, in main\n",
      "                main()main()main()main()\n",
      "    \n",
      "    main()  File \"/opt/ml/code/src/train.py\", line 19, in main\n",
      "  File \"/opt/ml/code/src/train.py\", line 19, in main\u001b[0m\n",
      "\u001b[34mmain()\n",
      "  File \"/opt/ml/code/src/train.py\", line 19, in main\n",
      "  File \"/opt/ml/code/src/train.py\", line 19, in main\n",
      "  File \"/opt/ml/code/src/train.py\", line 19, in main\n",
      "  File \"/opt/ml/code/src/train.py\", line 19, in main\n",
      "    dataset = _get_merged_dataset(data_args.dataset, model_args, data_args, training_args, stage)\n",
      "  File \"/opt/ml/code/src/llamafactory/data/loader.py\", line 154, in _get_merged_dataset\n",
      "    run_exp()\n",
      "  File \"/opt/ml/code/src/llamafactory/train/tuner.py\", line 50, in run_exp\n",
      "                run_exp()run_exp()run_exp()run_exp()\n",
      "          File \"/opt/ml/code/src/llamafactory/train/tuner.py\", line 50, in run_exp\n",
      "  File \"/opt/ml/code/src/llamafactory/train/tuner.py\", line 50, in run_exp\u001b[0m\n",
      "\u001b[34mrun_exp()run_exp()  File \"/opt/ml/code/src/llamafactory/train/tuner.py\", line 50, in run_exp\n",
      "  File \"/opt/ml/code/src/llamafactory/train/tuner.py\", line 50, in run_exp\n",
      "  File \"/opt/ml/code/src/llamafactory/train/tuner.py\", line 50, in run_exp\n",
      "  File \"/opt/ml/code/src/llamafactory/train/tuner.py\", line 50, in run_exp\u001b[0m\n",
      "\u001b[34mfor dataset_attr in get_dataset_list(dataset_names, data_args.dataset_dir):\n",
      "  File \"/opt/ml/code/src/llamafactory/data/parser.py\", line 94, in get_dataset_list\n",
      "    run_sft(model_args, data_args, training_args, finetuning_args, generating_args, callbacks)\n",
      "  File \"/opt/ml/code/src/llamafactory/train/sft/workflow.py\", line 47, in run_sft\n",
      "                        run_sft(model_args, data_args, training_args, finetuning_args, generating_args, callbacks)run_sft(model_args, data_args, training_args, finetuning_args, generating_args, callbacks)run_sft(model_args, data_args, training_args, finetuning_args, generating_args, callbacks)run_sft(model_args, data_args, training_args, finetuning_args, generating_args, callbacks)\u001b[0m\n",
      "\u001b[34mrun_sft(model_args, data_args, training_args, finetuning_args, generating_args, callbacks)run_sft(model_args, data_args, training_args, finetuning_args, generating_args, callbacks)\n",
      "  File \"/opt/ml/code/src/llamafactory/train/sft/workflow.py\", line 47, in run_sft\n",
      "  File \"/opt/ml/code/src/llamafactory/train/sft/workflow.py\", line 47, in run_sft\n",
      "  File \"/opt/ml/code/src/llamafactory/train/sft/workflow.py\", line 47, in run_sft\n",
      "  File \"/opt/ml/code/src/llamafactory/train/sft/workflow.py\", line 47, in run_sft\n",
      "  File \"/opt/ml/code/src/llamafactory/train/sft/workflow.py\", line 47, in run_sft\n",
      "  File \"/opt/ml/code/src/llamafactory/train/sft/workflow.py\", line 47, in run_sft\n",
      "    raise ValueError(\"Cannot open {} due to {}.\".format(config_path, str(err)))\u001b[0m\n",
      "\u001b[34mValueError: Cannot open data/dataset_info.json due to Expecting property name enclosed in double quotes: line 13 column 5 (char 314).\n",
      "    dataset_module = get_dataset(template, model_args, data_args, training_args, stage=\"sft\", **tokenizer_module)\n",
      "  File \"/opt/ml/code/src/llamafactory/data/loader.py\", line 248, in get_dataset\n",
      "                dataset_module = get_dataset(template, model_args, data_args, training_args, stage=\"sft\", **tokenizer_module)    dataset_module = get_dataset(template, model_args, data_args, training_args, stage=\"sft\", **tokenizer_module)    dataset_module = get_dataset(template, model_args, data_args, training_args, stage=\"sft\", **tokenizer_module)dataset_module = get_dataset(template, model_args, data_args, training_args, stage=\"sft\", **tokenizer_module)\u001b[0m\n",
      "\u001b[34mdataset_module = get_dataset(template, model_args, data_args, training_args, stage=\"sft\", **tokenizer_module)dataset_module = get_dataset(template, model_args, data_args, training_args, stage=\"sft\", **tokenizer_module)\n",
      "  File \"/opt/ml/code/src/llamafactory/data/loader.py\", line 248, in get_dataset\n",
      "  File \"/opt/ml/code/src/llamafactory/data/loader.py\", line 248, in get_dataset\n",
      "  File \"/opt/ml/code/src/llamafactory/data/loader.py\", line 248, in get_dataset\n",
      "  File \"/opt/ml/code/src/llamafactory/data/loader.py\", line 248, in get_dataset\n",
      "  File \"/opt/ml/code/src/llamafactory/data/loader.py\", line 248, in get_dataset\n",
      "  File \"/opt/ml/code/src/llamafactory/data/loader.py\", line 248, in get_dataset\n",
      "    dataset = _get_merged_dataset(data_args.dataset, model_args, data_args, training_args, stage)\n",
      "  File \"/opt/ml/code/src/llamafactory/data/loader.py\", line 154, in _get_merged_dataset\n",
      "            dataset = _get_merged_dataset(data_args.dataset, model_args, data_args, training_args, stage)    dataset = _get_merged_dataset(data_args.dataset, model_args, data_args, training_args, stage)        dataset = _get_merged_dataset(data_args.dataset, model_args, data_args, training_args, stage)dataset = _get_merged_dataset(data_args.dataset, model_args, data_args, training_args, stage)\u001b[0m\n",
      "\u001b[34mdataset = _get_merged_dataset(data_args.dataset, model_args, data_args, training_args, stage)dataset = _get_merged_dataset(data_args.dataset, model_args, data_args, training_args, stage)\n",
      "  File \"/opt/ml/code/src/llamafactory/data/loader.py\", line 154, in _get_merged_dataset\n",
      "  File \"/opt/ml/code/src/llamafactory/data/loader.py\", line 154, in _get_merged_dataset\n",
      "  File \"/opt/ml/code/src/llamafactory/data/loader.py\", line 154, in _get_merged_dataset\n",
      "  File \"/opt/ml/code/src/llamafactory/data/loader.py\", line 154, in _get_merged_dataset\n",
      "  File \"/opt/ml/code/src/llamafactory/data/loader.py\", line 154, in _get_merged_dataset\n",
      "  File \"/opt/ml/code/src/llamafactory/data/loader.py\", line 154, in _get_merged_dataset\n",
      "    for dataset_attr in get_dataset_list(dataset_names, data_args.dataset_dir):\n",
      "  File \"/opt/ml/code/src/llamafactory/data/parser.py\", line 94, in get_dataset_list\n",
      "    for dataset_attr in get_dataset_list(dataset_names, data_args.dataset_dir):        \u001b[0m\n",
      "\u001b[34mfor dataset_attr in get_dataset_list(dataset_names, data_args.dataset_dir):for dataset_attr in get_dataset_list(dataset_names, data_args.dataset_dir):\n",
      "  File \"/opt/ml/code/src/llamafactory/data/parser.py\", line 94, in get_dataset_list\n",
      "              File \"/opt/ml/code/src/llamafactory/data/parser.py\", line 94, in get_dataset_list\u001b[0m\n",
      "\u001b[34mfor dataset_attr in get_dataset_list(dataset_names, data_args.dataset_dir):  File \"/opt/ml/code/src/llamafactory/data/parser.py\", line 94, in get_dataset_list\u001b[0m\n",
      "\u001b[34mfor dataset_attr in get_dataset_list(dataset_names, data_args.dataset_dir):for dataset_attr in get_dataset_list(dataset_names, data_args.dataset_dir):\n",
      "  File \"/opt/ml/code/src/llamafactory/data/parser.py\", line 94, in get_dataset_list\n",
      "      File \"/opt/ml/code/src/llamafactory/data/parser.py\", line 94, in get_dataset_list\n",
      "  File \"/opt/ml/code/src/llamafactory/data/parser.py\", line 94, in get_dataset_list\u001b[0m\n",
      "\u001b[34mraise ValueError(\"Cannot open {} due to {}.\".format(config_path, str(err)))\u001b[0m\n",
      "\u001b[34mValueError: Cannot open data/dataset_info.json due to Expecting property name enclosed in double quotes: line 13 column 5 (char 314).\n",
      "        raise ValueError(\"Cannot open {} due to {}.\".format(config_path, str(err)))raise ValueError(\"Cannot open {} due to {}.\".format(config_path, str(err)))    \u001b[0m\n",
      "\u001b[34mraise ValueError(\"Cannot open {} due to {}.\".format(config_path, str(err)))    ValueError\u001b[0m\n",
      "\u001b[34mraise ValueError(\"Cannot open {} due to {}.\".format(config_path, str(err)))ValueError: \u001b[0m\n",
      "\u001b[34m: ValueErrorCannot open data/dataset_info.json due to Expecting property name enclosed in double quotes: line 13 column 5 (char 314).    Cannot open data/dataset_info.json due to Expecting property name enclosed in double quotes: line 13 column 5 (char 314).    ValueError\u001b[0m\n",
      "\u001b[34m: \u001b[0m\n",
      "\u001b[34mraise ValueError(\"Cannot open {} due to {}.\".format(config_path, str(err)))raise ValueError(\"Cannot open {} due to {}.\".format(config_path, str(err))): \u001b[0m\n",
      "\u001b[34mCannot open data/dataset_info.json due to Expecting property name enclosed in double quotes: line 13 column 5 (char 314).\u001b[0m\n",
      "\u001b[34mCannot open data/dataset_info.json due to Expecting property name enclosed in double quotes: line 13 column 5 (char 314).\u001b[0m\n",
      "\u001b[34mValueErrorValueError: : Cannot open data/dataset_info.json due to Expecting property name enclosed in double quotes: line 13 column 5 (char 314).Cannot open data/dataset_info.json due to Expecting property name enclosed in double quotes: line 13 column 5 (char 314).\u001b[0m\n",
      "\u001b[34m[2024-10-08 09:21:01,689] torch.distributed.elastic.multiprocessing.api: [ERROR] failed (exitcode: 1) local_rank: 0 (pid: 220) of binary: /opt/conda/bin/python\u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\n",
      "  File \"/opt/conda/bin/torchrun\", line 33, in <module>\u001b[0m\n",
      "\u001b[34msys.exit(load_entry_point('torch==2.1.0', 'console_scripts', 'torchrun')())\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 346, in wrapper\u001b[0m\n",
      "\u001b[34mreturn f(*args, **kwargs)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py\", line 806, in main\u001b[0m\n",
      "\u001b[34mrun(args)\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/distributed/run.py\", line 797, in run\u001b[0m\n",
      "\u001b[34melastic_launch(\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 134, in __call__\u001b[0m\n",
      "\u001b[34mreturn launch_agent(self._config, self._entrypoint, list(args))\n",
      "  File \"/opt/conda/lib/python3.10/site-packages/torch/distributed/launcher/api.py\", line 264, in launch_agent\u001b[0m\n",
      "\u001b[34mraise ChildFailedError(\u001b[0m\n",
      "\u001b[34mtorch.distributed.elastic.multiprocessing.errors\u001b[0m\n",
      "\u001b[34m.ChildFailedError: \u001b[0m\n",
      "\u001b[34m============================================================\u001b[0m\n",
      "\u001b[34msrc/train.py FAILED\u001b[0m\n",
      "\u001b[34m------------------------------------------------------------\u001b[0m\n",
      "\u001b[34mFailures:\u001b[0m\n",
      "\u001b[34m[1]:\n",
      "  time      : 2024-10-08_09:21:01\n",
      "  host      : algo-1\n",
      "  rank      : 1 (local_rank: 1)\n",
      "  exitcode  : 1 (pid: 221)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\u001b[0m\n",
      "\u001b[34m[2]:\n",
      "  time      : 2024-10-08_09:21:01\n",
      "  host      : algo-1\n",
      "  rank      : 2 (local_rank: 2)\n",
      "  exitcode  : 1 (pid: 222)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\u001b[0m\n",
      "\u001b[34m[3]:\n",
      "  time      : 2024-10-08_09:21:01\n",
      "  host      : algo-1\n",
      "  rank      : 3 (local_rank: 3)\n",
      "  exitcode  : 1 (pid: 223)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\u001b[0m\n",
      "\u001b[34m[4]:\n",
      "  time      : 2024-10-08_09:21:01\n",
      "  host      : algo-1\n",
      "  rank      : 4 (local_rank: 4)\n",
      "  exitcode  : 1 (pid: 224)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\u001b[0m\n",
      "\u001b[34m[5]:\n",
      "  time      : 2024-10-08_09:21:01\n",
      "  host      : algo-1\n",
      "  rank      : 5 (local_rank: 5)\n",
      "  exitcode  : 1 (pid: 225)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\u001b[0m\n",
      "\u001b[34m[6]:\n",
      "  time      : 2024-10-08_09:21:01\n",
      "  host      : algo-1\n",
      "  rank      : 6 (local_rank: 6)\n",
      "  exitcode  : 1 (pid: 226)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\u001b[0m\n",
      "\u001b[34m[7]:\n",
      "  time      : 2024-10-08_09:21:01\n",
      "  host      : algo-1\n",
      "  rank      : 7 (local_rank: 7)\n",
      "  exitcode  : 1 (pid: 227)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\u001b[0m\n",
      "\u001b[34m------------------------------------------------------------\u001b[0m\n",
      "\u001b[34mRoot Cause (first observed failure):\u001b[0m\n",
      "\u001b[34m[0]:\n",
      "  time      : 2024-10-08_09:21:01\n",
      "  host      : algo-1\n",
      "  rank      : 0 (local_rank: 0)\n",
      "  exitcode  : 1 (pid: 220)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\u001b[0m\n",
      "\u001b[34m============================================================\u001b[0m\n",
      "\u001b[34mTraining script error, please check CloudWatch logs\u001b[0m\n",
      "\u001b[34m*****************finished training, start cp finetuned model*****************************\u001b[0m\n",
      "\u001b[34mName:\u001b[0m\n",
      "\u001b[34msync - sync objects\u001b[0m\n",
      "\u001b[34mUsage:\n",
      "  sync [options] source destination\u001b[0m\n",
      "\u001b[34mOptions:\u001b[0m\n",
      "\u001b[34mERROR \"sync /tmp/finetuned_model s3://sagemaker-us-east-1-596899493901/Qwen2.5_7B_it/QA_fake_sft/\": given object not found\u001b[0m\n",
      "\u001b[34m--delete                       delete objects in destination but not in source (default: false)\n",
      "  --size-only                    make size of object only criteria to decide whether an object should be synced (default: false)\n",
      "  --no-follow-symlinks           do not follow symbolic links (default: false)\n",
      "  --storage-class value          set storage class for target ('STANDARD','REDUCED_REDUNDANCY','GLACIER','STANDARD_IA','ONEZONE_IA','INTELLIGENT_TIERING','DEEP_ARCHIVE')\u001b[0m\n",
      "\u001b[34m--concurrency value, -c value  number of concurrent parts transferred between host and remote server (default: 5)\n",
      "  --part-size value, -p value    size of each part transferred between host and remote server, in MiB (default: 50)\n",
      "  --sse value                    perform server side encryption of the data at its destination, e.g. aws:kms\n",
      "  --sse-kms-key-id value         customer master key (CMK) id for SSE-KMS encryption; leave it out if server-side generated key is desired\n",
      "  --acl value                    set acl for target: defines granted accesses and their types on different accounts/groups, e.g. cp --acl 'public-read'\n",
      "  --cache-control value          set cache control for target: defines cache control header for object, e.g. cp --cache-control 'public, max-age=345600'\u001b[0m\n",
      "\u001b[34m--expires value                set expires for target (uses RFC3339 format): defines expires header for object, e.g. cp  --expires '2024-10-01T20:30:00Z'\n",
      "  --force-glacier-transfer       force transfer of glacier objects whether they are restored or not (default: false)\n",
      "  --ignore-glacier-warnings      turns off glacier warnings: ignore errors encountered during copying, downloading and moving glacier objects (default: false)\n",
      "  --source-region value          set the region of source bucket; the region of the source bucket will be automatically discovered if --source-region is not specified\n",
      "  --destination-region value     set the region of destination bucket: the region of the destination bucket will be automatically discovered if --destination-region is not specified\n",
      "  --exclude value                exclude objects with given pattern\n",
      "  --raw                          disable the wildcard operations, useful with filenames that contains glob characters (default: false)\n",
      "  --help, -h                     show help (default: false)\n",
      "  \u001b[0m\n",
      "\u001b[34mExamples:\n",
      "  01. Sync local folder to s3 bucket\n",
      "     > s5cmd sync folder/ s3://bucket/\n",
      "  02. Sync S3 bucket to local folder\n",
      "     > s5cmd sync s3://bucket/* folder/\n",
      "  03. Sync S3 bucket objects under prefix to S3 bucket.\n",
      "     > s5cmd sync s3://sourcebucket/prefix/* s3://destbucket/\n",
      "  04. Sync local folder to S3 but delete the files that S3 bucket has but local does not have.\n",
      "     > s5cmd sync --delete folder/ s3://bucket/\n",
      "  05. Sync S3 bucket to local folder but use size as only comparison criteria.\n",
      "     > s5cmd sync --size-only s3://bucket/* folder/\n",
      "  06. Sync a file to S3 bucket\u001b[0m\n",
      "\u001b[34m> s5cmd sync myfile.gz s3://bucket/\n",
      "  07. Sync matching S3 objects to another bucket\n",
      "     > s5cmd sync s3://bucket/*.gz s3://target-bucket/prefix/\n",
      "  08. Perform KMS Server Side Encryption of the object(s) at the destination\n",
      "     > s5cmd sync --sse aws:kms s3://bucket/object s3://target-bucket/prefix/object\n",
      "  09. Perform KMS-SSE of the object(s) at the destination using customer managed Customer Master Key (CMK) key id\n",
      "     > s5cmd sync --sse aws:kms --sse-kms-key-id <your-kms-key-id> s3://bucket/object s3://target-bucket/prefix/object\n",
      "  10. Sync all files to S3 bucket but exclude the ones with txt and gz extension\n",
      "     > s5cmd sync --exclude \"*.txt\" --exclude \"*.gz\" dir/ s3://bucket\u001b[0m\n",
      "\u001b[34m-----finished cp-------\u001b[0m\n",
      "\u001b[34m2024-10-08 09:21:01,972 sagemaker-training-toolkit INFO     Waiting for the process to finish and give a return code.\u001b[0m\n",
      "\u001b[34m2024-10-08 09:21:01,973 sagemaker-training-toolkit INFO     Done waiting for a return code. Received 0 from exiting process.\u001b[0m\n",
      "\u001b[34m2024-10-08 09:21:01,973 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2024-10-08 09:21:16 Uploading - Uploading generated training model\n",
      "2024-10-08 09:21:16 Completed - Training job completed\n",
      "Training seconds: 398\n",
      "Billable seconds: 398\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker.pytorch import PyTorch\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "instance_count = 1\n",
    "instance_type = 'ml.p4d.24xlarge'  ## 8*40G\n",
    "max_time = 200000\n",
    "\n",
    "# Get the current time\n",
    "current_time = datetime.now()\n",
    "\n",
    "wandb.sagemaker_auth(path=\"llama_factory/\")\n",
    "# Format the current time as a string\n",
    "formatted_time = current_time.strftime(\"%Y%m%d%H%M%S\")\n",
    "print(formatted_time)\n",
    "\n",
    "base_job_name = 'Qwen25-7B-it-finetune'\n",
    "environment = {\n",
    "    'NODE_NUMBER':str(instance_count),\n",
    "    'MODEL_S3_PATH': f's3://{sagemaker_default_bucket}/Foundation-Models/Qwen2.5_7B_it', # source model files\n",
    "    'MODEL_LOCAL_PATH': '/tmp/pretrain_model',\n",
    "    'OUTPUT_MODEL_S3_PATH': f's3://{sagemaker_default_bucket}/Qwen2.5_7B_it/QA_fake_sft/', # destination\n",
    "}\n",
    "\n",
    "estimator = PyTorch(entry_point='entry.py',\n",
    "                            source_dir='llama_factory/',\n",
    "                            role=role,\n",
    "                            base_job_name=base_job_name,\n",
    "                            environment=environment,\n",
    "                            framework_version='2.1.0',\n",
    "                            py_version='py310',\n",
    "                            script_mode=True,\n",
    "                            instance_count=instance_count,\n",
    "                            instance_type=instance_type,\n",
    "                            max_run=max_time)\n",
    "\n",
    "# # data in channel will be automatically copied to each node - /opt/ml/input/data/train1\n",
    "#input_channel = {'train': f's3://{sagemaker_default_bucket}/datasets/qiandao/{version}/train.json'}\n",
    "estimator.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "20280b16-36e2-4668-a9cd-ba0e49cdc325",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1685352357.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[17], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    ./llama_factory/s5cmd sync s3://sagemaker-us-east-1-596899493901/Foundation-Models/meta_llama31_8_it/ ./\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "./llama_factory/s5cmd sync s3://sagemaker-us-east-1-596899493901/Foundation-Models/meta_llama31_8_it/ ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ca8793-a1fb-4073-a0d1-e3e4c74d455b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
